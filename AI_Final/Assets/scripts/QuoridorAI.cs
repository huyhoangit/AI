using UnityEngine;
using System.Collections.Generic;
using System.Linq;
using System.IO;

/// <summary>
/// AI System cho Quoridor Game s·ª≠ d·ª•ng Minimax v·ªõi Alpha-Beta Pruning
/// K·∫øt h·ª£p v·ªõi A* pathfinding, Decision Tree Classifier v√† Entropy Analysis
/// </summary>
public class QuoridorAI : MonoBehaviour
{
    [Header("AI Settings")]
    public int playerID = 2; // AI lu√¥n l√† player 2
    public int maxDepth = 4; // ƒê·ªô s√¢u t√¨m ki·∫øm Minimax
    public float thinkTime = 1.0f; // Th·ªùi gian AI "suy nghƒ©"
    
    [Header("AI Strategy")]
    [Range(0f, 1f)]
    public float aggressiveness = 0.6f; // M·ª©c ƒë·ªô t·∫•n c√¥ng vs ph√≤ng th·ªß
    [Range(0f, 1f)]
    public float wallPlacementProbability = 0.3f; // X√°c su·∫•t ƒë·∫∑t wall thay v√¨ di chuy·ªÉn
    
    [Header("Decision Tree & Entropy Settings")]
    [Range(0f, 1f)]
    public float entropyThreshold = 0.7f; // Ng∆∞·ª°ng entropy ƒë·ªÉ k√≠ch ho·∫°t randomness
    [Range(0f, 2f)]
    public float softmaxTemperature = 1.0f; // Temperature cho softmax distribution
    public bool useDecisionTree = true; // B·∫≠t/t·∫Øt Decision Tree
    public bool useEntropyAnalysis = true; // B·∫≠t/t·∫Øt Entropy Analysis
    
    [Header("References")]
    public ChessPlayer aiPlayer; // Qu√¢n AI
    public ChessPlayer humanPlayer; // Qu√¢n ng∆∞·ªùi ch∆°i
    public WallPlacer aiWallPlacer; // WallPlacer c·ªßa AI
    
    private BoardManager boardManager;
    private GameState currentGameState;
    private bool isThinking = false;
    
    // Cache cho A* pathfinding
    private Dictionary<Vector2Int, List<Vector2Int>> pathCache = new Dictionary<Vector2Int, List<Vector2Int>>();
    
    // Decision Tree Classifier
    private DecisionTreeClassifier decisionTree;
    
    // Entropy Analysis
    private List<float> recentMoveScores = new List<float>();
    private Dictionary<string, int> movePatternHistory = new Dictionary<string, int>();
    
    // 1. Th√™m bi·∫øn QLearningAgent
    private QLearningAgent qAgent = new QLearningAgent();
    [Header("Q-Learning Settings")]
    public bool useQLearning = true; // B·∫≠t/t·∫Øt Q-learning
    public string qTablePath = "StreamingAssets/qtable.json";
    public bool allowQTableSaving = false; // Ch·ªâ save Q-table khi ƒëang training
    public bool isTrainedModel = false; // Flag ƒë·ªÉ bi·∫øt model ƒë√£ trained
    
    // Backup path alternatives
    private string GetQTablePath()
    {
        // Try multiple paths for better compatibility - prioritize StreamingAssets
        string[] paths = {
            Path.Combine(Application.streamingAssetsPath, "qtable.json"),         // StreamingAssets (highest priority)
            Path.Combine(Application.dataPath, "StreamingAssets", "qtable.json"), // Manual StreamingAssets path
            Path.Combine(Application.dataPath, "qtable.json"),                    // Assets/qtable.json (trong build)
            Path.Combine(Application.persistentDataPath, "qtable.json"),          // Persistent data (cross-platform)
            qTablePath,                                                           // Original path
            "qtable.json"                                                         // Current directory fallback
        };
        
        foreach (string path in paths)
        {
            Debug.Log($"üîç Checking Q-table path: {path}");
            if (File.Exists(path))
            {
                var fileInfo = new FileInfo(path);
                Debug.Log($"‚úÖ Found Q-table at: {path}");
                Debug.Log($"üìä File size: {fileInfo.Length / 1024f:F1} KB, Modified: {fileInfo.LastWriteTime}");
                return path;
            }
        }
        
        // If not found, copy from StreamingAssets to persistentDataPath
        string streamingPath = Path.Combine(Application.streamingAssetsPath, "qtable.json");
        string assetsPath = Path.Combine(Application.dataPath, "qtable.json");
        string targetPath = Path.Combine(Application.persistentDataPath, "qtable.json");
        
        // Try to copy from StreamingAssets first
        if (File.Exists(streamingPath))
        {
            try
            {
                File.Copy(streamingPath, targetPath, true);
                Debug.Log($"üìÅ Copied Q-table from StreamingAssets: {streamingPath} to {targetPath}");
                return targetPath;
            }
            catch (System.Exception e)
            {
                Debug.LogWarning($"‚ö†Ô∏è Failed to copy from StreamingAssets: {e.Message}");
            }
        }
        
        // Fallback: copy from Assets
        if (File.Exists(assetsPath))
        {
            try
            {
                File.Copy(assetsPath, targetPath, true);
                Debug.Log($"üìÅ Copied Q-table from Assets: {assetsPath} to {targetPath}");
                return targetPath;
            }
            catch (System.Exception e)
            {
                Debug.LogWarning($"‚ö†Ô∏è Failed to copy from Assets: {e.Message}");
            }
        }
        
        Debug.Log($"üìÅ No Q-table found, will create new at: {targetPath}");
        return targetPath;
    }

    // 2. Trong Start(), load Q-table n·∫øu c√≥
    void Start()
    {
        boardManager = BoardManager.Instance;
        if (boardManager == null)
        {
            boardManager = FindFirstObjectByType<BoardManager>();
        }
        
        // T·ª± ƒë·ªông t√¨m c√°c component c·∫ßn thi·∫øt
        FindGameComponents();
        
        // Kh·ªüi t·∫°o game state
        InitializeGameState();
        
        // Kh·ªüi t·∫°o Decision Tree Classifier
        InitializeDecisionTree();
        
        // KI·ªÇM TRA xem c√≥ Self-Play Trainer ƒëang ch·∫°y kh√¥ng
        QuoridorSelfPlayTrainer trainer = FindFirstObjectByType<QuoridorSelfPlayTrainer>();
        if (trainer != null && trainer.autoStartTraining)
        {
            Debug.LogWarning("‚ö†Ô∏è QuoridorSelfPlayTrainer is auto-training! This may override your Q-table.");
            Debug.LogWarning("üí° Suggest: Set autoStartTraining = false on QuoridorSelfPlayTrainer to use existing trained model");
        }
        
        // Get best available Q-table path
        string actualQTablePath = GetQTablePath();
        
        // Trong Start(), load Q-table n·∫øu c√≥ TR∆Ø·ªöC khi initialize
        qAgent.LoadQTable(actualQTablePath);
        
        // N·∫øu Q-table c√≥ d·ªØ li·ªáu (ƒë√£ trained), preserve epsilon v√† init without reset
        if (qAgent.GetQTableSize() > 1000) // N·∫øu c√≥ > 1000 states (ƒë√£ trained)
        {
            qAgent.SetTrainedEpsilon(0.1f, 1000, 1000); // 10% exploration, ƒë√£ trained
            qAgent.InitializeWithoutEpsilonReset(); // Preserve trained epsilon
            isTrainedModel = true; // ƒê√°nh d·∫•u l√† model ƒë√£ trained
            allowQTableSaving = false; // Kh√¥ng save ƒë·ªÉ tr√°nh ghi ƒë√®
            Debug.Log($"üéì Using trained Q-table with exploitation mode (Œµ=0.1)");
            Debug.Log($"üîí Q-Learning will NOT retrain - using existing knowledge");
            Debug.Log($"üö´ Q-table saving DISABLED to preserve trained model");
        }
        else
        {
            // N·∫øu ch∆∞a c√≥ trained data, initialize normal
            qAgent.Initialize();
            isTrainedModel = false;
            allowQTableSaving = true; // Cho ph√©p save khi ƒëang training
            Debug.Log($"üÜï Starting fresh Q-Learning training");
            Debug.Log($"üíæ Q-table saving ENABLED for training");
        }
        
        // Debug Q-table status after loading
        Debug.Log($"üß† Q-Learning enabled: {useQLearning}");
        Debug.Log($"üìä Q-table loaded with {qAgent.GetQTableSize()} states");
        
        Debug.Log($"ü§ñ QuoridorAI initialized for Player {playerID} with Decision Tree and Entropy Analysis");
        
        // Debug all paths for troubleshooting
        DebugAllPaths();
    }
    
    /// <summary>
    /// Debug all available paths for Q-table
    /// </summary>
    [ContextMenu("Debug Q-table Paths")]
    public void DebugAllPaths()
    {
        Debug.Log("=== Q-TABLE PATH DEBUG ===");
        Debug.Log($"üìÅ Application.dataPath: {Application.dataPath}");
        Debug.Log($"üìÅ Application.persistentDataPath: {Application.persistentDataPath}");
        Debug.Log($"üìÅ Application.streamingAssetsPath: {Application.streamingAssetsPath}");
        Debug.Log($"üìÅ Current Directory: {System.Environment.CurrentDirectory}");
        
        string[] testPaths = {
            Path.Combine(Application.streamingAssetsPath, "qtable.json"),
            Path.Combine(Application.dataPath, "qtable.json"),
            Path.Combine(Application.dataPath, "StreamingAssets", "qtable.json"),
            Path.Combine(Application.persistentDataPath, "qtable.json"),
            qTablePath
        };
        
        for (int i = 0; i < testPaths.Length; i++)
        {
            bool exists = File.Exists(testPaths[i]);
            Debug.Log($"üîç Path {i+1}: {testPaths[i]} -> {(exists ? "‚úÖ EXISTS" : "‚ùå NOT FOUND")}");
            
            if (exists)
            {
                FileInfo info = new FileInfo(testPaths[i]);
                Debug.Log($"    üìä Size: {info.Length / 1024f:F1} KB, Modified: {info.LastWriteTime}");
            }
        }
    }
    
    /// <summary>
    /// T·ª± ƒë·ªông t√¨m c√°c component trong game
    /// </summary>
    void FindGameComponents()
    {
        // T√¨m t·∫•t c·∫£ ChessPlayer
        ChessPlayer[] allPlayers = FindObjectsByType<ChessPlayer>(FindObjectsSortMode.None);
        
        foreach (ChessPlayer player in allPlayers)
        {
            if (player.playerID == playerID)
            {
                aiPlayer = player;
                Debug.Log($"‚úÖ Found AI Player: {player.name}");
            }
            else if (player.playerID != playerID)
            {
                humanPlayer = player;
                Debug.Log($"‚úÖ Found Human Player: {player.name}");
            }
        }
        
        // T√¨m WallPlacer c·ªßa AI
        var allWallPlacers = FindObjectsByType<MonoBehaviour>(FindObjectsSortMode.None);
        foreach (var component in allWallPlacers)
        {
            if (component.GetType().Name == "WallPlacer")
            {
                var playerIDField = component.GetType().GetField("playerID");
                if (playerIDField != null)
                {
                    int wallPlayerID = (int)playerIDField.GetValue(component);
                    if (wallPlayerID == playerID)
                    {
                        // Cast to WallPlacer (using reflection)
                        Debug.Log($"‚úÖ Found AI WallPlacer: {component.name}");
                        break;
                    }
                }
            }
        }
    }
    
    /// <summary>
    /// Kh·ªüi t·∫°o tr·∫°ng th√°i game
    /// </summary>
    void InitializeGameState()
    {
        currentGameState = new GameState();
        
        if (aiPlayer != null)
        {
            currentGameState.aiPosition = new Vector2Int(aiPlayer.col, aiPlayer.row);
        }
        
        if (humanPlayer != null)
        {
            currentGameState.humanPosition = new Vector2Int(humanPlayer.col, humanPlayer.row);
        }
        
        currentGameState.placedWalls = GetAllPlacedWalls();
        currentGameState.aiWallsLeft = 10; // M·ªói player c√≥ 10 walls
        currentGameState.humanWallsLeft = 10;
    }
    
    /// <summary>
    /// AI th·ª±c hi·ªán l∆∞·ª£t ƒëi
    /// </summary>
    public void MakeAIMove()
    {
        if (isThinking) return;
        
        StartCoroutine(MakeAIMoveCoroutine());
    }
    
    /// <summary>
    /// Coroutine ƒë·ªÉ AI suy nghƒ© v√† th·ª±c hi·ªán n∆∞·ªõc ƒëi
    /// </summary>
    System.Collections.IEnumerator MakeAIMoveCoroutine()
    {
        isThinking = true;
        Debug.Log($"ü§ñ AI is thinking...");
        
        // C·∫≠p nh·∫≠t game state hi·ªán t·∫°i
        UpdateGameState();
        
        // Th·ªùi gian suy nghƒ©
        yield return new UnityEngine.WaitForSeconds(thinkTime);
        
        // T√¨m n∆∞·ªõc ƒëi t·ªët nh·∫•t b·∫±ng Minimax ho·∫∑c Q-Learning
        AIMove bestMove = null;
        List<AIMove> possibleMoves = GeneratePossibleMoves(currentGameState, true);
        
        if (useQLearning)
        {
            Debug.Log($"üß† Using Q-Learning algorithm (Epsilon: {qAgent.GetEpsilonInfo().currentEpsilon:F3})");
            bestMove = qAgent.ChooseAction(currentGameState, possibleMoves);
            Debug.Log($"üéØ Q-learning selected move: {qAgent.EncodeAction(bestMove)}");
        }
        else
        {
            Debug.Log($"üîç Using Minimax algorithm (Depth: {maxDepth})");
            bestMove = FindBestMove(); // Minimax
            Debug.Log($"üéØ Minimax selected move: {bestMove?.moveType} to {bestMove?.targetPosition}");
        }
        
        if (bestMove != null)
        {
            // L∆∞u l·∫°i state v√† action ƒë·ªÉ c·∫≠p nh·∫≠t Q sau khi th·ª±c hi·ªán
            lastState = currentGameState.Clone();
            lastAction = bestMove;
            ExecuteAIMove(bestMove);
        }
        else
        {
            Debug.LogWarning("‚ö†Ô∏è AI couldn't find a valid move!");
        }
        
        isThinking = false;
    }
    
    /// <summary>
    /// Public method ƒë·ªÉ GameManager call - s·∫Ω ch·ªçn algorithm d·ª±a v√†o useQLearning flag
    /// </summary>
    public AIMove GetBestMove()
    {
        UpdateGameState();
        List<AIMove> possibleMoves = GeneratePossibleMoves(currentGameState, true);
        
        if (useQLearning)
        {
            Debug.Log($"üß† AI using Q-Learning (Œµ={qAgent.GetEpsilonInfo().currentEpsilon:F3}, States={qAgent.GetQTableSize()})");
            var move = qAgent.ChooseAction(currentGameState, possibleMoves);
            Debug.Log($"üéØ Q-Learning selected: {qAgent.EncodeAction(move)}");
            return move;
        }
        else
        {
            Debug.Log($"üîç AI using Minimax (Depth={maxDepth}, Aggressiveness={aggressiveness:F2})");
            var move = FindBestMove();
            Debug.Log($"üéØ Minimax selected: {move?.moveType} to {move?.targetPosition}");
            return move;
        }
    }
    
    /// <summary>
    /// C·∫≠p nh·∫≠t tr·∫°ng th√°i game hi·ªán t·∫°i
    /// </summary>
    void UpdateGameState()
    {
        if (aiPlayer != null)
        {
            currentGameState.aiPosition = new Vector2Int(aiPlayer.col, aiPlayer.row);
        }
        
        if (humanPlayer != null)
        {
            currentGameState.humanPosition = new Vector2Int(humanPlayer.col, humanPlayer.row);
        }
        
        currentGameState.placedWalls = GetAllPlacedWalls();
        
        // Clear path cache khi c√≥ wall m·ªõi
        pathCache.Clear();
    }
    
    /// <summary>
    /// T√¨m n∆∞·ªõc ƒëi t·ªët nh·∫•t b·∫±ng Minimax v·ªõi Alpha-Beta Pruning, Decision Tree v√† Entropy Analysis
    /// </summary>
    AIMove FindBestMove()
    {
        Debug.Log($"üîç Finding best move with depth {maxDepth}");
        
        // Generate t·∫•t c·∫£ possible moves
        List<AIMove> possibleMoves = GeneratePossibleMoves(currentGameState, true);
        Debug.Log($"üìã Generated {possibleMoves.Count} possible moves for AI");
        
        // Phase 1: Decision Tree Classifier ƒë·ªÉ l·ªçc strategy
        if (useDecisionTree && decisionTree != null)
        {
            var strategyDecision = GetDecisionTreeStrategy();
            possibleMoves = FilterMovesByStrategy(possibleMoves, strategyDecision);
            Debug.Log($"üå≥ Decision Tree filtered to {possibleMoves.Count} moves (Strategy: {strategyDecision})");
        }
        
        // Phase 2: ƒê√°nh gi√° moves b·∫±ng Minimax
        var moveScores = new List<(AIMove move, float score)>();
        float bestScore = float.NegativeInfinity;
        
        foreach (AIMove move in possibleMoves)
        {
            // Apply move t·∫°m th·ªùi
            GameState newState = ApplyMove(currentGameState, move, true);
            
            // Minimax v·ªõi Alpha-Beta Pruning
            float score = Minimax(newState, maxDepth - 1, float.NegativeInfinity, float.PositiveInfinity, false);
            
            moveScores.Add((move, score));
            if (score > bestScore)
            {
                bestScore = score;
            }
            
            Debug.Log($"   Move {move.moveType} to {move.targetPosition} -> Score: {score:F2}");
        }
        
        // Phase 3: Entropy Analysis cho selection
        AIMove selectedMove = null;
        if (useEntropyAnalysis && moveScores.Count > 1)
        {
            selectedMove = SelectMoveWithEntropyAnalysis(moveScores, bestScore);
            Debug.Log($"üé≤ Entropy Analysis selected move with diversity consideration");
        }
        else
        {
            // Fallback: ch·ªçn move c√≥ score cao nh·∫•t
            selectedMove = moveScores.OrderByDescending(ms => ms.score).FirstOrDefault().move;
        }
        
        // C·∫≠p nh·∫≠t history cho entropy analysis
        UpdateMoveHistory(selectedMove, bestScore);
        
        Debug.Log($"üéØ Final selected move: {selectedMove?.moveType} with score {bestScore:F2}");
        return selectedMove;
    }
    
    /// <summary>
    /// Minimax algorithm v·ªõi Alpha-Beta Pruning
    /// </summary>
    float Minimax(GameState state, int depth, float alpha, float beta, bool maximizingPlayer)
    {
        // Base case: h·∫øt ƒë·ªô s√¢u ho·∫∑c game k·∫øt th√∫c
        if (depth == 0 || IsGameOver(state))
        {
            return EvaluateGameState(state);
        }
        
        if (maximizingPlayer) // AI turn
        {
            float maxEval = float.NegativeInfinity;
            List<AIMove> moves = GeneratePossibleMoves(state, true);
            
            foreach (AIMove move in moves)
            {
                GameState newState = ApplyMove(state, move, true);
                float eval = Minimax(newState, depth - 1, alpha, beta, false);
                maxEval = Mathf.Max(maxEval, eval);
                alpha = Mathf.Max(alpha, eval);
                
                if (beta <= alpha) break; // Alpha-Beta Pruning
            }
            
            return maxEval;
        }
        else // Human turn
        {
            float minEval = float.PositiveInfinity;
            List<AIMove> moves = GeneratePossibleMoves(state, false);
            
            foreach (AIMove move in moves)
            {
                GameState newState = ApplyMove(state, move, false);
                float eval = Minimax(newState, depth - 1, alpha, beta, true);
                minEval = Mathf.Min(minEval, eval);
                beta = Mathf.Min(beta, eval);
                
                if (beta <= alpha) break; // Alpha-Beta Pruning
            }
            
            return minEval;
        }
    }
    
    /// <summary>
    /// ƒê√°nh gi√° tr·∫°ng th√°i game (heuristic function)
    /// </summary>
    float EvaluateGameState(GameState state)
    {
        float score = 0f;
        
        // 1. Kho·∫£ng c√°ch ƒë·∫øn ƒë√≠ch
        int aiDistanceToGoal = GetShortestPathLength(state.aiPosition, GetAIGoalRow(), state.placedWalls);
        int humanDistanceToGoal = GetShortestPathLength(state.humanPosition, GetHumanGoalRow(), state.placedWalls);
        
        // AI mu·ªën gi·∫£m kho·∫£ng c√°ch c·ªßa m√¨nh v√† tƒÉng kho·∫£ng c√°ch c·ªßa ƒë·ªëi th·ªß
        score += (humanDistanceToGoal - aiDistanceToGoal) * 100f;
        
        // 2. Mobility (s·ªë n∆∞·ªõc ƒëi c√≥ th·ªÉ)
        int aiMobility = GetValidMoveCount(state.aiPosition, state.placedWalls);
        int humanMobility = GetValidMoveCount(state.humanPosition, state.placedWalls);
        score += (aiMobility - humanMobility) * 10f;
        
        // 3. Wall advantage
        score += (state.aiWallsLeft - state.humanWallsLeft) * 5f;
        
        // 4. Position evaluation
        score += EvaluatePosition(state.aiPosition, true) - EvaluatePosition(state.humanPosition, false);
        
        // 5. Win condition
        if (state.aiPosition.y == GetAIGoalRow())
        {
            score += 10000f; // AI wins
        }
        else if (state.humanPosition.y == GetHumanGoalRow())
        {
            score -= 10000f; // AI loses
        }
        
        return score;
    }
    
    /// <summary>
    /// T√≠nh ƒë∆∞·ªùng ƒëi ng·∫Øn nh·∫•t b·∫±ng A*
    /// </summary>
    int GetShortestPathLength(Vector2Int start, int goalRow, List<WallInfo> walls)
    {
        // S·ª≠ d·ª•ng cache n·∫øu c√≥
        Vector2Int cacheKey = new Vector2Int(start.x * 1000 + start.y, goalRow);
        if (pathCache.ContainsKey(cacheKey))
        {
            return pathCache[cacheKey].Count;
        }
        
        List<Vector2Int> path = FindPathAStar(start, goalRow, walls);
        pathCache[cacheKey] = path;
        
        return path != null ? path.Count : 999; // Return large number if no path
    }
    
    /// <summary>
    /// A* Pathfinding algorithm
    /// </summary>
    List<Vector2Int> FindPathAStar(Vector2Int start, int goalRow, List<WallInfo> walls)
    {
        var openSet = new List<AStarNode>();
        var closedSet = new HashSet<Vector2Int>();
        
        var startNode = new AStarNode(start, 0, Mathf.Abs(start.y - goalRow));
        openSet.Add(startNode);
        
        var cameFrom = new Dictionary<Vector2Int, Vector2Int>();
        
        while (openSet.Count > 0)
        {
            // T√¨m node c√≥ f-cost th·∫•p nh·∫•t
            var current = openSet.OrderBy(n => n.fCost).First();
            openSet.Remove(current);
            closedSet.Add(current.position);
            
            // ƒê√£ ƒë·∫øn ƒë√≠ch
            if (current.position.y == goalRow)
            {
                return ReconstructPath(cameFrom, current.position);
            }
            
            // Ki·ªÉm tra c√°c neighbor
            Vector2Int[] neighbors = {
                current.position + Vector2Int.up,
                current.position + Vector2Int.down,
                current.position + Vector2Int.left,
                current.position + Vector2Int.right
            };
            
            foreach (var neighbor in neighbors)
            {
                // Ki·ªÉm tra bounds
                if (neighbor.x < 0 || neighbor.x >= 9 || neighbor.y < 0 || neighbor.y >= 9)
                    continue;
                
                // ƒê√£ ƒë∆∞·ª£c explore
                if (closedSet.Contains(neighbor))
                    continue;
                
                // Ki·ªÉm tra wall blocking
                if (IsBlockedByWallInfo(current.position, neighbor, walls))
                    continue;
                
                float tentativeGCost = current.gCost + 1;
                
                var existingNode = openSet.FirstOrDefault(n => n.position == neighbor);
                if (existingNode == null)
                {
                    var newNode = new AStarNode(neighbor, tentativeGCost, Mathf.Abs(neighbor.y - goalRow));
                    openSet.Add(newNode);
                    cameFrom[neighbor] = current.position;
                }
                else if (tentativeGCost < existingNode.gCost)
                {
                    existingNode.gCost = tentativeGCost;
                    existingNode.fCost = existingNode.gCost + existingNode.hCost;
                    cameFrom[neighbor] = current.position;
                }
            }
        }
        
        return null; // No path found
    }
    
    /// <summary>
    /// Reconstruct path t·ª´ A*
    /// </summary>
    List<Vector2Int> ReconstructPath(Dictionary<Vector2Int, Vector2Int> cameFrom, Vector2Int current)
    {
        var path = new List<Vector2Int> { current };
        
        while (cameFrom.ContainsKey(current))
        {
            current = cameFrom[current];
            path.Insert(0, current);
        }
        
        return path;
    }
    
    /// <summary>
    /// Generate t·∫•t c·∫£ possible moves cho m·ªôt player
    /// </summary>
    List<AIMove> GeneratePossibleMoves(GameState state, bool isAIPlayer)
    {
        var moves = new List<AIMove>();
        Vector2Int playerPos = isAIPlayer ? state.aiPosition : state.humanPosition;
        int wallsLeft = isAIPlayer ? state.aiWallsLeft : state.humanWallsLeft;
        
        // 1. Movement moves
        Vector2Int[] directions = { Vector2Int.up, Vector2Int.down, Vector2Int.left, Vector2Int.right };
        
        foreach (var direction in directions)
        {
            Vector2Int newPos = playerPos + direction;
            
            if (IsValidPosition(newPos) && !IsBlockedByWallInfo(playerPos, newPos, state.placedWalls))
            {
                moves.Add(new AIMove
                {
                    moveType = MoveType.Movement,
                    targetPosition = newPos
                });
            }
        }
        
        // 2. Wall placement moves (n·∫øu c√≤n wall)
        if (wallsLeft > 0)
        {
            var wallMoves = GenerateWallMoves(state);
            
            // Ch·ªâ th√™m m·ªôt s·ªë wall moves t·ªët nh·∫•t ƒë·ªÉ tr√°nh explosion
            wallMoves = wallMoves.OrderByDescending(m => EvaluateWallMove(m, state, isAIPlayer))
                               .Take(8) // Limit s·ªë wall moves
                               .ToList();
            
            moves.AddRange(wallMoves);
        }
        
        return moves;
    }
    
    /// <summary>
    /// Generate possible wall moves
    /// </summary>
    List<AIMove> GenerateWallMoves(GameState state)
    {
        var wallMoves = new List<AIMove>();
        
        // T·∫°o wall moves cho c√°c v·ªã tr√≠ strategic
        for (int row = 0; row < 8; row++) // 8 v√¨ wall n·∫±m gi·ªØa c√°c √¥
        {
            for (int col = 0; col < 8; col++)
            {
                // Horizontal wall
                var hWallMove = new AIMove
                {
                    moveType = MoveType.WallPlacement,
                    targetPosition = new Vector2Int(col, row),
                    isHorizontalWall = true
                };
                
                if (IsValidWallPlacement(hWallMove, state))
                {
                    wallMoves.Add(hWallMove);
                }
                
                // Vertical wall
                var vWallMove = new AIMove
                {
                    moveType = MoveType.WallPlacement,
                    targetPosition = new Vector2Int(col, row),
                    isHorizontalWall = false
                };
                
                if (IsValidWallPlacement(vWallMove, state))
                {
                    wallMoves.Add(vWallMove);
                }
            }
        }
        
        return wallMoves;
    }
    
    /// <summary>
    /// Th·ª±c hi·ªán AI move
    /// </summary>
    void ExecuteAIMove(AIMove move)
    {
        Debug.Log($"ü§ñ AI executing move: {move.moveType}");
        
        if (move.moveType == MoveType.Movement)
        {
            // Di chuy·ªÉn qu√¢n AI
            if (aiPlayer != null)
            {
                aiPlayer.col = move.targetPosition.x;
                aiPlayer.row = move.targetPosition.y;
                
                // C·∫≠p nh·∫≠t v·ªã tr√≠ visual
                var updateMethod = aiPlayer.GetType().GetMethod("UpdatePosition", 
                    System.Reflection.BindingFlags.NonPublic | System.Reflection.BindingFlags.Instance);
                updateMethod?.Invoke(aiPlayer, null);
                
                Debug.Log($"ü§ñ AI moved to [{move.targetPosition.x}, {move.targetPosition.y}]");
            }
        }
        else if (move.moveType == MoveType.WallPlacement)
        {
            // ƒê·∫∑t wall
            PlaceAIWall(move);
        }
        
        // Ki·ªÉm tra win condition
        CheckWinCondition();
    }
    
    /// <summary>
    /// ƒê·∫∑t wall cho AI
    /// </summary>
    void PlaceAIWall(AIMove wallMove)
    {
        // S·ª≠ d·ª•ng reflection ƒë·ªÉ g·ªçi WallPlacer
        var allWallPlacers = FindObjectsByType<MonoBehaviour>(FindObjectsSortMode.None);
        foreach (var component in allWallPlacers)
        {
            if (component.GetType().Name == "WallPlacer")
            {
                var playerIDField = component.GetType().GetField("playerID");
                if (playerIDField != null)
                {
                    int wallPlayerID = (int)playerIDField.GetValue(component);
                    if (wallPlayerID == playerID)
                    {
                        // T√¨m method PlaceWallAt
                        var placeMethod = component.GetType().GetMethod("PlaceWallAt");
                        if (placeMethod != null)
                        {
                            // Convert AI move to world position
                            Vector3 wallWorldPos = CalculateWallWorldPosition(wallMove);
                            
                            placeMethod.Invoke(component, new object[] { wallWorldPos, wallMove.isHorizontalWall });
                            Debug.Log($"üß± AI placed {(wallMove.isHorizontalWall ? "horizontal" : "vertical")} wall at {wallWorldPos}");
                        }
                        break;
                    }
                }
            }
        }
    }
    
    /// <summary>
    /// T√≠nh v·ªã tr√≠ world cho wall placement
    /// </summary>
    Vector3 CalculateWallWorldPosition(AIMove wallMove)
    {
        Vector3 boardStart = new Vector3(-5.0f, 0f, -4.85f);
        float stepSize = 1.1f; // squareSize + spacing
        
        Vector3 worldPos;
        
        if (wallMove.isHorizontalWall)
        {
            // Horizontal wall gi·ªØa 2 h√†ng
            worldPos = new Vector3(
                boardStart.x + wallMove.targetPosition.x * stepSize + 0.5f,
                0.1f,
                boardStart.z + (wallMove.targetPosition.y + 1) * stepSize - 0.05f
            );
        }
        else
        {
            // Vertical wall gi·ªØa 2 c·ªôt
            worldPos = new Vector3(
                boardStart.x + (wallMove.targetPosition.x + 1) * stepSize - 0.05f,
                0.1f,
                boardStart.z + wallMove.targetPosition.y * stepSize + 0.5f
            );
        }
        
        return worldPos;
    }
    
    /// <summary>
    /// Ki·ªÉm tra ƒëi·ªÅu ki·ªán th·∫Øng
    /// </summary>
    void CheckWinCondition()
    {
        if (aiPlayer != null && aiPlayer.row == GetAIGoalRow())
        {
            Debug.Log("üèÜ AI WINS!");
            // TODO: Implement win screen
        }
        else if (humanPlayer != null && humanPlayer.row == GetHumanGoalRow())
        {
            Debug.Log("üèÜ HUMAN WINS!");
            // TODO: Implement win screen
        }
    }
    
    // ========== HELPER METHODS ==========
    
    int GetAIGoalRow() => 0; // AI (Player 2) mu·ªën v·ªÅ h√†ng 0
    int GetHumanGoalRow() => 8; // Human (Player 1) mu·ªën v·ªÅ h√†ng 8
    
    bool IsValidPosition(Vector2Int pos)
    {
        return pos.x >= 0 && pos.x < 9 && pos.y >= 0 && pos.y < 9;
    }
    
    List<WallInfo> GetAllPlacedWalls()
    {
        var walls = new List<WallInfo>();
        
        GameObject[] placedWalls1 = GameObject.FindGameObjectsWithTag("PlacedWall");
        GameObject[] placedWalls2 = GameObject.FindGameObjectsWithTag("Wall");
        
        foreach (var wall in placedWalls1.Concat(placedWalls2))
        {
            if (wall != null)
            {
                walls.Add(new WallInfo
                {
                    position = wall.transform.position,
                    isHorizontal = IsHorizontalWall(wall)
                });
            }
        }
        
        return walls;
    }
    
    bool IsHorizontalWall(GameObject wall)
    {
        if (wall.name.Contains("_H_") || wall.name.EndsWith("_H"))
            return true;
        if (wall.name.Contains("_V_") || wall.name.EndsWith("_V"))
            return false;
        
        Vector3 scale = wall.transform.localScale;
        return scale.x > scale.z;
    }
    
    bool IsBlockedByWallInfo(Vector2Int from, Vector2Int to, List<WallInfo> walls)
    {
        // Implement wall blocking logic similar to ChessPlayer
        // Simplified version for AI
        foreach (var wall in walls)
        {
            if (IsWallBlockingMovement(from, to, wall))
                return true;
        }
        return false;
    }
    
    bool IsWallBlockingMovement(Vector2Int from, Vector2Int to, WallInfo wall)
    {
        // Simplified wall blocking check
        Vector3 boardStart = new Vector3(-5.0f, 0f, -4.85f);
        float stepSize = 1.1f;
        
        int deltaCol = to.x - from.x;
        int deltaRow = to.y - from.y;
        
        Vector3 expectedWallPos = Vector3.zero;
        bool shouldBlock = false;
        
        if (deltaCol == 1) // Moving RIGHT
        {
            expectedWallPos = new Vector3(
                boardStart.x + (from.x + 1) * stepSize - 0.05f,
                0.1f,
                boardStart.z + from.y * stepSize + 0.5f
            );
            shouldBlock = !wall.isHorizontal;
        }
        else if (deltaCol == -1) // Moving LEFT
        {
            expectedWallPos = new Vector3(
                boardStart.x + from.x * stepSize - 0.05f,
                0.1f,
                boardStart.z + from.y * stepSize + 0.5f
            );
            shouldBlock = !wall.isHorizontal;
        }
        else if (deltaRow == 1) // Moving UP
        {
            expectedWallPos = new Vector3(
                boardStart.x + from.x * stepSize + 0.5f,
                0.1f,
                boardStart.z + (from.y + 1) * stepSize - 0.05f
            );
            shouldBlock = wall.isHorizontal;
        }
        else if (deltaRow == -1) // Moving DOWN
        {
            expectedWallPos = new Vector3(
                boardStart.x + from.x * stepSize + 0.5f,
                0.1f,
                boardStart.z + from.y * stepSize - 0.05f
            );
            shouldBlock = wall.isHorizontal;
        }
        
        float distance = Vector3.Distance(wall.position, expectedWallPos);
        return shouldBlock && distance < 0.7f;
    }
    
    int GetValidMoveCount(Vector2Int position, List<WallInfo> walls)
    {
        int count = 0;
        Vector2Int[] directions = { Vector2Int.up, Vector2Int.down, Vector2Int.left, Vector2Int.right };
        
        foreach (var direction in directions)
        {
            Vector2Int newPos = position + direction;
            if (IsValidPosition(newPos) && !IsBlockedByWallInfo(position, newPos, walls))
            {
                count++;
            }
        }
        
        return count;
    }
    
    float EvaluatePosition(Vector2Int position, bool isAI)
    {
        float score = 0f;
        
        // Center control
        float centerDistance = Vector2.Distance(position, new Vector2(4, 4));
        score += (9 - centerDistance) * 2f;
        
        // Progress toward goal
        int goalRow = isAI ? GetAIGoalRow() : GetHumanGoalRow();
        float progressScore = (8 - Mathf.Abs(position.y - goalRow)) * 3f;
        score += isAI ? progressScore : -progressScore;
        
        return score;
    }
    
    float EvaluateWallMove(AIMove wallMove, GameState state, bool isAIPlayer)
    {
        // Simple wall evaluation - can be improved
        Vector2Int enemyPos = isAIPlayer ? state.humanPosition : state.aiPosition;
        float distanceToEnemy = Vector2.Distance(wallMove.targetPosition, enemyPos);
        
        // Prefer walls closer to enemy
        return 10f - distanceToEnemy;
    }
    
    bool IsValidWallPlacement(AIMove wallMove, GameState state)
    {
        // Simple validation - check if position is not occupied
        foreach (var wall in state.placedWalls)
        {
            Vector3 wallWorldPos = CalculateWallWorldPosition(wallMove);
            if (Vector3.Distance(wall.position, wallWorldPos) < 0.5f)
            {
                return false; // Too close to existing wall
            }
        }
        
        return true;
    }
    
    GameState ApplyMove(GameState state, AIMove move, bool isAIPlayer)
    {
        GameState newState = state.Clone();
        
        if (move.moveType == MoveType.Movement)
        {
            if (isAIPlayer)
            {
                newState.aiPosition = move.targetPosition;
            }
            else
            {
                newState.humanPosition = move.targetPosition;
            }
        }
        else if (move.moveType == MoveType.WallPlacement)
        {
            newState.placedWalls.Add(new WallInfo
            {
                position = CalculateWallWorldPosition(move),
                isHorizontal = move.isHorizontalWall
            });
            
            if (isAIPlayer)
            {
                newState.aiWallsLeft--;
            }
            else
            {
                newState.humanWallsLeft--;
            }
        }
        
        return newState;
    }
    
    bool IsGameOver(GameState state)
    {
        return state.aiPosition.y == GetAIGoalRow() || state.humanPosition.y == GetHumanGoalRow();
    }
    
    // ========== DECISION TREE & ENTROPY STRUCTURES ==========
    
    /// <summary>
    /// Kh·ªüi t·∫°o Decision Tree Classifier
    /// </summary>
    void InitializeDecisionTree()
    {
        decisionTree = new DecisionTreeClassifier();
        decisionTree.Initialize();
        Debug.Log("üå≥ Decision Tree Classifier initialized");
    }
    
    /// <summary>
    /// L·∫•y strategy decision t·ª´ Decision Tree
    /// </summary>
    StrategyDecision GetDecisionTreeStrategy()
    {
        if (decisionTree == null) return StrategyDecision.Balanced;
        
        var features = ExtractDecisionFeatures();
        return decisionTree.Predict(features);
    }
    
    /// <summary>
    /// Tr√≠ch xu·∫•t features cho Decision Tree
    /// </summary>
    DecisionFeatures ExtractDecisionFeatures()
    {
        int aiDistanceToGoal = GetShortestPathLength(currentGameState.aiPosition, GetAIGoalRow(), currentGameState.placedWalls);
        int humanDistanceToGoal = GetShortestPathLength(currentGameState.humanPosition, GetHumanGoalRow(), currentGameState.placedWalls);
        
        return new DecisionFeatures
        {
            aiDistanceToGoal = aiDistanceToGoal,
            humanDistanceToGoal = humanDistanceToGoal,
            aiWallsLeft = currentGameState.aiWallsLeft,
            humanWallsLeft = currentGameState.humanWallsLeft,
            aiMobility = GetValidMoveCount(currentGameState.aiPosition, currentGameState.placedWalls),
            humanMobility = GetValidMoveCount(currentGameState.humanPosition, currentGameState.placedWalls),
            distanceAdvantage = humanDistanceToGoal - aiDistanceToGoal,
            wallAdvantage = currentGameState.aiWallsLeft - currentGameState.humanWallsLeft,
            isEarlyGame = (currentGameState.aiWallsLeft + currentGameState.humanWallsLeft) > 14,
            isMidGame = (currentGameState.aiWallsLeft + currentGameState.humanWallsLeft) > 6 && (currentGameState.aiWallsLeft + currentGameState.humanWallsLeft) <= 14,
            isEndGame = (currentGameState.aiWallsLeft + currentGameState.humanWallsLeft) <= 6
        };
    }
    
    /// <summary>
    /// L·ªçc moves theo strategy t·ª´ Decision Tree
    /// </summary>
    List<AIMove> FilterMovesByStrategy(List<AIMove> moves, StrategyDecision strategy)
    {
        switch (strategy)
        {
            case StrategyDecision.Aggressive:
                // ∆Øu ti√™n wall placement ƒë·ªÉ block ƒë·ªëi th·ªß
                return moves.Where(m => m.moveType == MoveType.WallPlacement || 
                                      (m.moveType == MoveType.Movement && IsAggressiveMove(m))).ToList();
                
            case StrategyDecision.Defensive:
                // ∆Øu ti√™n di chuy·ªÉn v·ªÅ ƒë√≠ch
                return moves.Where(m => m.moveType == MoveType.Movement || 
                                      (m.moveType == MoveType.WallPlacement && IsDefensiveWall(m))).ToList();
                
            case StrategyDecision.Balanced:
                // Gi·ªØ nguy√™n t·∫•t c·∫£ moves
                return moves;
                
            case StrategyDecision.Blocking:
                // Ch·ªâ wall placement
                var wallMoves = moves.Where(m => m.moveType == MoveType.WallPlacement).ToList();
                return wallMoves.Count > 0 ? wallMoves : moves.Where(m => m.moveType == MoveType.Movement).ToList();
                
            default:
                return moves;
        }
    }
    
    /// <summary>
    /// Ki·ªÉm tra move c√≥ ph·∫£i aggressive kh√¥ng
    /// </summary>
    bool IsAggressiveMove(AIMove move)
    {
        if (move.moveType != MoveType.Movement) return false;
        
        // Move h∆∞·ªõng v·ªÅ ph√≠a ƒë·ªëi th·ªß
        Vector2Int direction = move.targetPosition - currentGameState.aiPosition;
        Vector2Int towardEnemy = currentGameState.humanPosition - currentGameState.aiPosition;
        
        return Vector2.Dot(direction, towardEnemy) > 0;
    }
    
    /// <summary>
    /// Ki·ªÉm tra wall c√≥ ph·∫£i defensive kh√¥ng
    /// </summary>
    bool IsDefensiveWall(AIMove move)
    {
        if (move.moveType != MoveType.WallPlacement) return false;
        
        // Wall g·∫ßn AI position h∆°n
        float distanceToAI = Vector2.Distance(move.targetPosition, currentGameState.aiPosition);
        float distanceToHuman = Vector2.Distance(move.targetPosition, currentGameState.humanPosition);
        
        return distanceToAI < distanceToHuman;
    }
    
    /// <summary>
    /// Ch·ªçn move s·ª≠ d·ª•ng Entropy Analysis
    /// </summary>
    AIMove SelectMoveWithEntropyAnalysis(List<(AIMove move, float score)> moveScores, float bestScore)
    {
        // L·ªçc moves c√≥ score g·∫ßn v·ªõi best score
        float threshold = bestScore * 0.95f; // 95% c·ªßa best score
        var topMoves = moveScores.Where(ms => ms.score >= threshold).ToList();
        
        if (topMoves.Count <= 1)
        {
            return topMoves.FirstOrDefault().move;
        }
        
        // T√≠nh entropy cho selection
        float entropy = CalculateEntropyForMoves(topMoves);
        Debug.Log($"üé≤ Move entropy: {entropy:F3}, threshold: {entropyThreshold:F3}");
        
        if (entropy > entropyThreshold)
        {
            // High entropy -> s·ª≠ d·ª•ng softmax distribution
            return SelectMoveWithSoftmax(topMoves);
        }
        else
        {
            // Low entropy -> ch·ªçn best move
            return topMoves.OrderByDescending(ms => ms.score).First().move;
        }
    }
    
    /// <summary>
    /// T√≠nh entropy cho danh s√°ch moves
    /// </summary>
    float CalculateEntropyForMoves(List<(AIMove move, float score)> moves)
    {
        if (moves.Count <= 1) return 0f;
        
        // Normalize scores th√†nh probabilities
        float totalScore = moves.Sum(ms => Mathf.Exp(ms.score / softmaxTemperature));
        var probabilities = moves.Select(ms => Mathf.Exp(ms.score / softmaxTemperature) / totalScore).ToList();
        
        // T√≠nh entropy: H = -Œ£(p * log(p))
        float entropy = 0f;
        foreach (float p in probabilities)
        {
            if (p > 0f)
            {
                entropy -= p * Mathf.Log(p, 2f);
            }
        }
        
        return entropy;
    }
    
    /// <summary>
    /// Ch·ªçn move b·∫±ng softmax distribution
    /// </summary>
    AIMove SelectMoveWithSoftmax(List<(AIMove move, float score)> moves)
    {
        // T√≠nh softmax probabilities
        float totalExp = moves.Sum(ms => Mathf.Exp(ms.score / softmaxTemperature));
        var probabilities = moves.Select(ms => Mathf.Exp(ms.score / softmaxTemperature) / totalExp).ToList();
        
        // Random selection theo ph√¢n ph·ªëi
        float randomValue = Random.Range(0f, 1f);
        float cumulativeProbability = 0f;
        
        for (int i = 0; i < moves.Count; i++)
        {
            cumulativeProbability += probabilities[i];
            if (randomValue <= cumulativeProbability)
            {
                Debug.Log($"üéØ Softmax selected move {i} with probability {probabilities[i]:F3}");
                return moves[i].move;
            }
        }
        
        // Fallback
        return moves.Last().move;
    }
    
    /// <summary>
    /// C·∫≠p nh·∫≠t history cho entropy analysis
    /// </summary>
    void UpdateMoveHistory(AIMove selectedMove, float score)
    {
        // C·∫≠p nh·∫≠t recent scores
        recentMoveScores.Add(score);
        if (recentMoveScores.Count > 10) // Gi·ªØ 10 scores g·∫ßn nh·∫•t
        {
            recentMoveScores.RemoveAt(0);
        }
        
        // C·∫≠p nh·∫≠t move pattern history
        if (selectedMove != null)
        {
            string pattern = $"{selectedMove.moveType}_{selectedMove.targetPosition.x}_{selectedMove.targetPosition.y}";
            if (movePatternHistory.ContainsKey(pattern))
            {
                movePatternHistory[pattern]++;
            }
            else
            {
                movePatternHistory[pattern] = 1;
            }
            
            // Gi·ªõi h·∫°n size c·ªßa history
            if (movePatternHistory.Count > 50)
            {
                var oldestPattern = movePatternHistory.OrderBy(kvp => kvp.Value).First().Key;
                movePatternHistory.Remove(oldestPattern);
            }
        }
        
        // Debug logging
        LogAIPerformance(selectedMove, score);
    }
    
    /// <summary>
    /// Log AI performance metrics
    /// </summary>
    void LogAIPerformance(AIMove selectedMove, float score)
    {
        if (recentMoveScores.Count >= 5)
        {
            float avgScore = recentMoveScores.Average();
            float scoreVariance = recentMoveScores.Select(s => (s - avgScore) * (s - avgScore)).Average();
            float entropy = CalculateEntropyForMoves(recentMoveScores.Select(s => (selectedMove, s)).ToList());
            
            Debug.Log($"üìä AI Performance - Avg Score: {avgScore:F2}, Variance: {scoreVariance:F2}, Entropy: {entropy:F3}");
            Debug.Log($"üéØ Current Move: {selectedMove?.moveType} -> {selectedMove?.targetPosition}, Score: {score:F2}");
            
            // Pattern analysis
            if (movePatternHistory.Count > 5)
            {
                var topPatterns = movePatternHistory.OrderByDescending(kvp => kvp.Value).Take(3);
                Debug.Log($"üîÑ Top Move Patterns: {string.Join(", ", topPatterns.Select(kvp => $"{kvp.Key}({kvp.Value})"))}");
            }
        }
    }

    // 4. Th√™m bi·∫øn l∆∞u state/action tr∆∞·ªõc ƒë√≥ ƒë·ªÉ c·∫≠p nh·∫≠t Q
    private GameState lastState;
    private AIMove lastAction;

    // 5. Sau khi AI th·ª±c hi·ªán xong n∆∞·ªõc ƒëi, c·∫≠p nh·∫≠t Q-table (v√≠ d·ª• trong ExecuteAIMove ho·∫∑c sau khi nh·∫≠n reward)
    void OnAIMoveResult(float reward, GameState nextState)
    {
        if (useQLearning && lastState != null && lastAction != null)
        {
            List<AIMove> nextMoves = GeneratePossibleMoves(nextState, true);
            qAgent.UpdateQ(lastState, lastAction, reward, nextState, nextMoves);
        }
    }

    // 6. Sau m·ªói v√°n, l∆∞u Q-table v√† decay epsilon
    void OnGameEnd()
    {
        if (useQLearning && allowQTableSaving)
        {
            // Decay epsilon sau m·ªói episode (ch·ªâ khi ƒëang training)
            qAgent.DecayEpsilonEpisode();
            
            // Save Q-table to best available path (ch·ªâ khi cho ph√©p)
            string actualQTablePath = GetQTablePath();
            qAgent.SaveQTable(actualQTablePath);
            
            Debug.Log("üéÆ Game ended - Epsilon decayed and Q-table saved");
        }
        else if (useQLearning && !allowQTableSaving)
        {
            Debug.Log("üéÆ Game ended - Using trained model, no saving to preserve Q-table");
        }
    }

    // ==== STATIC METHODS FOR SELF-PLAY Q-LEARNING ====
    public static List<AIMove> GeneratePossibleMovesStatic(GameState state, bool isAIPlayer)
    {
        // Copy logic t·ª´ GeneratePossibleMoves, nh∆∞ng ch·ªâ d√πng state truy·ªÅn v√†o
        var moves = new List<AIMove>();
        Vector2Int pos = isAIPlayer ? state.aiPosition : state.humanPosition;
        int wallsLeft = isAIPlayer ? state.aiWallsLeft : state.humanWallsLeft;
        // Di chuy·ªÉn: 4 h∆∞·ªõng c∆° b·∫£n
        Vector2Int[] dirs = { Vector2Int.up, Vector2Int.down, Vector2Int.left, Vector2Int.right };
        foreach (var dir in dirs)
        {
            Vector2Int next = pos + dir;
            if (IsValidPositionStatic(next) && !IsBlockedByWallInfoStatic(pos, next, state.placedWalls))
            {
                moves.Add(new AIMove { moveType = MoveType.Movement, targetPosition = next });
            }
        }
        // ƒê·∫∑t t∆∞·ªùng (n·∫øu c√≤n)
        if (wallsLeft > 0)
        {
            for (int x = 0; x < 9; x++)
            {
                for (int y = 0; y < 9; y++)
                {
                    foreach (bool isHorizontal in new[] { true, false })
                    {
                        var wallMove = new AIMove { moveType = MoveType.WallPlacement, targetPosition = new Vector2Int(x, y), isHorizontalWall = isHorizontal };
                        if (IsValidWallPlacementStatic(wallMove, state))
                            moves.Add(wallMove);
                    }
                }
            }
        }
        return moves;
    }

    public static GameState ApplyMoveStatic(GameState state, AIMove move, bool isAIPlayer)
    {
        // T·∫°o b·∫£n sao state m·ªõi
        GameState newState = state.Clone();
        if (move.moveType == MoveType.Movement)
        {
            if (isAIPlayer)
                newState.aiPosition = move.targetPosition;
            else
                newState.humanPosition = move.targetPosition;
        }
        else if (move.moveType == MoveType.WallPlacement)
        {
            newState.placedWalls.Add(new WallInfo { position = new Vector3(move.targetPosition.x, 0, move.targetPosition.y), isHorizontal = move.isHorizontalWall });
            if (isAIPlayer)
                newState.aiWallsLeft--;
            else
                newState.humanWallsLeft--;
        }
        return newState;
    }

    public static bool IsGameOverStatic(GameState state)
    {
        // AI th·∫Øng n·∫øu aiPosition.y == 0, Human th·∫Øng n·∫øu humanPosition.y == 8
        if (state.aiPosition.y == 0) return true;
        if (state.humanPosition.y == 8) return true;
        return false;
    }

    // Helper static cho self-play
    public static bool IsValidPositionStatic(Vector2Int pos)
    {
        return pos.x >= 0 && pos.x < 9 && pos.y >= 0 && pos.y < 9;
    }
    public static bool IsBlockedByWallInfoStatic(Vector2Int from, Vector2Int to, List<WallInfo> walls)
    {
        // ƒê∆°n gi·∫£n: ch·ªâ ki·ªÉm tra c√≥ t∆∞·ªùng ƒë√∫ng v·ªã tr√≠ gi·ªØa from-to kh√¥ng
        foreach (var wall in walls)
        {
            if (IsWallBlockingMovementStatic(from, to, wall))
                return true;
        }
        return false;
    }
    public static bool IsWallBlockingMovementStatic(Vector2Int from, Vector2Int to, WallInfo wall)
    {
        // Logic ƒë∆°n gi·∫£n: ki·ªÉm tra n·∫øu wall n·∫±m gi·ªØa from v√† to
        if (wall.isHorizontal)
        {
            // T∆∞·ªùng ngang ch·∫∑n di chuy·ªÉn d·ªçc
            if ((from.y < to.y || to.y < from.y) && from.x == wall.position.x && (from.y == wall.position.z || to.y == wall.position.z))
                return true;
        }
        else
        {
            // T∆∞·ªùng d·ªçc ch·∫∑n di chuy·ªÉn ngang
            if ((from.x < to.x || to.x < from.x) && from.y == wall.position.z && (from.x == wall.position.x || to.x == wall.position.x))
                return true;
        }
        return false;
    }
    public static bool IsValidWallPlacementStatic(AIMove wallMove, GameState state)
    {
        // Kh√¥ng ƒë·∫∑t tr√πng v·ªã tr√≠ t∆∞·ªùng ƒë√£ c√≥
        foreach (var wall in state.placedWalls)
        {
            if (wall.position.x == wallMove.targetPosition.x && wall.position.z == wallMove.targetPosition.y && wall.isHorizontal == wallMove.isHorizontalWall)
                return false;
        }
        // C√≥ th·ªÉ b·ªï sung ki·ªÉm tra ch·∫∑n ƒë∆∞·ªùng ƒëi h·ª£p l·ªá ·ªü ƒë√¢y
        return true;
    }
    
    /// <summary>
    /// Debug Q-Learning system
    /// </summary>
    [ContextMenu("Debug Q-Learning System")]
    public void DebugQLearningSystem()
    {
        Debug.Log("=== QUORIDOR AI Q-LEARNING DEBUG ===");
        Debug.Log($"üéØ Player ID: {playerID}");
        Debug.Log($"üîß Use Q-Learning: {useQLearning}");
        Debug.Log($"üìÇ Q-Table Path: {qTablePath}");
        Debug.Log($"üìÇ Full Q-Table Path: {System.IO.Path.GetFullPath(qTablePath)}");
        
        if (qAgent != null)
        {
            qAgent.DebugQTable();
        }
        else
        {
            Debug.LogError("‚ùå Q-Learning Agent is null!");
        }
    }

    /// <summary>
    /// Test Q-table serialization
    /// </summary>
    [ContextMenu("Test Q-Table Serialization")]
    public void TestQTableSerialization()
    {
        Debug.Log("üß™ Testing Q-table serialization...");
        
        if (qAgent != null)
        {
            qAgent.TestSerialization();
        }
        else
        {
            Debug.LogError("‚ùå Q-Learning Agent is null!");
        }
    }

    /// <summary>
    /// Force reload Q-table
    /// </summary>
    [ContextMenu("Reload Q-Table")]
    public void ReloadQTable()
    {
        Debug.Log("üîÑ Force reloading Q-table...");
        
        if (qAgent != null)
        {
            string actualQTablePath = GetQTablePath();
            Debug.Log($"üîÑ Using path: {actualQTablePath}");
            
            // Load Q-table
            qAgent.LoadQTable(actualQTablePath);
            
            // Check if loaded successfully and update flags
            int qTableSize = qAgent.GetQTableSize();
            Debug.Log($"üìä Q-table loaded with {qTableSize} states");
            
            if (qTableSize > 1000) // If has > 1000 states (trained)
            {
                qAgent.SetTrainedEpsilon(0.1f, 1000, 1000); // 10% exploration, trained
                qAgent.InitializeWithoutEpsilonReset(); // Preserve trained epsilon
                isTrainedModel = true; // Mark as trained model
                allowQTableSaving = false; // Don't save to avoid overwriting
                Debug.Log($"üéì Detected trained Q-table with exploitation mode (Œµ=0.1)");
                Debug.Log($"üîí Q-Learning will NOT retrain - using existing knowledge");
                Debug.Log($"üö´ Q-table saving DISABLED to preserve trained model");
            }
            else
            {
                // If no trained data, initialize normal
                qAgent.Initialize();
                isTrainedModel = false;
                allowQTableSaving = true; // Allow saving when training
                Debug.Log($"üÜï Starting fresh Q-Learning training");
                Debug.Log($"üíæ Q-table saving ENABLED for training");
            }
        }
        else
        {
            Debug.LogError("‚ùå Q-Learning Agent is null!");
        }
    }

    /// <summary>
    /// Save current Q-table
    /// </summary>
    [ContextMenu("Save Q-Table")]
    public void SaveCurrentQTable()
    {
        Debug.Log("üíæ Saving Q-table...");
        
        if (qAgent != null)
        {
            string actualQTablePath = GetQTablePath();
            qAgent.SaveQTable(actualQTablePath);
        }
        else
        {
            Debug.LogError("‚ùå Q-Learning Agent is null!");
        }
    }
    
    /// <summary>
    /// Reset epsilon v·ªÅ initial value
    /// </summary>
    [ContextMenu("Reset Epsilon")]
    public void ResetEpsilon()
    {
        Debug.Log("üîÑ Resetting epsilon to initial value...");
        
        if (qAgent != null)
        {
            qAgent.ResetEpsilon();
        }
        else
        {
            Debug.LogError("‚ùå Q-Learning Agent is null!");
        }
    }
    
    /// <summary>
    /// Force decay epsilon m·ªôt episode
    /// </summary>
    [ContextMenu("Force Epsilon Decay")]
    public void ForceEpsilonDecay()
    {
        Debug.Log("üìâ Forcing epsilon decay...");
        
        if (qAgent != null)
        {
            qAgent.DecayEpsilonEpisode();
        }
        else
        {
            Debug.LogError("‚ùå Q-Learning Agent is null!");
        }
    }
    
    /// <summary>
    /// Enable training mode (allows Q-table saving)
    /// </summary>
    [ContextMenu("Enable Training Mode")]
    public void EnableTrainingMode()
    {
        allowQTableSaving = true;
        isTrainedModel = false;
        Debug.Log("üéì Training mode ENABLED - Q-table will be saved");
    }
    
    /// <summary>
    /// Disable training mode (preserve trained model)
    /// </summary>
    [ContextMenu("Disable Training Mode")]
    public void DisableTrainingMode()
    {
        allowQTableSaving = false;
        isTrainedModel = true;
        Debug.Log("üîí Training mode DISABLED - Q-table will be preserved");
    }
    
    /// <summary>
    /// Check current Q-Learning status
    /// </summary>
    [ContextMenu("Check Q-Learning Status")]
    public void CheckQLearningStatus()
    {
        Debug.Log("=== Q-LEARNING STATUS ===");
        Debug.Log($"üéØ Use Q-Learning: {useQLearning}");
        Debug.Log($"üìä Q-table size: {qAgent?.GetQTableSize() ?? 0} states");
        Debug.Log($"üéì Is trained model: {isTrainedModel}");
        Debug.Log($"üíæ Allow Q-table saving: {allowQTableSaving}");
        
        if (qAgent != null)
        {
            var epsilonInfo = qAgent.GetEpsilonInfo();
            Debug.Log($"üé≤ Current epsilon: {epsilonInfo.currentEpsilon:F3}");
            Debug.Log($"üìà Training step: {epsilonInfo.currentStep}");
            Debug.Log($"üìä Training episode: {epsilonInfo.currentEpisode}");
        }
    }
}

// ========== DATA STRUCTURES ==========

/// <summary>
/// Tr·∫°ng th√°i game cho AI
/// </summary>
[System.Serializable]
public class GameState
{
    public Vector2Int aiPosition;
    public Vector2Int humanPosition;
    public List<WallInfo> placedWalls = new List<WallInfo>();
    public int aiWallsLeft = 10;
    public int humanWallsLeft = 10;
    
    public GameState Clone()
    {
        var clone = new GameState();
        clone.aiPosition = aiPosition;
        clone.humanPosition = humanPosition;
        clone.placedWalls = new List<WallInfo>(placedWalls);
        clone.aiWallsLeft = aiWallsLeft;
        clone.humanWallsLeft = humanWallsLeft;
        return clone;
    }
}

/// <summary>
/// Th√¥ng tin wall cho AI
/// </summary>
[System.Serializable]
public class WallInfo
{
    public Vector3 position;
    public bool isHorizontal;
}

/// <summary>
/// AI move
/// </summary>
[System.Serializable]
public class AIMove
{
    public MoveType moveType;
    public Vector2Int targetPosition;
    public bool isHorizontalWall;
}

/// <summary>
/// Lo·∫°i n∆∞·ªõc ƒëi
/// </summary>
public enum MoveType
{
    Movement,
    WallPlacement
}

/// <summary>
/// A* Node
/// </summary>
public class AStarNode
{
    public Vector2Int position;
    public float gCost; // Distance from start
    public float hCost; // Distance to goal
    public float fCost; // Total cost
    
    public AStarNode(Vector2Int pos, float g, float h)
    {
        position = pos;
        gCost = g;
        hCost = h;
        fCost = g + h;
    }
}

// ========== DECISION TREE & ENTROPY STRUCTURES ==========

/// <summary>
/// Strategy decisions t·ª´ Decision Tree
/// </summary>
public enum StrategyDecision
{
    Aggressive,    // T·∫•n c√¥ng, ƒë·∫∑t wall block ƒë·ªëi th·ªß
    Defensive,     // Ph√≤ng th·ªß, di chuy·ªÉn v·ªÅ ƒë√≠ch
    Balanced,      // C√¢n b·∫±ng gi·ªØa t·∫•n c√¥ng v√† ph√≤ng th·ªß
    Blocking       // Chuy√™n block b·∫±ng wall
}

/// <summary>
/// Features cho Decision Tree
/// </summary>
[System.Serializable]
public class DecisionFeatures
{
    public int aiDistanceToGoal;
    public int humanDistanceToGoal;
    public int aiWallsLeft;
    public int humanWallsLeft;
    public int aiMobility;
    public int humanMobility;
    public int distanceAdvantage; // humanDistance - aiDistance
    public int wallAdvantage;     // aiWalls - humanWalls
    public bool isEarlyGame;
    public bool isMidGame;
    public bool isEndGame;
}

/// <summary>
/// Decision Tree Classifier cho Quoridor AI
/// </summary>
public class DecisionTreeClassifier
{
    private List<DecisionNode> nodes;
    
    public void Initialize()
    {
        // Kh·ªüi t·∫°o c√¢y quy·∫øt ƒë·ªãnh v·ªõi c√°c rules
        nodes = new List<DecisionNode>();
        BuildDecisionTree();
    }
    
    void BuildDecisionTree()
    {
        // Rule 1: Early Game - Balanced approach
        nodes.Add(new DecisionNode
        {
            condition = features => features.isEarlyGame,
            decision = StrategyDecision.Balanced,
            priority = 1
        });
        
        // Rule 2: End Game - Focus on reaching goal
        nodes.Add(new DecisionNode
        {
            condition = features => features.isEndGame,
            decision = StrategyDecision.Defensive,
            priority = 3
        });
        
        // Rule 3: AI is ahead - Block opponent
        nodes.Add(new DecisionNode
        {
            condition = features => features.distanceAdvantage > 2,
            decision = StrategyDecision.Blocking,
            priority = 2
        });
        
        // Rule 4: AI is behind - Aggressive push
        nodes.Add(new DecisionNode
        {
            condition = features => features.distanceAdvantage < -1,
            decision = StrategyDecision.Aggressive,
            priority = 2
        });
        
        // Rule 5: Low mobility - Defensive
        nodes.Add(new DecisionNode
        {
            condition = features => features.aiMobility <= 1,
            decision = StrategyDecision.Defensive,
            priority = 2
        });
        
        // Rule 6: High wall advantage - Use walls
        nodes.Add(new DecisionNode
        {
            condition = features => features.wallAdvantage > 3,
            decision = StrategyDecision.Blocking,
            priority = 2
        });
        
        // Rule 7: Low walls left - Conservative
        nodes.Add(new DecisionNode
        {
            condition = features => features.aiWallsLeft <= 2,
            decision = StrategyDecision.Defensive,
            priority = 2
        });
    }
    
    public StrategyDecision Predict(DecisionFeatures features)
    {
        // T√¨m rule c√≥ priority cao nh·∫•t th·ªèa m√£n
        var matchingNodes = nodes.Where(node => node.condition(features))
                                .OrderByDescending(node => node.priority)
                                .ToList();
        
        if (matchingNodes.Count > 0)
        {
            return matchingNodes.First().decision;
        }
        
        // Default: Balanced
        return StrategyDecision.Balanced;
    }
}

/// <summary>
/// Node trong Decision Tree
/// </summary>
public class DecisionNode
{
    public System.Func<DecisionFeatures, bool> condition;
    public StrategyDecision decision;
    public int priority;
}


