================================================================================
                    AI QUORIDOR GAME PROJECT - THU·∫¨T TO√ÅN AI CHI TI·∫æT
================================================================================

D·ª± √°n: AI Quoridor Game v·ªõi Machine Learning v√† Multi-Platform Integration
Ng√†y t·∫°o: 25/07/2025
T√°c gi·∫£: [Student Name]
Platform: Unity 3D
Language: C#

================================================================================
                            M·ª§C L·ª§C
================================================================================

1. T·ªîNG QUAN KI·∫æN TR√öC AI
2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
4. SPECIALIZED AI STYLES SYSTEM
5. DECISION TREE CLASSIFIER
6. ENTROPY ANALYSIS & SOFTMAX
7. TRAINING SYSTEM & SELF-PLAY
8. AI TESTING FRAMEWORK
9. HYBRID AI ARCHITECTURE
10. PERFORMANCE OPTIMIZATION

================================================================================
                        1. T·ªîNG QUAN KI·∫æN TR√öC AI
================================================================================

D·ª± √°n AI Quoridor s·ª≠ d·ª•ng m·ªôt ki·∫øn tr√∫c AI hybrid ti√™n ti·∫øn, k·∫øt h·ª£p:

1.1 CORE ALGORITHMS:
- Q-Learning (Reinforcement Learning) - Thu·∫≠t to√°n ch√≠nh
- Minimax + Alpha-Beta Pruning - Thu·∫≠t to√°n fallback
- Decision Tree Classifier - Ph√¢n t√≠ch ng·ªØ c·∫£nh game
- Entropy Analysis - ƒê√°nh gi√° ƒëa d·∫°ng move

1.2 SPECIALIZED SYSTEMS:
- 6 AI Personality Styles v·ªõi training ri√™ng bi·ªát
- Self-Play Training Pipeline t·ª± ƒë·ªông
- Real-time Performance Testing Framework
- Dynamic Algorithm Switching

1.3 TECHNICAL FOUNDATION:
- State Space: 13,118+ unique game states
- Action Space: Movement + Wall placement combinations
- Training Method: Self-play v·ªõi 1000 episodes
- Learning Rate: Œ± = 0.1, Discount Factor: Œ≥ = 0.99

================================================================================
                    2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
================================================================================

2.1 GI·ªöI THI·ªÜU Q-LEARNING:
Q-Learning l√† thu·∫≠t to√°n Reinforcement Learning kh√¥ng c·∫ßn model, h·ªçc th√¥ng qua
t∆∞∆°ng t√°c v·ªõi m√¥i tr∆∞·ªùng ƒë·ªÉ t·ªëi ∆∞u h√≥a policy. Trong Quoridor, AI h·ªçc c√°ch
ch·ªçn moves t·ªëi ∆∞u d·ª±a tr√™n kinh nghi·ªám t·ª´ h√†ng ng√†n game.

2.2 C√îNG TH·ª®C TO√ÅN H·ªåC:
Q(s,a) = Q(s,a) + Œ±[r + Œ≥*max(Q(s',a')) - Q(s,a)]

Trong ƒë√≥:
- Q(s,a): Q-value c·ªßa state s v·ªõi action a
- Œ±: Learning rate (0.1) - t·ªëc ƒë·ªô h·ªçc
- r: Immediate reward t·ª´ action
- Œ≥: Discount factor (0.99) - t·∫ßm quan tr·ªçng future rewards
- s': Next state sau khi th·ª±c hi·ªán action
- max(Q(s',a')): Maximum Q-value c·ªßa next state

2.3 IMPLEMENTATION CHI TI·∫æT:

```csharp
public class QLearningAgent
{
    // Core Q-Table: Dictionary l∆∞u tr·ªØ Q-values
    private Dictionary<string, Dictionary<string, float>> qTable;
    
    // Hyperparameters
    private float learningRate = 0.1f;      // T·ªëc ƒë·ªô h·ªçc
    private float discountFactor = 0.99f;   // Discount future rewards
    private float epsilon = 0.995f;         // Exploration rate
    
    // Q-Learning Update Implementation
    public void UpdateQValue(string state, string action, float reward, string nextState)
    {
        // L·∫•y Q-value hi·ªán t·∫°i
        float currentQ = GetQValue(state, action);
        
        // T√¨m max Q-value c·ªßa next state
        float maxNextQ = GetMaxQValue(nextState);
        
        // √Åp d·ª•ng c√¥ng th·ª©c Q-Learning
        float newQ = currentQ + learningRate * (reward + discountFactor * maxNextQ - currentQ);
        
        // C·∫≠p nh·∫≠t Q-table
        SetQValue(state, action, newQ);
    }
}
```

2.4 STATE ENCODING:
Game state ƒë∆∞·ª£c encode th√†nh string duy nh·∫•t:
"aiPos_x,aiPos_y-humanPos_x,humanPos_y-aiWalls-humanWalls-wallsList"

V√≠ d·ª•: "4,8-4,0-9-10-7,4,False|5,6,True"
- AI ·ªü (4,8), Human ·ªü (4,0)
- AI c√≥ 9 walls, Human c√≥ 10 walls
- Walls ƒë√£ ƒë·∫∑t: (7,4,False), (5,6,True)

2.5 ACTION ENCODING:
Actions ƒë∆∞·ª£c encode th√†nh string:
- Movement: "M:x,y" (v√≠ d·ª•: "M:4,7")
- Wall: "W:x,y,horizontal" (v√≠ d·ª•: "W:3,5,True")

2.6 EPSILON-GREEDY STRATEGY:
```csharp
public AIMove ChooseAction(GameState state, List<AIMove> possibleMoves)
{
    // Exploration vs Exploitation trade-off
    if (Random.Range(0f, 1f) < epsilon)
    {
        // EXPLORATION: Ch·ªçn random ƒë·ªÉ kh√°m ph√°
        return possibleMoves[Random.Range(0, possibleMoves.Count)];
    }
    else
    {
        // EXPLOITATION: Ch·ªçn action c√≥ Q-value cao nh·∫•t
        return GetBestAction(state, possibleMoves);
    }
}
```

2.7 EPSILON DECAY:
Epsilon gi·∫£m d·∫ßn t·ª´ 0.995 ‚Üí 0.1 qua training:
- Ban ƒë·∫ßu: High exploration (99.5%)
- Cu·ªëi training: High exploitation (90%)

2.8 REWARD SYSTEM:
- Win game: +1000
- Lose game: -1000
- Move toward goal: +10
- Block opponent: +50
- Waste move: -5
- Strategic position: +15

================================================================================
                3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
================================================================================

3.1 GI·ªöI THI·ªÜU MINIMAX:
Minimax l√† thu·∫≠t to√°n game theory c·ªï ƒëi·ªÉn, t√¨m ki·∫øm move t·ªëi ∆∞u b·∫±ng c√°ch
d·ª± ƒëo√°n tr∆∞·ªõc nhi·ªÅu moves. Alpha-Beta pruning t·ªëi ∆∞u hi·ªáu su·∫•t b·∫±ng c√°ch
lo·∫°i b·ªè c√°c nh√°nh kh√¥ng c·∫ßn thi·∫øt.

3.2 NGUY√äN L√ù HO·∫†T ƒê·ªòNG:
- Maximizing Player (AI): T·ªëi ƒëa h√≥a score
- Minimizing Player (Human): T·ªëi thi·ªÉu h√≥a score AI
- Search depth: 4 levels (configurable)
- Pruning: Lo·∫°i b·ªè nh√°nh kh√¥ng promising

3.3 IMPLEMENTATION:

```csharp
float Minimax(GameState state, int depth, bool isMaximizing, float alpha, float beta)
{
    // Base case: Depth = 0 ho·∫∑c game over
    if (depth == 0 || IsGameOver(state))
        return EvaluatePosition(state);
    
    if (isMaximizing) // AI turn
    {
        float maxEval = float.NegativeInfinity;
        foreach (var move in GeneratePossibleMoves(state, true))
        {
            GameState newState = ApplyMove(state, move, true);
            float eval = Minimax(newState, depth - 1, false, alpha, beta);
            maxEval = Mathf.Max(maxEval, eval);
            alpha = Mathf.Max(alpha, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return maxEval;
    }
    else // Human turn
    {
        float minEval = float.PositiveInfinity;
        foreach (var move in GeneratePossibleMoves(state, false))
        {
            GameState newState = ApplyMove(state, move, false);
            float eval = Minimax(newState, depth - 1, true, alpha, beta);
            minEval = Mathf.Min(minEval, eval);
            beta = Mathf.Min(beta, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return minEval;
    }
}
```

3.4 EVALUATION FUNCTION:
Position evaluation d·ª±a tr√™n multiple factors:

```csharp
float EvaluatePosition(GameState state)
{
    float score = 0f;
    
    // Distance to goal (c√†ng g·∫ßn c√†ng t·ªët)
    int aiDistance = GetShortestPath(state.aiPosition, goalRow);
    int humanDistance = GetShortestPath(state.humanPosition, goalRow);
    score += (humanDistance - aiDistance) * 100;
    
    // Wall advantage
    score += (state.aiWallsLeft - state.humanWallsLeft) * 50;
    
    // Mobility (s·ªë moves c√≥ th·ªÉ)
    int aiMobility = GetValidMoves(state.aiPosition).Count;
    int humanMobility = GetValidMoves(state.humanPosition).Count;
    score += (aiMobility - humanMobility) * 20;
    
    // Center control
    score += GetCenterControlScore(state) * 30;
    
    return score;
}
```

3.5 ALPHA-BETA PRUNING HI·ªÜU QU·∫¢:
- Reduces search space t·ª´ O(b^d) xu·ªëng O(b^(d/2))
- Best case: 50% reduction in nodes evaluated
- Ordering moves by promising gi√∫p pruning hi·ªáu qu·∫£ h∆°n

================================================================================
                        4. SPECIALIZED AI STYLES SYSTEM
================================================================================

4.1 CONCEPT:
6 AI personalities v·ªõi training v√† behavior ri√™ng bi·ªát, m·ªói style c√≥:
- Ri√™ng Q-table file
- Custom reward structure
- Specific move prioritization
- Unique decision making logic

4.2 AI STYLES CHI TI·∫æT:

üî• AGGRESSIVE STYLE:
- M·ª•c ti√™u: T·∫•n c√¥ng v√† ch·∫∑n ƒë·ªëi th·ªß
- Wall Usage: 70% chance
- Reward Structure:
  * blockingOpponentReward = 50 * intensity
  * wallPlacementReward = 30 * intensity
  * defensiveWallPenalty = -20 * intensity
- Q-table: qtable_aggressive.json

üõ°Ô∏è DEFENSIVE STYLE:
- M·ª•c ti√™u: Ph√≤ng th·ªß v√† di chuy·ªÉn an to√†n
- Wall Usage: 50% chance
- Reward Structure:
  * defensiveWallReward = 35 * intensity
  * movementReward = 20 * intensity
  * aggressiveMovePenalty = -15 * intensity
- Q-table: qtable_defensive.json

üß± WALL MASTER STYLE:
- M·ª•c ti√™u: Chuy√™n gia ƒë·∫∑t t∆∞·ªùng v√† t·∫°o m√™ cung
- Wall Usage: 90% chance
- Reward Structure:
  * wallPlacementReward = 60 * intensity
  * strategicWallReward = 40 * intensity
  * wallComboReward = 50 * intensity
  * movementWithoutWallPenalty = -25 * intensity
- Q-table: qtable_wallmaster.json

‚ö° SPEED RUNNER STYLE:
- M·ª•c ti√™u: ƒê·∫°t ƒë√≠ch nhanh nh·∫•t
- Wall Usage: 10% chance
- Reward Structure:
  * movementReward = 40 * intensity
  * progressToGoalReward = 50 * intensity
  * quickWinBonus = 100 * intensity
  * wallPlacementPenalty = -20 * intensity
- Q-table: qtable_speedrunner.json

üß† TACTICAL STYLE:
- M·ª•c ti√™u: L·ª£i th·∫ø v·ªã tr√≠ v√† k·∫ø ho·∫°ch d√†i h·∫°n
- Wall Usage: Variable (strategic)
- Reward Structure:
  * tacticalMoveReward = 45 * intensity
  * positionAdvantageReward = 35 * intensity
  * mobilityReward = 25 * intensity
  * patternRecognitionBonus = 40 * intensity
- Q-table: qtable_tactical.json

‚öñÔ∏è BALANCED STYLE:
- M·ª•c ti√™u: C√¢n b·∫±ng t·∫•t c·∫£ aspects
- Wall Usage: 30% chance
- Reward Structure: Default balanced values
- Q-table: qtable.json

4.3 STYLE SWITCHING MECHANISM:

```csharp
public void SwitchToStyle(PlayStyle newStyle)
{
    // Save current Q-table
    if (allowQTableSaving) qAgent.SaveQTable(GetStyleSpecificQTablePath());
    
    // Load new style Q-table
    specializedStyle = newStyle;
    qAgent.LoadQTable(GetStyleSpecificQTablePath());
    
    // Reconfigure parameters
    ConfigureStyleSpecificTraining();
}
```

4.4 STYLE INTENSITY:
Parameter t·ª´ 0.0 ‚Üí 1.0 control m·ª©c ƒë·ªô chuy√™n h√≥a:
- 0.0: Balanced behavior
- 0.5: Moderate specialization
- 1.0: Extreme specialization

================================================================================
                        5. DECISION TREE CLASSIFIER
================================================================================

5.1 PURPOSE:
Decision Tree ph√¢n t√≠ch game context ƒë·ªÉ ch·ªçn strategy ph√π h·ª£p, d·ª±a tr√™n:
- Game phase (Early/Mid/End)
- Position advantage
- Resource advantage
- Mobility situation

5.2 FEATURE EXTRACTION:

```csharp
public struct DecisionFeatures
{
    public int aiDistanceToGoal;      // AI distance to goal line
    public int humanDistanceToGoal;   // Human distance to goal line
    public int aiWallsLeft;           // AI walls remaining
    public int humanWallsLeft;        // Human walls remaining
    public int aiMobility;            // AI possible moves
    public int humanMobility;         // Human possible moves
    public float distanceAdvantage;   // Human - AI distance
    public float wallAdvantage;       // AI - Human walls
    public bool isEarlyGame;          // > 15 moves estimated remaining
    public bool isMidGame;            // 5-15 moves remaining
    public bool isEndGame;            // < 5 moves remaining
}
```

5.3 DECISION LOGIC:

```csharp
public AIStrategy Predict(DecisionFeatures features)
{
    // Early Game: Play conservatively
    if (features.isEarlyGame)
        return AIStrategy.Balanced;
    
    // AI has significant lead: Focus on blocking
    if (features.distanceAdvantage > 2)
        return AIStrategy.Blocking;
    
    // AI is behind: Be aggressive
    if (features.distanceAdvantage < -1)
        return AIStrategy.Aggressive;
    
    // End game with wall advantage: Play defensively
    if (features.isEndGame && features.wallAdvantage > 2)
        return AIStrategy.Defensive;
    
    // Close game: Balanced approach
    return AIStrategy.Balanced;
}
```

5.4 STRATEGY OUTCOMES:
- Balanced: Equal weight to movement v√† walls
- Blocking: Prioritize moves that increase opponent path
- Aggressive: Focus on attacking positions
- Defensive: Protect own position and path

================================================================================
                        6. ENTROPY ANALYSIS & SOFTMAX
================================================================================

6.1 ENTROPY CONCEPT:
Entropy ƒëo ƒë·ªô ƒëa d·∫°ng/uncertainty c·ªßa move scores. High entropy = nhi·ªÅu moves
c√≥ scores t∆∞∆°ng ƒë∆∞∆°ng, low entropy = m·ªôt move r√µ r√†ng t·ªët nh·∫•t.

6.2 ENTROPY CALCULATION:

```csharp
float CalculateEntropy(List<float> moveScores)
{
    // Normalize th√†nh probabilities
    float total = moveScores.Sum();
    var probabilities = moveScores.Select(s => s / total);
    
    // Calculate Shannon entropy
    float entropy = 0f;
    foreach (float p in probabilities)
    {
        if (p > 0f)
            entropy -= p * Mathf.Log(p, 2f);
    }
    
    return entropy; // Range: 0 (deterministic) to log2(n) (uniform)
}
```

6.3 SOFTMAX DISTRIBUTION:

```csharp
float[] CalculateSoftmax(List<float> scores, float temperature)
{
    float[] probabilities = new float[scores.Count];
    float sum = scores.Sum(s => Mathf.Exp(s / temperature));
    
    for (int i = 0; i < scores.Count; i++)
    {
        probabilities[i] = Mathf.Exp(scores[i] / temperature) / sum;
    }
    
    return probabilities;
}
```

6.4 TEMPERATURE PARAMETER:
- Low temperature (0.1): Sharp distribution, focus on best moves
- High temperature (2.0): Flat distribution, more exploration
- Default temperature (1.0): Balanced exploitation/exploration

6.5 ADAPTIVE MOVE SELECTION:

```csharp
public AIMove ChooseActionWithEntropy(GameState state, List<AIMove> possibleMoves)
{
    var moveScores = EvaluateAllMoves(state, possibleMoves);
    float entropy = CalculateEntropy(moveScores);
    
    if (entropy > entropyThreshold) // High diversity
    {
        // Use softmax for probabilistic selection
        var probabilities = CalculateSoftmax(moveScores, softmaxTemperature);
        return SelectMoveByProbability(possibleMoves, probabilities);
    }
    else // Clear best move
    {
        // Use deterministic best move
        return GetBestScoredMove(possibleMoves, moveScores);
    }
}
```

================================================================================
                        7. TRAINING SYSTEM & SELF-PLAY
================================================================================

7.1 SELF-PLAY CONCEPT:
AI h·ªçc b·∫±ng c√°ch ch∆°i v·ªõi ch√≠nh n√≥ h√†ng ng√†n l·∫ßn, gradually improve performance
th√¥ng qua trial v√† error.

7.2 TRAINING PIPELINE:

```csharp
IEnumerator RunSelfPlayTraining()
{
    for (int episode = 1; episode <= maxEpisodes; episode++)
    {
        // Reset game state
        ResetGameForTraining();
        
        while (!IsGameOver())
        {
            // Get current state
            GameState currentState = GetCurrentGameState();
            List<AIMove> possibleMoves = GetPossibleMoves();
            
            // Choose action using Œµ-greedy
            AIMove chosenMove = qLearningAgent.ChooseAction(currentState, possibleMoves);
            
            // Execute move
            ExecuteMove(chosenMove);
            
            // Calculate reward
            float reward = CalculateReward(currentState, chosenMove);
            
            // Update Q-table
            GameState newState = GetCurrentGameState();
            qLearningAgent.UpdateQValue(
                EncodeState(currentState),
                EncodeAction(chosenMove),
                reward,
                EncodeState(newState)
            );
            
            yield return new WaitForEndOfFrame();
        }
        
        // Epsilon decay
        qLearningAgent.DecayEpsilon();
        
        // Save progress periodically
        if (episode % 100 == 0)
        {
            qLearningAgent.SaveQTable();
        }
    }
}
```

7.3 REWARD ENGINEERING:
Critical cho Q-Learning success:

```csharp
float CalculateReward(GameState state, AIMove move)
{
    float reward = 0f;
    
    // Game outcome rewards
    if (IsWin(state)) reward += 1000f;
    if (IsLoss(state)) reward -= 1000f;
    
    // Progress rewards
    if (move.moveType == MoveType.Movement)
    {
        if (IsMoveTowardGoal(move)) reward += 20f;
        else reward -= 5f;
    }
    
    // Strategic rewards
    if (move.moveType == MoveType.WallPlacement)
    {
        if (IsWallBlockingOpponent(move)) reward += 50f;
        if (IsWallStrategic(move)) reward += 30f;
        if (IsWallWasted(move)) reward -= 25f;
    }
    
    // Style-specific rewards
    reward += GetStyleSpecificReward(state, move);
    
    return reward;
}
```

7.4 TRAINING METRICS:
- Episodes: 1000 standard
- Duration: ~15 minutes
- Convergence: Usually around episode 800
- Q-table size: 13,118+ states at completion
- Final epsilon: 0.1 (90% exploitation)

================================================================================
                        8. AI TESTING FRAMEWORK
================================================================================

8.1 TESTING PHILOSOPHY:
Comprehensive testing ensures AI quality v√† reliability across different
game scenarios v√† edge cases.

8.2 TEST CATEGORIES:

A) DECISION TREE TESTS:
```csharp
void TestDecisionTreeStrategies()
{
    // Test Early Game
    var earlyGameFeatures = new DecisionFeatures
    {
        aiDistanceToGoal = 8, humanDistanceToGoal = 8,
        isEarlyGame = true
    };
    var strategy = decisionTree.Predict(earlyGameFeatures);
    Assert.AreEqual(AIStrategy.Balanced, strategy);
    
    // Test AI Leading
    var leadingFeatures = new DecisionFeatures
    {
        distanceAdvantage = 3, // AI leading
        isMidGame = true
    };
    strategy = decisionTree.Predict(leadingFeatures);
    Assert.AreEqual(AIStrategy.Blocking, strategy);
}
```

B) ENTROPY ANALYSIS TESTS:
```csharp
void TestEntropyAnalysis()
{
    // High entropy case (diverse scores)
    var highEntropyScores = new List<float> { 100f, 98f, 97f, 95f, 94f };
    float entropy = CalculateTestEntropy(highEntropyScores);
    Assert.IsTrue(entropy > 0.5f);
    
    // Low entropy case (clear winner)
    var lowEntropyScores = new List<float> { 100f, 50f, 50f, 50f, 50f };
    entropy = CalculateTestEntropy(lowEntropyScores);
    Assert.IsTrue(entropy < 0.5f);
}
```

C) SOFTMAX DISTRIBUTION TESTS:
```csharp
void TestSoftmaxDistribution()
{
    var testScores = new List<float> { 100f, 98f, 95f, 90f };
    float temperature = 1.0f;
    
    var probabilities = CalculateSoftmax(testScores, temperature);
    
    // Verify probabilities sum to 1
    float sum = probabilities.Sum();
    Assert.AreEqual(1.0f, sum, 0.001f);
    
    // Verify highest score gets highest probability
    int maxIndex = Array.IndexOf(probabilities, probabilities.Max());
    Assert.AreEqual(0, maxIndex);
}
```

D) PERFORMANCE BENCHMARKS:
- Move generation time: <100ms
- Q-Learning decision: <50ms
- Minimax decision (depth 4): <200ms
- Memory usage: <256MB

8.3 AUTOMATED TESTING:
Tests run automatically ƒë·ªÉ ensure code quality:
- Unit tests cho individual algorithms
- Integration tests cho AI system interactions
- Performance tests cho response times
- Regression tests ƒë·ªÉ prevent quality degradation

================================================================================
                        9. HYBRID AI ARCHITECTURE
================================================================================

9.1 CONCEPT:
Hybrid system k·∫øt h·ª£p strengths c·ªßa multiple algorithms:
- Q-Learning: Data-driven, learns from experience
- Minimax: Logical, guaranteed optimal given evaluation
- Decision Tree: Fast, context-aware switching
- Entropy: Adaptive, handles uncertainty

9.2 ALGORITHM SELECTION LOGIC:

```csharp
public AIMove GetBestMove()
{
    UpdateGameState();
    List<AIMove> possibleMoves = GeneratePossibleMoves();
    
    // Primary: Use Q-Learning if trained model available
    if (useQLearning && qAgent.IsTrainedModel())
    {
        Debug.Log($"üß† Using {specializedStyle} Q-Learning");
        return qAgent.ChooseAction(currentGameState, possibleMoves);
    }
    
    // Fallback: Use Minimax for guaranteed reasonable play
    else
    {
        Debug.Log($"üîç Using {specializedStyle} Minimax");
        return FindBestMoveWithMinimax();
    }
}
```

9.3 SEAMLESS SWITCHING:
- No interruption in gameplay
- Maintains consistent API
- Preserves game state integrity
- Logs algorithm used for debugging

9.4 BENEFITS:
- Robustness: Fallback ensures AI never fails
- Performance: Q-Learning for trained scenarios, Minimax for edge cases
- Flexibility: Can switch algorithms based on situation
- Development: Easy to test individual components

================================================================================
                        10. PERFORMANCE OPTIMIZATION
================================================================================

10.1 Q-TABLE OPTIMIZATION:
- Sparse representation: Only store visited states
- Compression: String interning for state keys
- Lazy loading: Load Q-table on demand
- Periodic saving: Every 100 training episodes

10.2 MINIMAX OPTIMIZATION:
- Alpha-Beta pruning: 50% search reduction
- Move ordering: Promising moves first
- Iterative deepening: Gradual depth increase
- Transposition tables: Cache evaluated positions

10.3 MEMORY MANAGEMENT:
- Object pooling cho GameState objects
- String caching cho repeated state/action encodings
- Garbage collection optimization
- Memory profiling v√† monitoring

10.4 MULTITHREADING CONSIDERATIONS:
- Q-Learning training: Background thread
- Move calculation: Main thread (Unity requirement)
- File I/O: Async operations where possible
- Thread-safe collections cho shared data

10.5 CACHING STRATEGIES:
- Path finding results
- Move generation results
- Evaluation function results
- Feature extraction results

================================================================================
                            K·∫æT LU·∫¨N
================================================================================

D·ª± √°n AI Quoridor Project th·ªÉ hi·ªán m·ªôt implementation comprehensive c·ªßa modern
game AI techniques:

TECHNICAL INNOVATIONS:
‚úÖ Hybrid Q-Learning + Minimax architecture
‚úÖ 6 specialized AI personalities v·ªõi separate training
‚úÖ Advanced decision making v·ªõi entropy analysis
‚úÖ Comprehensive testing v√† validation framework
‚úÖ Professional development tools v√† debugging

EDUCATIONAL VALUE:
‚úÖ Practical machine learning implementation
‚úÖ Traditional game AI algorithms
‚úÖ Modern software engineering practices
‚úÖ Performance optimization techniques
‚úÖ Automated testing methodologies

FUTURE ENHANCEMENTS:
üöÄ Deep Q-Learning (DQN) integration
üöÄ Multi-agent reinforcement learning
üöÄ Neural network-based evaluation functions
üöÄ Online learning v√† adaptation
üöÄ Transfer learning between game variants

D·ª± √°n n√†y demonstrating successful integration c·ªßa classical AI methods v·ªõi
modern machine learning, t·∫°o ra m·ªôt intelligent, adaptive, v√† highly
interactive gaming experience.

================================================================================
                            APPENDIX
================================================================================

FILES LI√äN QUAN:
- Assets/scripts/QLearningAgent.cs - Q-Learning implementation
- Assets/scripts/QuoridorAI.cs - Main AI controller
- Assets/scripts/DecisionTreeClassifier.cs - Decision tree logic
- Assets/scripts/QuoridorSelfPlayTrainer.cs - Training system
- Assets/scripts/AITestScript.cs - Testing framework
- Assets/scripts/Editor/ForceRetrainAI.cs - Development tools

Q-TABLE FILES:
- StreamingAssets/qtable.json - Balanced style
- StreamingAssets/qtable_aggressive.json - Aggressive style
- StreamingAssets/qtable_defensive.json - Defensive style
- StreamingAssets/qtable_wallmaster.json - Wall Master style
- StreamingAssets/qtable_speedrunner.json - Speed Runner style
- StreamingAssets/qtable_tactical.json - Tactical style

DOCUMENTATION:
- AI_ALGORITHMS_DETAILED_DESCRIPTION.txt - This file
- README.md - Project overview
- API_INTEGRATION_GUIDE.md - Voice/Chat integration
- TRAINING_GUIDE.md - AI training instructions

================================================================================
================================================================================
                    AI QUORIDOR GAME PROJECT - THU·∫¨T TO√ÅN AI CHI TI·∫æT
================================================================================

D·ª± √°n: AI Quoridor Game v·ªõi Machine Learning v√† Multi-Platform Integration
Ng√†y t·∫°o: 25/07/2025
T√°c gi·∫£: [Student Name]
Platform: Unity 3D
Language: C#

================================================================================
                            M·ª§C L·ª§C
================================================================================

1. T·ªîNG QUAN KI·∫æN TR√öC AI
2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
4. SPECIALIZED AI STYLES SYSTEM
5. DECISION TREE CLASSIFIER
6. ENTROPY ANALYSIS & SOFTMAX
7. TRAINING SYSTEM & SELF-PLAY
8. AI TESTING FRAMEWORK
9. HYBRID AI ARCHITECTURE
10. PERFORMANCE OPTIMIZATION

================================================================================
                        1. T·ªîNG QUAN KI·∫æN TR√öC AI
================================================================================

D·ª± √°n AI Quoridor s·ª≠ d·ª•ng m·ªôt ki·∫øn tr√∫c AI hybrid ti√™n ti·∫øn, k·∫øt h·ª£p:

1.1 CORE ALGORITHMS:
- Q-Learning (Reinforcement Learning) - Thu·∫≠t to√°n ch√≠nh
- Minimax + Alpha-Beta Pruning - Thu·∫≠t to√°n fallback
- Decision Tree Classifier - Ph√¢n t√≠ch ng·ªØ c·∫£nh game
- Entropy Analysis - ƒê√°nh gi√° ƒëa d·∫°ng move

1.2 SPECIALIZED SYSTEMS:
- 6 AI Personality Styles v·ªõi training ri√™ng bi·ªát
- Self-Play Training Pipeline t·ª± ƒë·ªông
- Real-time Performance Testing Framework
- Dynamic Algorithm Switching

1.3 TECHNICAL FOUNDATION:
- State Space: 13,118+ unique game states
- Action Space: Movement + Wall placement combinations
- Training Method: Self-play v·ªõi 1000 episodes
- Learning Rate: Œ± = 0.1, Discount Factor: Œ≥ = 0.99

================================================================================
                    2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
================================================================================

2.1 GI·ªöI THI·ªÜU Q-LEARNING:
Q-Learning l√† thu·∫≠t to√°n Reinforcement Learning kh√¥ng c·∫ßn model, h·ªçc th√¥ng qua
t∆∞∆°ng t√°c v·ªõi m√¥i tr∆∞·ªùng ƒë·ªÉ t·ªëi ∆∞u h√≥a policy. Trong Quoridor, AI h·ªçc c√°ch
ch·ªçn moves t·ªëi ∆∞u d·ª±a tr√™n kinh nghi·ªám t·ª´ h√†ng ng√†n game.

2.2 C√îNG TH·ª®C TO√ÅN H·ªåC:
Q(s,a) = Q(s,a) + Œ±[r + Œ≥*max(Q(s',a')) - Q(s,a)]

Trong ƒë√≥:
- Q(s,a): Q-value c·ªßa state s v·ªõi action a
- Œ±: Learning rate (0.1) - t·ªëc ƒë·ªô h·ªçc
- r: Immediate reward t·ª´ action
- Œ≥: Discount factor (0.99) - t·∫ßm quan tr·ªçng future rewards
- s': Next state sau khi th·ª±c hi·ªán action
- max(Q(s',a')): Maximum Q-value c·ªßa next state

2.3 IMPLEMENTATION CHI TI·∫æT:

```csharp
public class QLearningAgent
{
    // Core Q-Table: Dictionary l∆∞u tr·ªØ Q-values
    private Dictionary<string, Dictionary<string, float>> qTable;
    
    // Hyperparameters
    private float learningRate = 0.1f;      // T·ªëc ƒë·ªô h·ªçc
    private float discountFactor = 0.99f;   // Discount future rewards
    private float epsilon = 0.995f;         // Exploration rate
    
    // Q-Learning Update Implementation
    public void UpdateQValue(string state, string action, float reward, string nextState)
    {
        // L·∫•y Q-value hi·ªán t·∫°i
        float currentQ = GetQValue(state, action);
        
        // T√¨m max Q-value c·ªßa next state
        float maxNextQ = GetMaxQValue(nextState);
        
        // √Åp d·ª•ng c√¥ng th·ª©c Q-Learning
        float newQ = currentQ + learningRate * (reward + discountFactor * maxNextQ - currentQ);
        
        // C·∫≠p nh·∫≠t Q-table
        SetQValue(state, action, newQ);
    }
}
```

2.4 STATE ENCODING:
Game state ƒë∆∞·ª£c encode th√†nh string duy nh·∫•t:
"aiPos_x,aiPos_y-humanPos_x,humanPos_y-aiWalls-humanWalls-wallsList"

V√≠ d·ª•: "4,8-4,0-9-10-7,4,False|5,6,True"
- AI ·ªü (4,8), Human ·ªü (4,0)
- AI c√≥ 9 walls, Human c√≥ 10 walls
- Walls ƒë√£ ƒë·∫∑t: (7,4,False), (5,6,True)

2.5 ACTION ENCODING:
Actions ƒë∆∞·ª£c encode th√†nh string:
- Movement: "M:x,y" (v√≠ d·ª•: "M:4,7")
- Wall: "W:x,y,horizontal" (v√≠ d·ª•: "W:3,5,True")

2.6 EPSILON-GREEDY STRATEGY:
```csharp
public AIMove ChooseAction(GameState state, List<AIMove> possibleMoves)
{
    // Exploration vs Exploitation trade-off
    if (Random.Range(0f, 1f) < epsilon)
    {
        // EXPLORATION: Ch·ªçn random ƒë·ªÉ kh√°m ph√°
        return possibleMoves[Random.Range(0, possibleMoves.Count)];
    }
    else
    {
        // EXPLOITATION: Ch·ªçn action c√≥ Q-value cao nh·∫•t
        return GetBestAction(state, possibleMoves);
    }
}
```

2.7 EPSILON DECAY:
Epsilon gi·∫£m d·∫ßn t·ª´ 0.995 ‚Üí 0.1 qua training:
- Ban ƒë·∫ßu: High exploration (99.5%)
- Cu·ªëi training: High exploitation (90%)

2.8 REWARD SYSTEM:
- Win game: +1000
- Lose game: -1000
- Move toward goal: +10
- Block opponent: +50
- Waste move: -5
- Strategic position: +15

================================================================================
                3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
================================================================================

3.1 GI·ªöI THI·ªÜU MINIMAX:
Minimax l√† thu·∫≠t to√°n game theory c·ªï ƒëi·ªÉn, t√¨m ki·∫øm move t·ªëi ∆∞u b·∫±ng c√°ch
d·ª± ƒëo√°n tr∆∞·ªõc nhi·ªÅu moves. Alpha-Beta pruning t·ªëi ∆∞u hi·ªáu su·∫•t b·∫±ng c√°ch
lo·∫°i b·ªè c√°c nh√°nh kh√¥ng c·∫ßn thi·∫øt.

3.2 NGUY√äN L√ù HO·∫†T ƒê·ªòNG:
- Maximizing Player (AI): T·ªëi ƒëa h√≥a score
- Minimizing Player (Human): T·ªëi thi·ªÉu h√≥a score AI
- Search depth: 4 levels (configurable)
- Pruning: Lo·∫°i b·ªè nh√°nh kh√¥ng promising

3.3 IMPLEMENTATION:

```csharp
float Minimax(GameState state, int depth, bool isMaximizing, float alpha, float beta)
{
    // Base case: Depth = 0 ho·∫∑c game over
    if (depth == 0 || IsGameOver(state))
        return EvaluatePosition(state);
    
    if (isMaximizing) // AI turn
    {
        float maxEval = float.NegativeInfinity;
        foreach (var move in GeneratePossibleMoves(state, true))
        {
            GameState newState = ApplyMove(state, move, true);
            float eval = Minimax(newState, depth - 1, false, alpha, beta);
            maxEval = Mathf.Max(maxEval, eval);
            alpha = Mathf.Max(alpha, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return maxEval;
    }
    else // Human turn
    {
        float minEval = float.PositiveInfinity;
        foreach (var move in GeneratePossibleMoves(state, false))
        {
            GameState newState = ApplyMove(state, move, false);
            float eval = Minimax(newState, depth - 1, true, alpha, beta);
            minEval = Mathf.Min(minEval, eval);
            beta = Mathf.Min(beta, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return minEval;
    }
}
```

3.4 EVALUATION FUNCTION:
Position evaluation d·ª±a tr√™n multiple factors:

```csharp
float EvaluatePosition(GameState state)
{
    float score = 0f;
    
    // Distance to goal (c√†ng g·∫ßn c√†ng t·ªët)
    int aiDistance = GetShortestPath(state.aiPosition, goalRow);
    int humanDistance = GetShortestPath(state.humanPosition, goalRow);
    score += (humanDistance - aiDistance) * 100;
    
    // Wall advantage
    score += (state.aiWallsLeft - state.humanWallsLeft) * 50;
    
    // Mobility (s·ªë moves c√≥ th·ªÉ)
    int aiMobility = GetValidMoves(state.aiPosition).Count;
    int humanMobility = GetValidMoves(state.humanPosition).Count;
    score += (aiMobility - humanMobility) * 20;
    
    // Center control
    score += GetCenterControlScore(state) * 30;
    
    return score;
}
```

3.5 ALPHA-BETA PRUNING HI·ªÜU QU·∫¢:
- Reduces search space t·ª´ O(b^d) xu·ªëng O(b^(d/2))
- Best case: 50% reduction in nodes evaluated
- Ordering moves by promising gi√∫p pruning hi·ªáu qu·∫£ h∆°n

================================================================================
                        4. SPECIALIZED AI STYLES SYSTEM
================================================================================

4.1 CONCEPT:
6 AI personalities v·ªõi training v√† behavior ri√™ng bi·ªát, m·ªói style c√≥:
- Ri√™ng Q-table file
- Custom reward structure
- Specific move prioritization
- Unique decision making logic

4.2 AI STYLES CHI TI·∫æT:

üî• AGGRESSIVE STYLE:
- M·ª•c ti√™u: T·∫•n c√¥ng v√† ch·∫∑n ƒë·ªëi th·ªß
- Wall Usage: 70% chance
- Reward Structure:
  * blockingOpponentReward = 50 * intensity
  * wallPlacementReward = 30 * intensity
  * defensiveWallPenalty = -20 * intensity
- Q-table: qtable_aggressive.json

üõ°Ô∏è DEFENSIVE STYLE:
- M·ª•c ti√™u: Ph√≤ng th·ªß v√† di chuy·ªÉn an to√†n
- Wall Usage: 50% chance
- Reward Structure:
  * defensiveWallReward = 35 * intensity
  * movementReward = 20 * intensity
  * aggressiveMovePenalty = -15 * intensity
- Q-table: qtable_defensive.json

üß± WALL MASTER STYLE:
- M·ª•c ti√™u: Chuy√™n gia ƒë·∫∑t t∆∞·ªùng v√† t·∫°o m√™ cung
- Wall Usage: 90% chance
- Reward Structure:
  * wallPlacementReward = 60 * intensity
  * strategicWallReward = 40 * intensity
  * wallComboReward = 50 * intensity
  * movementWithoutWallPenalty = -25 * intensity
- Q-table: qtable_wallmaster.json

‚ö° SPEED RUNNER STYLE:
- M·ª•c ti√™u: ƒê·∫°t ƒë√≠ch nhanh nh·∫•t
- Wall Usage: 10% chance
- Reward Structure:
  * movementReward = 40 * intensity
  * progressToGoalReward = 50 * intensity
  * quickWinBonus = 100 * intensity
  * wallPlacementPenalty = -20 * intensity
- Q-table: qtable_speedrunner.json

üß† TACTICAL STYLE:
- M·ª•c ti√™u: L·ª£i th·∫ø v·ªã tr√≠ v√† k·∫ø ho·∫°ch d√†i h·∫°n
- Wall Usage: Variable (strategic)
- Reward Structure:
  * tacticalMoveReward = 45 * intensity
  * positionAdvantageReward = 35 * intensity
  * mobilityReward = 25 * intensity
  * patternRecognitionBonus = 40 * intensity
- Q-table: qtable_tactical.json

‚öñÔ∏è BALANCED STYLE:
- M·ª•c ti√™u: C√¢n b·∫±ng t·∫•t c·∫£ aspects
- Wall Usage: 30% chance
- Reward Structure: Default balanced values
- Q-table: qtable.json

4.3 STYLE SWITCHING MECHANISM:

```csharp
public void SwitchToStyle(PlayStyle newStyle)
{
    // Save current Q-table
    if (allowQTableSaving) qAgent.SaveQTable(GetStyleSpecificQTablePath());
    
    // Load new style Q-table
    specializedStyle = newStyle;
    qAgent.LoadQTable(GetStyleSpecificQTablePath());
    
    // Reconfigure parameters
    ConfigureStyleSpecificTraining();
}
```

4.4 STYLE INTENSITY:
Parameter t·ª´ 0.0 ‚Üí 1.0 control m·ª©c ƒë·ªô chuy√™n h√≥a:
- 0.0: Balanced behavior
- 0.5: Moderate specialization
- 1.0: Extreme specialization

================================================================================
                        5. DECISION TREE CLASSIFIER
================================================================================

5.1 PURPOSE:
Decision Tree ph√¢n t√≠ch game context ƒë·ªÉ ch·ªçn strategy ph√π h·ª£p, d·ª±a tr√™n:
- Game phase (Early/Mid/End)
- Position advantage
- Resource advantage
- Mobility situation

5.2 FEATURE EXTRACTION:

```csharp
public struct DecisionFeatures
{
    public int aiDistanceToGoal;      // AI distance to goal line
    public int humanDistanceToGoal;   // Human distance to goal line
    public int aiWallsLeft;           // AI walls remaining
    public int humanWallsLeft;        // Human walls remaining
    public int aiMobility;            // AI possible moves
    public int humanMobility;         // Human possible moves
    public float distanceAdvantage;   // Human - AI distance
    public float wallAdvantage;       // AI - Human walls
    public bool isEarlyGame;          // > 15 moves estimated remaining
    public bool isMidGame;            // 5-15 moves remaining
    public bool isEndGame;            // < 5 moves remaining
}
```

5.3 DECISION LOGIC:

```csharp
public AIStrategy Predict(DecisionFeatures features)
{
    // Early Game: Play conservatively
    if (features.isEarlyGame)
        return AIStrategy.Balanced;
    
    // AI has significant lead: Focus on blocking
    if (features.distanceAdvantage > 2)
        return AIStrategy.Blocking;
    
    // AI is behind: Be aggressive
    if (features.distanceAdvantage < -1)
        return AIStrategy.Aggressive;
    
    // End game with wall advantage: Play defensively
    if (features.isEndGame && features.wallAdvantage > 2)
        return AIStrategy.Defensive;
    
    // Close game: Balanced approach
    return AIStrategy.Balanced;
}
```

5.4 STRATEGY OUTCOMES:
- Balanced: Equal weight to movement v√† walls
- Blocking: Prioritize moves that increase opponent path
- Aggressive: Focus on attacking positions
- Defensive: Protect own position and path

================================================================================
                        6. ENTROPY ANALYSIS & SOFTMAX
================================================================================

6.1 ENTROPY CONCEPT:
Entropy ƒëo ƒë·ªô ƒëa d·∫°ng/uncertainty c·ªßa move scores. High entropy = nhi·ªÅu moves
c√≥ scores t∆∞∆°ng ƒë∆∞∆°ng, low entropy = m·ªôt move r√µ r√†ng t·ªët nh·∫•t.

6.2 ENTROPY CALCULATION:

```csharp
float CalculateEntropy(List<float> moveScores)
{
    // Normalize th√†nh probabilities
    float total = moveScores.Sum();
    var probabilities = moveScores.Select(s => s / total);
    
    // Calculate Shannon entropy
    float entropy = 0f;
    foreach (float p in probabilities)
    {
        if (p > 0f)
            entropy -= p * Mathf.Log(p, 2f);
    }
    
    return entropy; // Range: 0 (deterministic) to log2(n) (uniform)
}
```

6.3 SOFTMAX DISTRIBUTION:

```csharp
float[] CalculateSoftmax(List<float> scores, float temperature)
{
    float[] probabilities = new float[scores.Count];
    float sum = scores.Sum(s => Mathf.Exp(s / temperature));
    
    for (int i = 0; i < scores.Count; i++)
    {
        probabilities[i] = Mathf.Exp(scores[i] / temperature) / sum;
    }
    
    return probabilities;
}
```

6.4 TEMPERATURE PARAMETER:
- Low temperature (0.1): Sharp distribution, focus on best moves
- High temperature (2.0): Flat distribution, more exploration
- Default temperature (1.0): Balanced exploitation/exploration

6.5 ADAPTIVE MOVE SELECTION:

```csharp
public AIMove ChooseActionWithEntropy(GameState state, List<AIMove> possibleMoves)
{
    var moveScores = EvaluateAllMoves(state, possibleMoves);
    float entropy = CalculateEntropy(moveScores);
    
    if (entropy > entropyThreshold) // High diversity
    {
        // Use softmax for probabilistic selection
        var probabilities = CalculateSoftmax(moveScores, softmaxTemperature);
        return SelectMoveByProbability(possibleMoves, probabilities);
    }
    else // Clear best move
    {
        // Use deterministic best move
        return GetBestScoredMove(possibleMoves, moveScores);
    }
}
```

================================================================================
                        7. TRAINING SYSTEM & SELF-PLAY
================================================================================

7.1 SELF-PLAY CONCEPT:
AI h·ªçc b·∫±ng c√°ch ch∆°i v·ªõi ch√≠nh n√≥ h√†ng ng√†n l·∫ßn, gradually improve performance
th√¥ng qua trial v√† error.

7.2 TRAINING PIPELINE:

```csharp
IEnumerator RunSelfPlayTraining()
{
    for (int episode = 1; episode <= maxEpisodes; episode++)
    {
        // Reset game state
        ResetGameForTraining();
        
        while (!IsGameOver())
        {
            // Get current state
            GameState currentState = GetCurrentGameState();
            List<AIMove> possibleMoves = GetPossibleMoves();
            
            // Choose action using Œµ-greedy
            AIMove chosenMove = qLearningAgent.ChooseAction(currentState, possibleMoves);
            
            // Execute move
            ExecuteMove(chosenMove);
            
            // Calculate reward
            float reward = CalculateReward(currentState, chosenMove);
            
            // Update Q-table
            GameState newState = GetCurrentGameState();
            qLearningAgent.UpdateQValue(
                EncodeState(currentState),
                EncodeAction(chosenMove),
                reward,
                EncodeState(newState)
            );
            
            yield return new WaitForEndOfFrame();
        }
        
        // Epsilon decay
        qLearningAgent.DecayEpsilon();
        
        // Save progress periodically
        if (episode % 100 == 0)
        {
            qLearningAgent.SaveQTable();
        }
    }
}
```

7.3 REWARD ENGINEERING:
Critical cho Q-Learning success:

```csharp
float CalculateReward(GameState state, AIMove move)
{
    float reward = 0f;
    
    // Game outcome rewards
    if (IsWin(state)) reward += 1000f;
    if (IsLoss(state)) reward -= 1000f;
    
    // Progress rewards
    if (move.moveType == MoveType.Movement)
    {
        if (IsMoveTowardGoal(move)) reward += 20f;
        else reward -= 5f;
    }
    
    // Strategic rewards
    if (move.moveType == MoveType.WallPlacement)
    {
        if (IsWallBlockingOpponent(move)) reward += 50f;
        if (IsWallStrategic(move)) reward += 30f;
        if (IsWallWasted(move)) reward -= 25f;
    }
    
    // Style-specific rewards
    reward += GetStyleSpecificReward(state, move);
    
    return reward;
}
```

7.4 TRAINING METRICS:
- Episodes: 1000 standard
- Duration: ~15 minutes
- Convergence: Usually around episode 800
- Q-table size: 13,118+ states at completion
- Final epsilon: 0.1 (90% exploitation)

================================================================================
                        8. AI TESTING FRAMEWORK
================================================================================

8.1 TESTING PHILOSOPHY:
Comprehensive testing ensures AI quality v√† reliability across different
game scenarios v√† edge cases.

8.2 TEST CATEGORIES:

A) DECISION TREE TESTS:
```csharp
void TestDecisionTreeStrategies()
{
    // Test Early Game
    var earlyGameFeatures = new DecisionFeatures
    {
        aiDistanceToGoal = 8, humanDistanceToGoal = 8,
        isEarlyGame = true
    };
    var strategy = decisionTree.Predict(earlyGameFeatures);
    Assert.AreEqual(AIStrategy.Balanced, strategy);
    
    // Test AI Leading
    var leadingFeatures = new DecisionFeatures
    {
        distanceAdvantage = 3, // AI leading
        isMidGame = true
    };
    strategy = decisionTree.Predict(leadingFeatures);
    Assert.AreEqual(AIStrategy.Blocking, strategy);
}
```

B) ENTROPY ANALYSIS TESTS:
```csharp
void TestEntropyAnalysis()
{
    // High entropy case (diverse scores)
    var highEntropyScores = new List<float> { 100f, 98f, 97f, 95f, 94f };
    float entropy = CalculateTestEntropy(highEntropyScores);
    Assert.IsTrue(entropy > 0.5f);
    
    // Low entropy case (clear winner)
    var lowEntropyScores = new List<float> { 100f, 50f, 50f, 50f, 50f };
    entropy = CalculateTestEntropy(lowEntropyScores);
    Assert.IsTrue(entropy < 0.5f);
}
```

C) SOFTMAX DISTRIBUTION TESTS:
```csharp
void TestSoftmaxDistribution()
{
    var testScores = new List<float> { 100f, 98f, 95f, 90f };
    float temperature = 1.0f;
    
    var probabilities = CalculateSoftmax(testScores, temperature);
    
    // Verify probabilities sum to 1
    float sum = probabilities.Sum();
    Assert.AreEqual(1.0f, sum, 0.001f);
    
    // Verify highest score gets highest probability
    int maxIndex = Array.IndexOf(probabilities, probabilities.Max());
    Assert.AreEqual(0, maxIndex);
}
```

D) PERFORMANCE BENCHMARKS:
- Move generation time: <100ms
- Q-Learning decision: <50ms
- Minimax decision (depth 4): <200ms
- Memory usage: <256MB

8.3 AUTOMATED TESTING:
Tests run automatically ƒë·ªÉ ensure code quality:
- Unit tests cho individual algorithms
- Integration tests cho AI system interactions
- Performance tests cho response times
- Regression tests ƒë·ªÉ prevent quality degradation

================================================================================
                        9. HYBRID AI ARCHITECTURE
================================================================================

9.1 CONCEPT:
Hybrid system k·∫øt h·ª£p strengths c·ªßa multiple algorithms:
- Q-Learning: Data-driven, learns from experience
- Minimax: Logical, guaranteed optimal given evaluation
- Decision Tree: Fast, context-aware switching
- Entropy: Adaptive, handles uncertainty

9.2 ALGORITHM SELECTION LOGIC:

```csharp
public AIMove GetBestMove()
{
    UpdateGameState();
    List<AIMove> possibleMoves = GeneratePossibleMoves();
    
    // Primary: Use Q-Learning if trained model available
    if (useQLearning && qAgent.IsTrainedModel())
    {
        Debug.Log($"üß† Using {specializedStyle} Q-Learning");
        return qAgent.ChooseAction(currentGameState, possibleMoves);
    }
    
    // Fallback: Use Minimax for guaranteed reasonable play
    else
    {
        Debug.Log($"üîç Using {specializedStyle} Minimax");
        return FindBestMoveWithMinimax();
    }
}
```

9.3 SEAMLESS SWITCHING:
- No interruption in gameplay
- Maintains consistent API
- Preserves game state integrity
- Logs algorithm used for debugging

9.4 BENEFITS:
- Robustness: Fallback ensures AI never fails
- Performance: Q-Learning for trained scenarios, Minimax for edge cases
- Flexibility: Can switch algorithms based on situation
- Development: Easy to test individual components

================================================================================
                        10. PERFORMANCE OPTIMIZATION
================================================================================

10.1 Q-TABLE OPTIMIZATION:
- Sparse representation: Only store visited states
- Compression: String interning for state keys
- Lazy loading: Load Q-table on demand
- Periodic saving: Every 100 training episodes

10.2 MINIMAX OPTIMIZATION:
- Alpha-Beta pruning: 50% search reduction
- Move ordering: Promising moves first
- Iterative deepening: Gradual depth increase
- Transposition tables: Cache evaluated positions

10.3 MEMORY MANAGEMENT:
- Object pooling cho GameState objects
- String caching cho repeated state/action encodings
- Garbage collection optimization
- Memory profiling v√† monitoring

10.4 MULTITHREADING CONSIDERATIONS:
- Q-Learning training: Background thread
- Move calculation: Main thread (Unity requirement)
- File I/O: Async operations where possible
- Thread-safe collections cho shared data

10.5 CACHING STRATEGIES:
- Path finding results
- Move generation results
- Evaluation function results
- Feature extraction results

================================================================================
                            K·∫æT LU·∫¨N
================================================================================

D·ª± √°n AI Quoridor Project th·ªÉ hi·ªán m·ªôt implementation comprehensive c·ªßa modern
game AI techniques:

TECHNICAL INNOVATIONS:
‚úÖ Hybrid Q-Learning + Minimax architecture
‚úÖ 6 specialized AI personalities v·ªõi separate training
‚úÖ Advanced decision making v·ªõi entropy analysis
‚úÖ Comprehensive testing v√† validation framework
‚úÖ Professional development tools v√† debugging

EDUCATIONAL VALUE:
‚úÖ Practical machine learning implementation
‚úÖ Traditional game AI algorithms
‚úÖ Modern software engineering practices
‚úÖ Performance optimization techniques
‚úÖ Automated testing methodologies

FUTURE ENHANCEMENTS:
üöÄ Deep Q-Learning (DQN) integration
üöÄ Multi-agent reinforcement learning
üöÄ Neural network-based evaluation functions
üöÄ Online learning v√† adaptation
üöÄ Transfer learning between game variants

D·ª± √°n n√†y demonstrating successful integration c·ªßa classical AI methods v·ªõi
modern machine learning, t·∫°o ra m·ªôt intelligent, adaptive, v√† highly
interactive gaming experience.

================================================================================
                            APPENDIX
================================================================================

FILES LI√äN QUAN:
- Assets/scripts/QLearningAgent.cs - Q-Learning implementation
- Assets/scripts/QuoridorAI.cs - Main AI controller
- Assets/scripts/DecisionTreeClassifier.cs - Decision tree logic
- Assets/scripts/QuoridorSelfPlayTrainer.cs - Training system
- Assets/scripts/AITestScript.cs - Testing framework
- Assets/scripts/Editor/ForceRetrainAI.cs - Development tools

Q-TABLE FILES:
- StreamingAssets/qtable.json - Balanced style
- StreamingAssets/qtable_aggressive.json - Aggressive style
- StreamingAssets/qtable_defensive.json - Defensive style
- StreamingAssets/qtable_wallmaster.json - Wall Master style
- StreamingAssets/qtable_speedrunner.json - Speed Runner style
- StreamingAssets/qtable_tactical.json - Tactical style

DOCUMENTATION:
- AI_ALGORITHMS_DETAILED_DESCRIPTION.txt - This file
- README.md - Project overview
- API_INTEGRATION_GUIDE.md - Voice/Chat integration
- TRAINING_GUIDE.md - AI training instructions

================================================================================
================================================================================
                    AI QUORIDOR GAME PROJECT - THU·∫¨T TO√ÅN AI CHI TI·∫æT
================================================================================

D·ª± √°n: AI Quoridor Game v·ªõi Machine Learning v√† Multi-Platform Integration
Ng√†y t·∫°o: 25/07/2025
T√°c gi·∫£: [Student Name]
Platform: Unity 3D
Language: C#

================================================================================
                            M·ª§C L·ª§C
================================================================================

1. T·ªîNG QUAN KI·∫æN TR√öC AI
2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
4. SPECIALIZED AI STYLES SYSTEM
5. DECISION TREE CLASSIFIER
6. ENTROPY ANALYSIS & SOFTMAX
7. TRAINING SYSTEM & SELF-PLAY
8. AI TESTING FRAMEWORK
9. HYBRID AI ARCHITECTURE
10. PERFORMANCE OPTIMIZATION

================================================================================
                        1. T·ªîNG QUAN KI·∫æN TR√öC AI
================================================================================

D·ª± √°n AI Quoridor s·ª≠ d·ª•ng m·ªôt ki·∫øn tr√∫c AI hybrid ti√™n ti·∫øn, k·∫øt h·ª£p:

1.1 CORE ALGORITHMS:
- Q-Learning (Reinforcement Learning) - Thu·∫≠t to√°n ch√≠nh
- Minimax + Alpha-Beta Pruning - Thu·∫≠t to√°n fallback
- Decision Tree Classifier - Ph√¢n t√≠ch ng·ªØ c·∫£nh game
- Entropy Analysis - ƒê√°nh gi√° ƒëa d·∫°ng move

1.2 SPECIALIZED SYSTEMS:
- 6 AI Personality Styles v·ªõi training ri√™ng bi·ªát
- Self-Play Training Pipeline t·ª± ƒë·ªông
- Real-time Performance Testing Framework
- Dynamic Algorithm Switching

1.3 TECHNICAL FOUNDATION:
- State Space: 13,118+ unique game states
- Action Space: Movement + Wall placement combinations
- Training Method: Self-play v·ªõi 1000 episodes
- Learning Rate: Œ± = 0.1, Discount Factor: Œ≥ = 0.99

================================================================================
                    2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
================================================================================

2.1 GI·ªöI THI·ªÜU Q-LEARNING:
Q-Learning l√† thu·∫≠t to√°n Reinforcement Learning kh√¥ng c·∫ßn model, h·ªçc th√¥ng qua
t∆∞∆°ng t√°c v·ªõi m√¥i tr∆∞·ªùng ƒë·ªÉ t·ªëi ∆∞u h√≥a policy. Trong Quoridor, AI h·ªçc c√°ch
ch·ªçn moves t·ªëi ∆∞u d·ª±a tr√™n kinh nghi·ªám t·ª´ h√†ng ng√†n game.

2.2 C√îNG TH·ª®C TO√ÅN H·ªåC:
Q(s,a) = Q(s,a) + Œ±[r + Œ≥*max(Q(s',a')) - Q(s,a)]

Trong ƒë√≥:
- Q(s,a): Q-value c·ªßa state s v·ªõi action a
- Œ±: Learning rate (0.1) - t·ªëc ƒë·ªô h·ªçc
- r: Immediate reward t·ª´ action
- Œ≥: Discount factor (0.99) - t·∫ßm quan tr·ªçng future rewards
- s': Next state sau khi th·ª±c hi·ªán action
- max(Q(s',a')): Maximum Q-value c·ªßa next state

2.3 IMPLEMENTATION CHI TI·∫æT:

```csharp
public class QLearningAgent
{
    // Core Q-Table: Dictionary l∆∞u tr·ªØ Q-values
    private Dictionary<string, Dictionary<string, float>> qTable;
    
    // Hyperparameters
    private float learningRate = 0.1f;      // T·ªëc ƒë·ªô h·ªçc
    private float discountFactor = 0.99f;   // Discount future rewards
    private float epsilon = 0.995f;         // Exploration rate
    
    // Q-Learning Update Implementation
    public void UpdateQValue(string state, string action, float reward, string nextState)
    {
        // L·∫•y Q-value hi·ªán t·∫°i
        float currentQ = GetQValue(state, action);
        
        // T√¨m max Q-value c·ªßa next state
        float maxNextQ = GetMaxQValue(nextState);
        
        // √Åp d·ª•ng c√¥ng th·ª©c Q-Learning
        float newQ = currentQ + learningRate * (reward + discountFactor * maxNextQ - currentQ);
        
        // C·∫≠p nh·∫≠t Q-table
        SetQValue(state, action, newQ);
    }
}
```

2.4 STATE ENCODING:
Game state ƒë∆∞·ª£c encode th√†nh string duy nh·∫•t:
"aiPos_x,aiPos_y-humanPos_x,humanPos_y-aiWalls-humanWalls-wallsList"

V√≠ d·ª•: "4,8-4,0-9-10-7,4,False|5,6,True"
- AI ·ªü (4,8), Human ·ªü (4,0)
- AI c√≥ 9 walls, Human c√≥ 10 walls
- Walls ƒë√£ ƒë·∫∑t: (7,4,False), (5,6,True)

2.5 ACTION ENCODING:
Actions ƒë∆∞·ª£c encode th√†nh string:
- Movement: "M:x,y" (v√≠ d·ª•: "M:4,7")
- Wall: "W:x,y,horizontal" (v√≠ d·ª•: "W:3,5,True")

2.6 EPSILON-GREEDY STRATEGY:
```csharp
public AIMove ChooseAction(GameState state, List<AIMove> possibleMoves)
{
    // Exploration vs Exploitation trade-off
    if (Random.Range(0f, 1f) < epsilon)
    {
        // EXPLORATION: Ch·ªçn random ƒë·ªÉ kh√°m ph√°
        return possibleMoves[Random.Range(0, possibleMoves.Count)];
    }
    else
    {
        // EXPLOITATION: Ch·ªçn action c√≥ Q-value cao nh·∫•t
        return GetBestAction(state, possibleMoves);
    }
}
```

2.7 EPSILON DECAY:
Epsilon gi·∫£m d·∫ßn t·ª´ 0.995 ‚Üí 0.1 qua training:
- Ban ƒë·∫ßu: High exploration (99.5%)
- Cu·ªëi training: High exploitation (90%)

2.8 REWARD SYSTEM:
- Win game: +1000
- Lose game: -1000
- Move toward goal: +10
- Block opponent: +50
- Waste move: -5
- Strategic position: +15

================================================================================
                3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
================================================================================

3.1 GI·ªöI THI·ªÜU MINIMAX:
Minimax l√† thu·∫≠t to√°n game theory c·ªï ƒëi·ªÉn, t√¨m ki·∫øm move t·ªëi ∆∞u b·∫±ng c√°ch
d·ª± ƒëo√°n tr∆∞·ªõc nhi·ªÅu moves. Alpha-Beta pruning t·ªëi ∆∞u hi·ªáu su·∫•t b·∫±ng c√°ch
lo·∫°i b·ªè c√°c nh√°nh kh√¥ng c·∫ßn thi·∫øt.

3.2 NGUY√äN L√ù HO·∫†T ƒê·ªòNG:
- Maximizing Player (AI): T·ªëi ƒëa h√≥a score
- Minimizing Player (Human): T·ªëi thi·ªÉu h√≥a score AI
- Search depth: 4 levels (configurable)
- Pruning: Lo·∫°i b·ªè nh√°nh kh√¥ng promising

3.3 IMPLEMENTATION:

```csharp
float Minimax(GameState state, int depth, bool isMaximizing, float alpha, float beta)
{
    // Base case: Depth = 0 ho·∫∑c game over
    if (depth == 0 || IsGameOver(state))
        return EvaluatePosition(state);
    
    if (isMaximizing) // AI turn
    {
        float maxEval = float.NegativeInfinity;
        foreach (var move in GeneratePossibleMoves(state, true))
        {
            GameState newState = ApplyMove(state, move, true);
            float eval = Minimax(newState, depth - 1, false, alpha, beta);
            maxEval = Mathf.Max(maxEval, eval);
            alpha = Mathf.Max(alpha, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return maxEval;
    }
    else // Human turn
    {
        float minEval = float.PositiveInfinity;
        foreach (var move in GeneratePossibleMoves(state, false))
        {
            GameState newState = ApplyMove(state, move, false);
            float eval = Minimax(newState, depth - 1, true, alpha, beta);
            minEval = Mathf.Min(minEval, eval);
            beta = Mathf.Min(beta, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return minEval;
    }
}
```

3.4 EVALUATION FUNCTION:
Position evaluation d·ª±a tr√™n multiple factors:

```csharp
float EvaluatePosition(GameState state)
{
    float score = 0f;
    
    // Distance to goal (c√†ng g·∫ßn c√†ng t·ªët)
    int aiDistance = GetShortestPath(state.aiPosition, goalRow);
    int humanDistance = GetShortestPath(state.humanPosition, goalRow);
    score += (humanDistance - aiDistance) * 100;
    
    // Wall advantage
    score += (state.aiWallsLeft - state.humanWallsLeft) * 50;
    
    // Mobility (s·ªë moves c√≥ th·ªÉ)
    int aiMobility = GetValidMoves(state.aiPosition).Count;
    int humanMobility = GetValidMoves(state.humanPosition).Count;
    score += (aiMobility - humanMobility) * 20;
    
    // Center control
    score += GetCenterControlScore(state) * 30;
    
    return score;
}
```

3.5 ALPHA-BETA PRUNING HI·ªÜU QU·∫¢:
- Reduces search space t·ª´ O(b^d) xu·ªëng O(b^(d/2))
- Best case: 50% reduction in nodes evaluated
- Ordering moves by promising gi√∫p pruning hi·ªáu qu·∫£ h∆°n

================================================================================
                        4. SPECIALIZED AI STYLES SYSTEM
================================================================================

4.1 CONCEPT:
6 AI personalities v·ªõi training v√† behavior ri√™ng bi·ªát, m·ªói style c√≥:
- Ri√™ng Q-table file
- Custom reward structure
- Specific move prioritization
- Unique decision making logic

4.2 AI STYLES CHI TI·∫æT:

üî• AGGRESSIVE STYLE:
- M·ª•c ti√™u: T·∫•n c√¥ng v√† ch·∫∑n ƒë·ªëi th·ªß
- Wall Usage: 70% chance
- Reward Structure:
  * blockingOpponentReward = 50 * intensity
  * wallPlacementReward = 30 * intensity
  * defensiveWallPenalty = -20 * intensity
- Q-table: qtable_aggressive.json

üõ°Ô∏è DEFENSIVE STYLE:
- M·ª•c ti√™u: Ph√≤ng th·ªß v√† di chuy·ªÉn an to√†n
- Wall Usage: 50% chance
- Reward Structure:
  * defensiveWallReward = 35 * intensity
  * movementReward = 20 * intensity
  * aggressiveMovePenalty = -15 * intensity
- Q-table: qtable_defensive.json

üß± WALL MASTER STYLE:
- M·ª•c ti√™u: Chuy√™n gia ƒë·∫∑t t∆∞·ªùng v√† t·∫°o m√™ cung
- Wall Usage: 90% chance
- Reward Structure:
  * wallPlacementReward = 60 * intensity
  * strategicWallReward = 40 * intensity
  * wallComboReward = 50 * intensity
  * movementWithoutWallPenalty = -25 * intensity
- Q-table: qtable_wallmaster.json

‚ö° SPEED RUNNER STYLE:
- M·ª•c ti√™u: ƒê·∫°t ƒë√≠ch nhanh nh·∫•t
- Wall Usage: 10% chance
- Reward Structure:
  * movementReward = 40 * intensity
  * progressToGoalReward = 50 * intensity
  * quickWinBonus = 100 * intensity
  * wallPlacementPenalty = -20 * intensity
- Q-table: qtable_speedrunner.json

üß† TACTICAL STYLE:
- M·ª•c ti√™u: L·ª£i th·∫ø v·ªã tr√≠ v√† k·∫ø ho·∫°ch d√†i h·∫°n
- Wall Usage: Variable (strategic)
- Reward Structure:
  * tacticalMoveReward = 45 * intensity
  * positionAdvantageReward = 35 * intensity
  * mobilityReward = 25 * intensity
  * patternRecognitionBonus = 40 * intensity
- Q-table: qtable_tactical.json

‚öñÔ∏è BALANCED STYLE:
- M·ª•c ti√™u: C√¢n b·∫±ng t·∫•t c·∫£ aspects
- Wall Usage: 30% chance
- Reward Structure: Default balanced values
- Q-table: qtable.json

4.3 STYLE SWITCHING MECHANISM:

```csharp
public void SwitchToStyle(PlayStyle newStyle)
{
    // Save current Q-table
    if (allowQTableSaving) qAgent.SaveQTable(GetStyleSpecificQTablePath());
    
    // Load new style Q-table
    specializedStyle = newStyle;
    qAgent.LoadQTable(GetStyleSpecificQTablePath());
    
    // Reconfigure parameters
    ConfigureStyleSpecificTraining();
}
```

4.4 STYLE INTENSITY:
Parameter t·ª´ 0.0 ‚Üí 1.0 control m·ª©c ƒë·ªô chuy√™n h√≥a:
- 0.0: Balanced behavior
- 0.5: Moderate specialization
- 1.0: Extreme specialization

================================================================================
                        5. DECISION TREE CLASSIFIER
================================================================================

5.1 PURPOSE:
Decision Tree ph√¢n t√≠ch game context ƒë·ªÉ ch·ªçn strategy ph√π h·ª£p, d·ª±a tr√™n:
- Game phase (Early/Mid/End)
- Position advantage
- Resource advantage
- Mobility situation

5.2 FEATURE EXTRACTION:

```csharp
public struct DecisionFeatures
{
    public int aiDistanceToGoal;      // AI distance to goal line
    public int humanDistanceToGoal;   // Human distance to goal line
    public int aiWallsLeft;           // AI walls remaining
    public int humanWallsLeft;        // Human walls remaining
    public int aiMobility;            // AI possible moves
    public int humanMobility;         // Human possible moves
    public float distanceAdvantage;   // Human - AI distance
    public float wallAdvantage;       // AI - Human walls
    public bool isEarlyGame;          // > 15 moves estimated remaining
    public bool isMidGame;            // 5-15 moves remaining
    public bool isEndGame;            // < 5 moves remaining
}
```

5.3 DECISION LOGIC:

```csharp
public AIStrategy Predict(DecisionFeatures features)
{
    // Early Game: Play conservatively
    if (features.isEarlyGame)
        return AIStrategy.Balanced;
    
    // AI has significant lead: Focus on blocking
    if (features.distanceAdvantage > 2)
        return AIStrategy.Blocking;
    
    // AI is behind: Be aggressive
    if (features.distanceAdvantage < -1)
        return AIStrategy.Aggressive;
    
    // End game with wall advantage: Play defensively
    if (features.isEndGame && features.wallAdvantage > 2)
        return AIStrategy.Defensive;
    
    // Close game: Balanced approach
    return AIStrategy.Balanced;
}
```

5.4 STRATEGY OUTCOMES:
- Balanced: Equal weight to movement v√† walls
- Blocking: Prioritize moves that increase opponent path
- Aggressive: Focus on attacking positions
- Defensive: Protect own position and path

================================================================================
                        6. ENTROPY ANALYSIS & SOFTMAX
================================================================================

6.1 ENTROPY CONCEPT:
Entropy ƒëo ƒë·ªô ƒëa d·∫°ng/uncertainty c·ªßa move scores. High entropy = nhi·ªÅu moves
c√≥ scores t∆∞∆°ng ƒë∆∞∆°ng, low entropy = m·ªôt move r√µ r√†ng t·ªët nh·∫•t.

6.2 ENTROPY CALCULATION:

```csharp
float CalculateEntropy(List<float> moveScores)
{
    // Normalize th√†nh probabilities
    float total = moveScores.Sum();
    var probabilities = moveScores.Select(s => s / total);
    
    // Calculate Shannon entropy
    float entropy = 0f;
    foreach (float p in probabilities)
    {
        if (p > 0f)
            entropy -= p * Mathf.Log(p, 2f);
    }
    
    return entropy; // Range: 0 (deterministic) to log2(n) (uniform)
}
```

6.3 SOFTMAX DISTRIBUTION:

```csharp
float[] CalculateSoftmax(List<float> scores, float temperature)
{
    float[] probabilities = new float[scores.Count];
    float sum = scores.Sum(s => Mathf.Exp(s / temperature));
    
    for (int i = 0; i < scores.Count; i++)
    {
        probabilities[i] = Mathf.Exp(scores[i] / temperature) / sum;
    }
    
    return probabilities;
}
```

6.4 TEMPERATURE PARAMETER:
- Low temperature (0.1): Sharp distribution, focus on best moves
- High temperature (2.0): Flat distribution, more exploration
- Default temperature (1.0): Balanced exploitation/exploration

6.5 ADAPTIVE MOVE SELECTION:

```csharp
public AIMove ChooseActionWithEntropy(GameState state, List<AIMove> possibleMoves)
{
    var moveScores = EvaluateAllMoves(state, possibleMoves);
    float entropy = CalculateEntropy(moveScores);
    
    if (entropy > entropyThreshold) // High diversity
    {
        // Use softmax for probabilistic selection
        var probabilities = CalculateSoftmax(moveScores, softmaxTemperature);
        return SelectMoveByProbability(possibleMoves, probabilities);
    }
    else // Clear best move
    {
        // Use deterministic best move
        return GetBestScoredMove(possibleMoves, moveScores);
    }
}
```

================================================================================
                        7. TRAINING SYSTEM & SELF-PLAY
================================================================================

7.1 SELF-PLAY CONCEPT:
AI h·ªçc b·∫±ng c√°ch ch∆°i v·ªõi ch√≠nh n√≥ h√†ng ng√†n l·∫ßn, gradually improve performance
th√¥ng qua trial v√† error.

7.2 TRAINING PIPELINE:

```csharp
IEnumerator RunSelfPlayTraining()
{
    for (int episode = 1; episode <= maxEpisodes; episode++)
    {
        // Reset game state
        ResetGameForTraining();
        
        while (!IsGameOver())
        {
            // Get current state
            GameState currentState = GetCurrentGameState();
            List<AIMove> possibleMoves = GetPossibleMoves();
            
            // Choose action using Œµ-greedy
            AIMove chosenMove = qLearningAgent.ChooseAction(currentState, possibleMoves);
            
            // Execute move
            ExecuteMove(chosenMove);
            
            // Calculate reward
            float reward = CalculateReward(currentState, chosenMove);
            
            // Update Q-table
            GameState newState = GetCurrentGameState();
            qLearningAgent.UpdateQValue(
                EncodeState(currentState),
                EncodeAction(chosenMove),
                reward,
                EncodeState(newState)
            );
            
            yield return new WaitForEndOfFrame();
        }
        
        // Epsilon decay
        qLearningAgent.DecayEpsilon();
        
        // Save progress periodically
        if (episode % 100 == 0)
        {
            qLearningAgent.SaveQTable();
        }
    }
}
```

7.3 REWARD ENGINEERING:
Critical cho Q-Learning success:

```csharp
float CalculateReward(GameState state, AIMove move)
{
    float reward = 0f;
    
    // Game outcome rewards
    if (IsWin(state)) reward += 1000f;
    if (IsLoss(state)) reward -= 1000f;
    
    // Progress rewards
    if (move.moveType == MoveType.Movement)
    {
        if (IsMoveTowardGoal(move)) reward += 20f;
        else reward -= 5f;
    }
    
    // Strategic rewards
    if (move.moveType == MoveType.WallPlacement)
    {
        if (IsWallBlockingOpponent(move)) reward += 50f;
        if (IsWallStrategic(move)) reward += 30f;
        if (IsWallWasted(move)) reward -= 25f;
    }
    
    // Style-specific rewards
    reward += GetStyleSpecificReward(state, move);
    
    return reward;
}
```

7.4 TRAINING METRICS:
- Episodes: 1000 standard
- Duration: ~15 minutes
- Convergence: Usually around episode 800
- Q-table size: 13,118+ states at completion
- Final epsilon: 0.1 (90% exploitation)

================================================================================
                        8. AI TESTING FRAMEWORK
================================================================================

8.1 TESTING PHILOSOPHY:
Comprehensive testing ensures AI quality v√† reliability across different
game scenarios v√† edge cases.

8.2 TEST CATEGORIES:

A) DECISION TREE TESTS:
```csharp
void TestDecisionTreeStrategies()
{
    // Test Early Game
    var earlyGameFeatures = new DecisionFeatures
    {
        aiDistanceToGoal = 8, humanDistanceToGoal = 8,
        isEarlyGame = true
    };
    var strategy = decisionTree.Predict(earlyGameFeatures);
    Assert.AreEqual(AIStrategy.Balanced, strategy);
    
    // Test AI Leading
    var leadingFeatures = new DecisionFeatures
    {
        distanceAdvantage = 3, // AI leading
        isMidGame = true
    };
    strategy = decisionTree.Predict(leadingFeatures);
    Assert.AreEqual(AIStrategy.Blocking, strategy);
}
```

B) ENTROPY ANALYSIS TESTS:
```csharp
void TestEntropyAnalysis()
{
    // High entropy case (diverse scores)
    var highEntropyScores = new List<float> { 100f, 98f, 97f, 95f, 94f };
    float entropy = CalculateTestEntropy(highEntropyScores);
    Assert.IsTrue(entropy > 0.5f);
    
    // Low entropy case (clear winner)
    var lowEntropyScores = new List<float> { 100f, 50f, 50f, 50f, 50f };
    entropy = CalculateTestEntropy(lowEntropyScores);
    Assert.IsTrue(entropy < 0.5f);
}
```

C) SOFTMAX DISTRIBUTION TESTS:
```csharp
void TestSoftmaxDistribution()
{
    var testScores = new List<float> { 100f, 98f, 95f, 90f };
    float temperature = 1.0f;
    
    var probabilities = CalculateSoftmax(testScores, temperature);
    
    // Verify probabilities sum to 1
    float sum = probabilities.Sum();
    Assert.AreEqual(1.0f, sum, 0.001f);
    
    // Verify highest score gets highest probability
    int maxIndex = Array.IndexOf(probabilities, probabilities.Max());
    Assert.AreEqual(0, maxIndex);
}
```

D) PERFORMANCE BENCHMARKS:
- Move generation time: <100ms
- Q-Learning decision: <50ms
- Minimax decision (depth 4): <200ms
- Memory usage: <256MB

8.3 AUTOMATED TESTING:
Tests run automatically ƒë·ªÉ ensure code quality:
- Unit tests cho individual algorithms
- Integration tests cho AI system interactions
- Performance tests cho response times
- Regression tests ƒë·ªÉ prevent quality degradation

================================================================================
                        9. HYBRID AI ARCHITECTURE
================================================================================

9.1 CONCEPT:
Hybrid system k·∫øt h·ª£p strengths c·ªßa multiple algorithms:
- Q-Learning: Data-driven, learns from experience
- Minimax: Logical, guaranteed optimal given evaluation
- Decision Tree: Fast, context-aware switching
- Entropy: Adaptive, handles uncertainty

9.2 ALGORITHM SELECTION LOGIC:

```csharp
public AIMove GetBestMove()
{
    UpdateGameState();
    List<AIMove> possibleMoves = GeneratePossibleMoves();
    
    // Primary: Use Q-Learning if trained model available
    if (useQLearning && qAgent.IsTrainedModel())
    {
        Debug.Log($"üß† Using {specializedStyle} Q-Learning");
        return qAgent.ChooseAction(currentGameState, possibleMoves);
    }
    
    // Fallback: Use Minimax for guaranteed reasonable play
    else
    {
        Debug.Log($"üîç Using {specializedStyle} Minimax");
        return FindBestMoveWithMinimax();
    }
}
```

9.3 SEAMLESS SWITCHING:
- No interruption in gameplay
- Maintains consistent API
- Preserves game state integrity
- Logs algorithm used for debugging

9.4 BENEFITS:
- Robustness: Fallback ensures AI never fails
- Performance: Q-Learning for trained scenarios, Minimax for edge cases
- Flexibility: Can switch algorithms based on situation
- Development: Easy to test individual components

================================================================================
                        10. PERFORMANCE OPTIMIZATION
================================================================================

10.1 Q-TABLE OPTIMIZATION:
- Sparse representation: Only store visited states
- Compression: String interning for state keys
- Lazy loading: Load Q-table on demand
- Periodic saving: Every 100 training episodes

10.2 MINIMAX OPTIMIZATION:
- Alpha-Beta pruning: 50% search reduction
- Move ordering: Promising moves first
- Iterative deepening: Gradual depth increase
- Transposition tables: Cache evaluated positions

10.3 MEMORY MANAGEMENT:
- Object pooling cho GameState objects
- String caching cho repeated state/action encodings
- Garbage collection optimization
- Memory profiling v√† monitoring

10.4 MULTITHREADING CONSIDERATIONS:
- Q-Learning training: Background thread
- Move calculation: Main thread (Unity requirement)
- File I/O: Async operations where possible
- Thread-safe collections cho shared data

10.5 CACHING STRATEGIES:
- Path finding results
- Move generation results
- Evaluation function results
- Feature extraction results

================================================================================
                            K·∫æT LU·∫¨N
================================================================================

D·ª± √°n AI Quoridor Project th·ªÉ hi·ªán m·ªôt implementation comprehensive c·ªßa modern
game AI techniques:

TECHNICAL INNOVATIONS:
‚úÖ Hybrid Q-Learning + Minimax architecture
‚úÖ 6 specialized AI personalities v·ªõi separate training
‚úÖ Advanced decision making v·ªõi entropy analysis
‚úÖ Comprehensive testing v√† validation framework
‚úÖ Professional development tools v√† debugging

EDUCATIONAL VALUE:
‚úÖ Practical machine learning implementation
‚úÖ Traditional game AI algorithms
‚úÖ Modern software engineering practices
‚úÖ Performance optimization techniques
‚úÖ Automated testing methodologies

FUTURE ENHANCEMENTS:
üöÄ Deep Q-Learning (DQN) integration
üöÄ Multi-agent reinforcement learning
üöÄ Neural network-based evaluation functions
üöÄ Online learning v√† adaptation
üöÄ Transfer learning between game variants

D·ª± √°n n√†y demonstrating successful integration c·ªßa classical AI methods v·ªõi
modern machine learning, t·∫°o ra m·ªôt intelligent, adaptive, v√† highly
interactive gaming experience.

================================================================================
                            APPENDIX
================================================================================

FILES LI√äN QUAN:
- Assets/scripts/QLearningAgent.cs - Q-Learning implementation
- Assets/scripts/QuoridorAI.cs - Main AI controller
- Assets/scripts/DecisionTreeClassifier.cs - Decision tree logic
- Assets/scripts/QuoridorSelfPlayTrainer.cs - Training system
- Assets/scripts/AITestScript.cs - Testing framework
- Assets/scripts/Editor/ForceRetrainAI.cs - Development tools

Q-TABLE FILES:
- StreamingAssets/qtable.json - Balanced style
- StreamingAssets/qtable_aggressive.json - Aggressive style
- StreamingAssets/qtable_defensive.json - Defensive style
- StreamingAssets/qtable_wallmaster.json - Wall Master style
- StreamingAssets/qtable_speedrunner.json - Speed Runner style
- StreamingAssets/qtable_tactical.json - Tactical style

DOCUMENTATION:
- AI_ALGORITHMS_DETAILED_DESCRIPTION.txt - This file
- README.md - Project overview
- API_INTEGRATION_GUIDE.md - Voice/Chat integration
- TRAINING_GUIDE.md - AI training instructions

================================================================================
================================================================================
                    AI QUORIDOR GAME PROJECT - THU·∫¨T TO√ÅN AI CHI TI·∫æT
================================================================================

D·ª± √°n: AI Quoridor Game v·ªõi Machine Learning v√† Multi-Platform Integration
Ng√†y t·∫°o: 25/07/2025
T√°c gi·∫£: [Student Name]
Platform: Unity 3D
Language: C#

================================================================================
                            M·ª§C L·ª§C
================================================================================

1. T·ªîNG QUAN KI·∫æN TR√öC AI
2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
4. SPECIALIZED AI STYLES SYSTEM
5. DECISION TREE CLASSIFIER
6. ENTROPY ANALYSIS & SOFTMAX
7. TRAINING SYSTEM & SELF-PLAY
8. AI TESTING FRAMEWORK
9. HYBRID AI ARCHITECTURE
10. PERFORMANCE OPTIMIZATION

================================================================================
                        1. T·ªîNG QUAN KI·∫æN TR√öC AI
================================================================================

D·ª± √°n AI Quoridor s·ª≠ d·ª•ng m·ªôt ki·∫øn tr√∫c AI hybrid ti√™n ti·∫øn, k·∫øt h·ª£p:

1.1 CORE ALGORITHMS:
- Q-Learning (Reinforcement Learning) - Thu·∫≠t to√°n ch√≠nh
- Minimax + Alpha-Beta Pruning - Thu·∫≠t to√°n fallback
- Decision Tree Classifier - Ph√¢n t√≠ch ng·ªØ c·∫£nh game
- Entropy Analysis - ƒê√°nh gi√° ƒëa d·∫°ng move

1.2 SPECIALIZED SYSTEMS:
- 6 AI Personality Styles v·ªõi training ri√™ng bi·ªát
- Self-Play Training Pipeline t·ª± ƒë·ªông
- Real-time Performance Testing Framework
- Dynamic Algorithm Switching

1.3 TECHNICAL FOUNDATION:
- State Space: 13,118+ unique game states
- Action Space: Movement + Wall placement combinations
- Training Method: Self-play v·ªõi 1000 episodes
- Learning Rate: Œ± = 0.1, Discount Factor: Œ≥ = 0.99

================================================================================
                    2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
================================================================================

2.1 GI·ªöI THI·ªÜU Q-LEARNING:
Q-Learning l√† thu·∫≠t to√°n Reinforcement Learning kh√¥ng c·∫ßn model, h·ªçc th√¥ng qua
t∆∞∆°ng t√°c v·ªõi m√¥i tr∆∞·ªùng ƒë·ªÉ t·ªëi ∆∞u h√≥a policy. Trong Quoridor, AI h·ªçc c√°ch
ch·ªçn moves t·ªëi ∆∞u d·ª±a tr√™n kinh nghi·ªám t·ª´ h√†ng ng√†n game.

2.2 C√îNG TH·ª®C TO√ÅN H·ªåC:
Q(s,a) = Q(s,a) + Œ±[r + Œ≥*max(Q(s',a')) - Q(s,a)]

Trong ƒë√≥:
- Q(s,a): Q-value c·ªßa state s v·ªõi action a
- Œ±: Learning rate (0.1) - t·ªëc ƒë·ªô h·ªçc
- r: Immediate reward t·ª´ action
- Œ≥: Discount factor (0.99) - t·∫ßm quan tr·ªçng future rewards
- s': Next state sau khi th·ª±c hi·ªán action
- max(Q(s',a')): Maximum Q-value c·ªßa next state

2.3 IMPLEMENTATION CHI TI·∫æT:

```csharp
public class QLearningAgent
{
    // Core Q-Table: Dictionary l∆∞u tr·ªØ Q-values
    private Dictionary<string, Dictionary<string, float>> qTable;
    
    // Hyperparameters
    private float learningRate = 0.1f;      // T·ªëc ƒë·ªô h·ªçc
    private float discountFactor = 0.99f;   // Discount future rewards
    private float epsilon = 0.995f;         // Exploration rate
    
    // Q-Learning Update Implementation
    public void UpdateQValue(string state, string action, float reward, string nextState)
    {
        // L·∫•y Q-value hi·ªán t·∫°i
        float currentQ = GetQValue(state, action);
        
        // T√¨m max Q-value c·ªßa next state
        float maxNextQ = GetMaxQValue(nextState);
        
        // √Åp d·ª•ng c√¥ng th·ª©c Q-Learning
        float newQ = currentQ + learningRate * (reward + discountFactor * maxNextQ - currentQ);
        
        // C·∫≠p nh·∫≠t Q-table
        SetQValue(state, action, newQ);
    }
}
```

2.4 STATE ENCODING:
Game state ƒë∆∞·ª£c encode th√†nh string duy nh·∫•t:
"aiPos_x,aiPos_y-humanPos_x,humanPos_y-aiWalls-humanWalls-wallsList"

V√≠ d·ª•: "4,8-4,0-9-10-7,4,False|5,6,True"
- AI ·ªü (4,8), Human ·ªü (4,0)
- AI c√≥ 9 walls, Human c√≥ 10 walls
- Walls ƒë√£ ƒë·∫∑t: (7,4,False), (5,6,True)

2.5 ACTION ENCODING:
Actions ƒë∆∞·ª£c encode th√†nh string:
- Movement: "M:x,y" (v√≠ d·ª•: "M:4,7")
- Wall: "W:x,y,horizontal" (v√≠ d·ª•: "W:3,5,True")

2.6 EPSILON-GREEDY STRATEGY:
```csharp
public AIMove ChooseAction(GameState state, List<AIMove> possibleMoves)
{
    // Exploration vs Exploitation trade-off
    if (Random.Range(0f, 1f) < epsilon)
    {
        // EXPLORATION: Ch·ªçn random ƒë·ªÉ kh√°m ph√°
        return possibleMoves[Random.Range(0, possibleMoves.Count)];
    }
    else
    {
        // EXPLOITATION: Ch·ªçn action c√≥ Q-value cao nh·∫•t
        return GetBestAction(state, possibleMoves);
    }
}
```

2.7 EPSILON DECAY:
Epsilon gi·∫£m d·∫ßn t·ª´ 0.995 ‚Üí 0.1 qua training:
- Ban ƒë·∫ßu: High exploration (99.5%)
- Cu·ªëi training: High exploitation (90%)

2.8 REWARD SYSTEM:
- Win game: +1000
- Lose game: -1000
- Move toward goal: +10
- Block opponent: +50
- Waste move: -5
- Strategic position: +15

================================================================================
                3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
================================================================================

3.1 GI·ªöI THI·ªÜU MINIMAX:
Minimax l√† thu·∫≠t to√°n game theory c·ªï ƒëi·ªÉn, t√¨m ki·∫øm move t·ªëi ∆∞u b·∫±ng c√°ch
d·ª± ƒëo√°n tr∆∞·ªõc nhi·ªÅu moves. Alpha-Beta pruning t·ªëi ∆∞u hi·ªáu su·∫•t b·∫±ng c√°ch
lo·∫°i b·ªè c√°c nh√°nh kh√¥ng c·∫ßn thi·∫øt.

3.2 NGUY√äN L√ù HO·∫†T ƒê·ªòNG:
- Maximizing Player (AI): T·ªëi ƒëa h√≥a score
- Minimizing Player (Human): T·ªëi thi·ªÉu h√≥a score AI
- Search depth: 4 levels (configurable)
- Pruning: Lo·∫°i b·ªè nh√°nh kh√¥ng promising

3.3 IMPLEMENTATION:

```csharp
float Minimax(GameState state, int depth, bool isMaximizing, float alpha, float beta)
{
    // Base case: Depth = 0 ho·∫∑c game over
    if (depth == 0 || IsGameOver(state))
        return EvaluatePosition(state);
    
    if (isMaximizing) // AI turn
    {
        float maxEval = float.NegativeInfinity;
        foreach (var move in GeneratePossibleMoves(state, true))
        {
            GameState newState = ApplyMove(state, move, true);
            float eval = Minimax(newState, depth - 1, false, alpha, beta);
            maxEval = Mathf.Max(maxEval, eval);
            alpha = Mathf.Max(alpha, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return maxEval;
    }
    else // Human turn
    {
        float minEval = float.PositiveInfinity;
        foreach (var move in GeneratePossibleMoves(state, false))
        {
            GameState newState = ApplyMove(state, move, false);
            float eval = Minimax(newState, depth - 1, true, alpha, beta);
            minEval = Mathf.Min(minEval, eval);
            beta = Mathf.Min(beta, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return minEval;
    }
}
```

3.4 EVALUATION FUNCTION:
Position evaluation d·ª±a tr√™n multiple factors:

```csharp
float EvaluatePosition(GameState state)
{
    float score = 0f;
    
    // Distance to goal (c√†ng g·∫ßn c√†ng t·ªët)
    int aiDistance = GetShortestPath(state.aiPosition, goalRow);
    int humanDistance = GetShortestPath(state.humanPosition, goalRow);
    score += (humanDistance - aiDistance) * 100;
    
    // Wall advantage
    score += (state.aiWallsLeft - state.humanWallsLeft) * 50;
    
    // Mobility (s·ªë moves c√≥ th·ªÉ)
    int aiMobility = GetValidMoves(state.aiPosition).Count;
    int humanMobility = GetValidMoves(state.humanPosition).Count;
    score += (aiMobility - humanMobility) * 20;
    
    // Center control
    score += GetCenterControlScore(state) * 30;
    
    return score;
}
```

3.5 ALPHA-BETA PRUNING HI·ªÜU QU·∫¢:
- Reduces search space t·ª´ O(b^d) xu·ªëng O(b^(d/2))
- Best case: 50% reduction in nodes evaluated
- Ordering moves by promising gi√∫p pruning hi·ªáu qu·∫£ h∆°n

================================================================================
                        4. SPECIALIZED AI STYLES SYSTEM
================================================================================

4.1 CONCEPT:
6 AI personalities v·ªõi training v√† behavior ri√™ng bi·ªát, m·ªói style c√≥:
- Ri√™ng Q-table file
- Custom reward structure
- Specific move prioritization
- Unique decision making logic

4.2 AI STYLES CHI TI·∫æT:

üî• AGGRESSIVE STYLE:
- M·ª•c ti√™u: T·∫•n c√¥ng v√† ch·∫∑n ƒë·ªëi th·ªß
- Wall Usage: 70% chance
- Reward Structure:
  * blockingOpponentReward = 50 * intensity
  * wallPlacementReward = 30 * intensity
  * defensiveWallPenalty = -20 * intensity
- Q-table: qtable_aggressive.json

üõ°Ô∏è DEFENSIVE STYLE:
- M·ª•c ti√™u: Ph√≤ng th·ªß v√† di chuy·ªÉn an to√†n
- Wall Usage: 50% chance
- Reward Structure:
  * defensiveWallReward = 35 * intensity
  * movementReward = 20 * intensity
  * aggressiveMovePenalty = -15 * intensity
- Q-table: qtable_defensive.json

üß± WALL MASTER STYLE:
- M·ª•c ti√™u: Chuy√™n gia ƒë·∫∑t t∆∞·ªùng v√† t·∫°o m√™ cung
- Wall Usage: 90% chance
- Reward Structure:
  * wallPlacementReward = 60 * intensity
  * strategicWallReward = 40 * intensity
  * wallComboReward = 50 * intensity
  * movementWithoutWallPenalty = -25 * intensity
- Q-table: qtable_wallmaster.json

‚ö° SPEED RUNNER STYLE:
- M·ª•c ti√™u: ƒê·∫°t ƒë√≠ch nhanh nh·∫•t
- Wall Usage: 10% chance
- Reward Structure:
  * movementReward = 40 * intensity
  * progressToGoalReward = 50 * intensity
  * quickWinBonus = 100 * intensity
  * wallPlacementPenalty = -20 * intensity
- Q-table: qtable_speedrunner.json

üß† TACTICAL STYLE:
- M·ª•c ti√™u: L·ª£i th·∫ø v·ªã tr√≠ v√† k·∫ø ho·∫°ch d√†i h·∫°n
- Wall Usage: Variable (strategic)
- Reward Structure:
  * tacticalMoveReward = 45 * intensity
  * positionAdvantageReward = 35 * intensity
  * mobilityReward = 25 * intensity
  * patternRecognitionBonus = 40 * intensity
- Q-table: qtable_tactical.json

‚öñÔ∏è BALANCED STYLE:
- M·ª•c ti√™u: C√¢n b·∫±ng t·∫•t c·∫£ aspects
- Wall Usage: 30% chance
- Reward Structure: Default balanced values
- Q-table: qtable.json

4.3 STYLE SWITCHING MECHANISM:

```csharp
public void SwitchToStyle(PlayStyle newStyle)
{
    // Save current Q-table
    if (allowQTableSaving) qAgent.SaveQTable(GetStyleSpecificQTablePath());
    
    // Load new style Q-table
    specializedStyle = newStyle;
    qAgent.LoadQTable(GetStyleSpecificQTablePath());
    
    // Reconfigure parameters
    ConfigureStyleSpecificTraining();
}
```

4.4 STYLE INTENSITY:
Parameter t·ª´ 0.0 ‚Üí 1.0 control m·ª©c ƒë·ªô chuy√™n h√≥a:
- 0.0: Balanced behavior
- 0.5: Moderate specialization
- 1.0: Extreme specialization

================================================================================
                        5. DECISION TREE CLASSIFIER
================================================================================

5.1 PURPOSE:
Decision Tree ph√¢n t√≠ch game context ƒë·ªÉ ch·ªçn strategy ph√π h·ª£p, d·ª±a tr√™n:
- Game phase (Early/Mid/End)
- Position advantage
- Resource advantage
- Mobility situation

5.2 FEATURE EXTRACTION:

```csharp
public struct DecisionFeatures
{
    public int aiDistanceToGoal;      // AI distance to goal line
    public int humanDistanceToGoal;   // Human distance to goal line
    public int aiWallsLeft;           // AI walls remaining
    public int humanWallsLeft;        // Human walls remaining
    public int aiMobility;            // AI possible moves
    public int humanMobility;         // Human possible moves
    public float distanceAdvantage;   // Human - AI distance
    public float wallAdvantage;       // AI - Human walls
    public bool isEarlyGame;          // > 15 moves estimated remaining
    public bool isMidGame;            // 5-15 moves remaining
    public bool isEndGame;            // < 5 moves remaining
}
```

5.3 DECISION LOGIC:

```csharp
public AIStrategy Predict(DecisionFeatures features)
{
    // Early Game: Play conservatively
    if (features.isEarlyGame)
        return AIStrategy.Balanced;
    
    // AI has significant lead: Focus on blocking
    if (features.distanceAdvantage > 2)
        return AIStrategy.Blocking;
    
    // AI is behind: Be aggressive
    if (features.distanceAdvantage < -1)
        return AIStrategy.Aggressive;
    
    // End game with wall advantage: Play defensively
    if (features.isEndGame && features.wallAdvantage > 2)
        return AIStrategy.Defensive;
    
    // Close game: Balanced approach
    return AIStrategy.Balanced;
}
```

5.4 STRATEGY OUTCOMES:
- Balanced: Equal weight to movement v√† walls
- Blocking: Prioritize moves that increase opponent path
- Aggressive: Focus on attacking positions
- Defensive: Protect own position and path

================================================================================
                        6. ENTROPY ANALYSIS & SOFTMAX
================================================================================

6.1 ENTROPY CONCEPT:
Entropy ƒëo ƒë·ªô ƒëa d·∫°ng/uncertainty c·ªßa move scores. High entropy = nhi·ªÅu moves
c√≥ scores t∆∞∆°ng ƒë∆∞∆°ng, low entropy = m·ªôt move r√µ r√†ng t·ªët nh·∫•t.

6.2 ENTROPY CALCULATION:

```csharp
float CalculateEntropy(List<float> moveScores)
{
    // Normalize th√†nh probabilities
    float total = moveScores.Sum();
    var probabilities = moveScores.Select(s => s / total);
    
    // Calculate Shannon entropy
    float entropy = 0f;
    foreach (float p in probabilities)
    {
        if (p > 0f)
            entropy -= p * Mathf.Log(p, 2f);
    }
    
    return entropy; // Range: 0 (deterministic) to log2(n) (uniform)
}
```

6.3 SOFTMAX DISTRIBUTION:

```csharp
float[] CalculateSoftmax(List<float> scores, float temperature)
{
    float[] probabilities = new float[scores.Count];
    float sum = scores.Sum(s => Mathf.Exp(s / temperature));
    
    for (int i = 0; i < scores.Count; i++)
    {
        probabilities[i] = Mathf.Exp(scores[i] / temperature) / sum;
    }
    
    return probabilities;
}
```

6.4 TEMPERATURE PARAMETER:
- Low temperature (0.1): Sharp distribution, focus on best moves
- High temperature (2.0): Flat distribution, more exploration
- Default temperature (1.0): Balanced exploitation/exploration

6.5 ADAPTIVE MOVE SELECTION:

```csharp
public AIMove ChooseActionWithEntropy(GameState state, List<AIMove> possibleMoves)
{
    var moveScores = EvaluateAllMoves(state, possibleMoves);
    float entropy = CalculateEntropy(moveScores);
    
    if (entropy > entropyThreshold) // High diversity
    {
        // Use softmax for probabilistic selection
        var probabilities = CalculateSoftmax(moveScores, softmaxTemperature);
        return SelectMoveByProbability(possibleMoves, probabilities);
    }
    else // Clear best move
    {
        // Use deterministic best move
        return GetBestScoredMove(possibleMoves, moveScores);
    }
}
```

================================================================================
                        7. TRAINING SYSTEM & SELF-PLAY
================================================================================

7.1 SELF-PLAY CONCEPT:
AI h·ªçc b·∫±ng c√°ch ch∆°i v·ªõi ch√≠nh n√≥ h√†ng ng√†n l·∫ßn, gradually improve performance
th√¥ng qua trial v√† error.

7.2 TRAINING PIPELINE:

```csharp
IEnumerator RunSelfPlayTraining()
{
    for (int episode = 1; episode <= maxEpisodes; episode++)
    {
        // Reset game state
        ResetGameForTraining();
        
        while (!IsGameOver())
        {
            // Get current state
            GameState currentState = GetCurrentGameState();
            List<AIMove> possibleMoves = GetPossibleMoves();
            
            // Choose action using Œµ-greedy
            AIMove chosenMove = qLearningAgent.ChooseAction(currentState, possibleMoves);
            
            // Execute move
            ExecuteMove(chosenMove);
            
            // Calculate reward
            float reward = CalculateReward(currentState, chosenMove);
            
            // Update Q-table
            GameState newState = GetCurrentGameState();
            qLearningAgent.UpdateQValue(
                EncodeState(currentState),
                EncodeAction(chosenMove),
                reward,
                EncodeState(newState)
            );
            
            yield return new WaitForEndOfFrame();
        }
        
        // Epsilon decay
        qLearningAgent.DecayEpsilon();
        
        // Save progress periodically
        if (episode % 100 == 0)
        {
            qLearningAgent.SaveQTable();
        }
    }
}
```

7.3 REWARD ENGINEERING:
Critical cho Q-Learning success:

```csharp
float CalculateReward(GameState state, AIMove move)
{
    float reward = 0f;
    
    // Game outcome rewards
    if (IsWin(state)) reward += 1000f;
    if (IsLoss(state)) reward -= 1000f;
    
    // Progress rewards
    if (move.moveType == MoveType.Movement)
    {
        if (IsMoveTowardGoal(move)) reward += 20f;
        else reward -= 5f;
    }
    
    // Strategic rewards
    if (move.moveType == MoveType.WallPlacement)
    {
        if (IsWallBlockingOpponent(move)) reward += 50f;
        if (IsWallStrategic(move)) reward += 30f;
        if (IsWallWasted(move)) reward -= 25f;
    }
    
    // Style-specific rewards
    reward += GetStyleSpecificReward(state, move);
    
    return reward;
}
```

7.4 TRAINING METRICS:
- Episodes: 1000 standard
- Duration: ~15 minutes
- Convergence: Usually around episode 800
- Q-table size: 13,118+ states at completion
- Final epsilon: 0.1 (90% exploitation)

================================================================================
                        8. AI TESTING FRAMEWORK
================================================================================

8.1 TESTING PHILOSOPHY:
Comprehensive testing ensures AI quality v√† reliability across different
game scenarios v√† edge cases.

8.2 TEST CATEGORIES:

A) DECISION TREE TESTS:
```csharp
void TestDecisionTreeStrategies()
{
    // Test Early Game
    var earlyGameFeatures = new DecisionFeatures
    {
        aiDistanceToGoal = 8, humanDistanceToGoal = 8,
        isEarlyGame = true
    };
    var strategy = decisionTree.Predict(earlyGameFeatures);
    Assert.AreEqual(AIStrategy.Balanced, strategy);
    
    // Test AI Leading
    var leadingFeatures = new DecisionFeatures
    {
        distanceAdvantage = 3, // AI leading
        isMidGame = true
    };
    strategy = decisionTree.Predict(leadingFeatures);
    Assert.AreEqual(AIStrategy.Blocking, strategy);
}
```

B) ENTROPY ANALYSIS TESTS:
```csharp
void TestEntropyAnalysis()
{
    // High entropy case (diverse scores)
    var highEntropyScores = new List<float> { 100f, 98f, 97f, 95f, 94f };
    float entropy = CalculateTestEntropy(highEntropyScores);
    Assert.IsTrue(entropy > 0.5f);
    
    // Low entropy case (clear winner)
    var lowEntropyScores = new List<float> { 100f, 50f, 50f, 50f, 50f };
    entropy = CalculateTestEntropy(lowEntropyScores);
    Assert.IsTrue(entropy < 0.5f);
}
```

C) SOFTMAX DISTRIBUTION TESTS:
```csharp
void TestSoftmaxDistribution()
{
    var testScores = new List<float> { 100f, 98f, 95f, 90f };
    float temperature = 1.0f;
    
    var probabilities = CalculateSoftmax(testScores, temperature);
    
    // Verify probabilities sum to 1
    float sum = probabilities.Sum();
    Assert.AreEqual(1.0f, sum, 0.001f);
    
    // Verify highest score gets highest probability
    int maxIndex = Array.IndexOf(probabilities, probabilities.Max());
    Assert.AreEqual(0, maxIndex);
}
```

D) PERFORMANCE BENCHMARKS:
- Move generation time: <100ms
- Q-Learning decision: <50ms
- Minimax decision (depth 4): <200ms
- Memory usage: <256MB

8.3 AUTOMATED TESTING:
Tests run automatically ƒë·ªÉ ensure code quality:
- Unit tests cho individual algorithms
- Integration tests cho AI system interactions
- Performance tests cho response times
- Regression tests ƒë·ªÉ prevent quality degradation

================================================================================
                        9. HYBRID AI ARCHITECTURE
================================================================================

9.1 CONCEPT:
Hybrid system k·∫øt h·ª£p strengths c·ªßa multiple algorithms:
- Q-Learning: Data-driven, learns from experience
- Minimax: Logical, guaranteed optimal given evaluation
- Decision Tree: Fast, context-aware switching
- Entropy: Adaptive, handles uncertainty

9.2 ALGORITHM SELECTION LOGIC:

```csharp
public AIMove GetBestMove()
{
    UpdateGameState();
    List<AIMove> possibleMoves = GeneratePossibleMoves();
    
    // Primary: Use Q-Learning if trained model available
    if (useQLearning && qAgent.IsTrainedModel())
    {
        Debug.Log($"üß† Using {specializedStyle} Q-Learning");
        return qAgent.ChooseAction(currentGameState, possibleMoves);
    }
    
    // Fallback: Use Minimax for guaranteed reasonable play
    else
    {
        Debug.Log($"üîç Using {specializedStyle} Minimax");
        return FindBestMoveWithMinimax();
    }
}
```

9.3 SEAMLESS SWITCHING:
- No interruption in gameplay
- Maintains consistent API
- Preserves game state integrity
- Logs algorithm used for debugging

9.4 BENEFITS:
- Robustness: Fallback ensures AI never fails
- Performance: Q-Learning for trained scenarios, Minimax for edge cases
- Flexibility: Can switch algorithms based on situation
- Development: Easy to test individual components

================================================================================
                        10. PERFORMANCE OPTIMIZATION
================================================================================

10.1 Q-TABLE OPTIMIZATION:
- Sparse representation: Only store visited states
- Compression: String interning for state keys
- Lazy loading: Load Q-table on demand
- Periodic saving: Every 100 training episodes

10.2 MINIMAX OPTIMIZATION:
- Alpha-Beta pruning: 50% search reduction
- Move ordering: Promising moves first
- Iterative deepening: Gradual depth increase
- Transposition tables: Cache evaluated positions

10.3 MEMORY MANAGEMENT:
- Object pooling cho GameState objects
- String caching cho repeated state/action encodings
- Garbage collection optimization
- Memory profiling v√† monitoring

10.4 MULTITHREADING CONSIDERATIONS:
- Q-Learning training: Background thread
- Move calculation: Main thread (Unity requirement)
- File I/O: Async operations where possible
- Thread-safe collections cho shared data

10.5 CACHING STRATEGIES:
- Path finding results
- Move generation results
- Evaluation function results
- Feature extraction results

================================================================================
                            K·∫æT LU·∫¨N
================================================================================

D·ª± √°n AI Quoridor Project th·ªÉ hi·ªán m·ªôt implementation comprehensive c·ªßa modern
game AI techniques:

TECHNICAL INNOVATIONS:
‚úÖ Hybrid Q-Learning + Minimax architecture
‚úÖ 6 specialized AI personalities v·ªõi separate training
‚úÖ Advanced decision making v·ªõi entropy analysis
‚úÖ Comprehensive testing v√† validation framework
‚úÖ Professional development tools v√† debugging

EDUCATIONAL VALUE:
‚úÖ Practical machine learning implementation
‚úÖ Traditional game AI algorithms
‚úÖ Modern software engineering practices
‚úÖ Performance optimization techniques
‚úÖ Automated testing methodologies

FUTURE ENHANCEMENTS:
üöÄ Deep Q-Learning (DQN) integration
üöÄ Multi-agent reinforcement learning
üöÄ Neural network-based evaluation functions
üöÄ Online learning v√† adaptation
üöÄ Transfer learning between game variants

D·ª± √°n n√†y demonstrating successful integration c·ªßa classical AI methods v·ªõi
modern machine learning, t·∫°o ra m·ªôt intelligent, adaptive, v√† highly
interactive gaming experience.

================================================================================
                            APPENDIX
================================================================================

FILES LI√äN QUAN:
- Assets/scripts/QLearningAgent.cs - Q-Learning implementation
- Assets/scripts/QuoridorAI.cs - Main AI controller
- Assets/scripts/DecisionTreeClassifier.cs - Decision tree logic
- Assets/scripts/QuoridorSelfPlayTrainer.cs - Training system
- Assets/scripts/AITestScript.cs - Testing framework
- Assets/scripts/Editor/ForceRetrainAI.cs - Development tools

Q-TABLE FILES:
- StreamingAssets/qtable.json - Balanced style
- StreamingAssets/qtable_aggressive.json - Aggressive style
- StreamingAssets/qtable_defensive.json - Defensive style
- StreamingAssets/qtable_wallmaster.json - Wall Master style
- StreamingAssets/qtable_speedrunner.json - Speed Runner style
- StreamingAssets/qtable_tactical.json - Tactical style

DOCUMENTATION:
- AI_ALGORITHMS_DETAILED_DESCRIPTION.txt - This file
- README.md - Project overview
- API_INTEGRATION_GUIDE.md - Voice/Chat integration
- TRAINING_GUIDE.md - AI training instructions

================================================================================
================================================================================
                    AI QUORIDOR GAME PROJECT - THU·∫¨T TO√ÅN AI CHI TI·∫æT
================================================================================

D·ª± √°n: AI Quoridor Game v·ªõi Machine Learning v√† Multi-Platform Integration
Ng√†y t·∫°o: 25/07/2025
T√°c gi·∫£: [Student Name]
Platform: Unity 3D
Language: C#

================================================================================
                            M·ª§C L·ª§C
================================================================================

1. T·ªîNG QUAN KI·∫æN TR√öC AI
2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
4. SPECIALIZED AI STYLES SYSTEM
5. DECISION TREE CLASSIFIER
6. ENTROPY ANALYSIS & SOFTMAX
7. TRAINING SYSTEM & SELF-PLAY
8. AI TESTING FRAMEWORK
9. HYBRID AI ARCHITECTURE
10. PERFORMANCE OPTIMIZATION

================================================================================
                        1. T·ªîNG QUAN KI·∫æN TR√öC AI
================================================================================

D·ª± √°n AI Quoridor s·ª≠ d·ª•ng m·ªôt ki·∫øn tr√∫c AI hybrid ti√™n ti·∫øn, k·∫øt h·ª£p:

1.1 CORE ALGORITHMS:
- Q-Learning (Reinforcement Learning) - Thu·∫≠t to√°n ch√≠nh
- Minimax + Alpha-Beta Pruning - Thu·∫≠t to√°n fallback
- Decision Tree Classifier - Ph√¢n t√≠ch ng·ªØ c·∫£nh game
- Entropy Analysis - ƒê√°nh gi√° ƒëa d·∫°ng move

1.2 SPECIALIZED SYSTEMS:
- 6 AI Personality Styles v·ªõi training ri√™ng bi·ªát
- Self-Play Training Pipeline t·ª± ƒë·ªông
- Real-time Performance Testing Framework
- Dynamic Algorithm Switching

1.3 TECHNICAL FOUNDATION:
- State Space: 13,118+ unique game states
- Action Space: Movement + Wall placement combinations
- Training Method: Self-play v·ªõi 1000 episodes
- Learning Rate: Œ± = 0.1, Discount Factor: Œ≥ = 0.99

================================================================================
                    2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
================================================================================

2.1 GI·ªöI THI·ªÜU Q-LEARNING:
Q-Learning l√† thu·∫≠t to√°n Reinforcement Learning kh√¥ng c·∫ßn model, h·ªçc th√¥ng qua
t∆∞∆°ng t√°c v·ªõi m√¥i tr∆∞·ªùng ƒë·ªÉ t·ªëi ∆∞u h√≥a policy. Trong Quoridor, AI h·ªçc c√°ch
ch·ªçn moves t·ªëi ∆∞u d·ª±a tr√™n kinh nghi·ªám t·ª´ h√†ng ng√†n game.

2.2 C√îNG TH·ª®C TO√ÅN H·ªåC:
Q(s,a) = Q(s,a) + Œ±[r + Œ≥*max(Q(s',a')) - Q(s,a)]

Trong ƒë√≥:
- Q(s,a): Q-value c·ªßa state s v·ªõi action a
- Œ±: Learning rate (0.1) - t·ªëc ƒë·ªô h·ªçc
- r: Immediate reward t·ª´ action
- Œ≥: Discount factor (0.99) - t·∫ßm quan tr·ªçng future rewards
- s': Next state sau khi th·ª±c hi·ªán action
- max(Q(s',a')): Maximum Q-value c·ªßa next state

2.3 IMPLEMENTATION CHI TI·∫æT:

```csharp
public class QLearningAgent
{
    // Core Q-Table: Dictionary l∆∞u tr·ªØ Q-values
    private Dictionary<string, Dictionary<string, float>> qTable;
    
    // Hyperparameters
    private float learningRate = 0.1f;      // T·ªëc ƒë·ªô h·ªçc
    private float discountFactor = 0.99f;   // Discount future rewards
    private float epsilon = 0.995f;         // Exploration rate
    
    // Q-Learning Update Implementation
    public void UpdateQValue(string state, string action, float reward, string nextState)
    {
        // L·∫•y Q-value hi·ªán t·∫°i
        float currentQ = GetQValue(state, action);
        
        // T√¨m max Q-value c·ªßa next state
        float maxNextQ = GetMaxQValue(nextState);
        
        // √Åp d·ª•ng c√¥ng th·ª©c Q-Learning
        float newQ = currentQ + learningRate * (reward + discountFactor * maxNextQ - currentQ);
        
        // C·∫≠p nh·∫≠t Q-table
        SetQValue(state, action, newQ);
    }
}
```

2.4 STATE ENCODING:
Game state ƒë∆∞·ª£c encode th√†nh string duy nh·∫•t:
"aiPos_x,aiPos_y-humanPos_x,humanPos_y-aiWalls-humanWalls-wallsList"

V√≠ d·ª•: "4,8-4,0-9-10-7,4,False|5,6,True"
- AI ·ªü (4,8), Human ·ªü (4,0)
- AI c√≥ 9 walls, Human c√≥ 10 walls
- Walls ƒë√£ ƒë·∫∑t: (7,4,False), (5,6,True)

2.5 ACTION ENCODING:
Actions ƒë∆∞·ª£c encode th√†nh string:
- Movement: "M:x,y" (v√≠ d·ª•: "M:4,7")
- Wall: "W:x,y,horizontal" (v√≠ d·ª•: "W:3,5,True")

2.6 EPSILON-GREEDY STRATEGY:
```csharp
public AIMove ChooseAction(GameState state, List<AIMove> possibleMoves)
{
    // Exploration vs Exploitation trade-off
    if (Random.Range(0f, 1f) < epsilon)
    {
        // EXPLORATION: Ch·ªçn random ƒë·ªÉ kh√°m ph√°
        return possibleMoves[Random.Range(0, possibleMoves.Count)];
    }
    else
    {
        // EXPLOITATION: Ch·ªçn action c√≥ Q-value cao nh·∫•t
        return GetBestAction(state, possibleMoves);
    }
}
```

2.7 EPSILON DECAY:
Epsilon gi·∫£m d·∫ßn t·ª´ 0.995 ‚Üí 0.1 qua training:
- Ban ƒë·∫ßu: High exploration (99.5%)
- Cu·ªëi training: High exploitation (90%)

2.8 REWARD SYSTEM:
- Win game: +1000
- Lose game: -1000
- Move toward goal: +10
- Block opponent: +50
- Waste move: -5
- Strategic position: +15

================================================================================
                3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
================================================================================

3.1 GI·ªöI THI·ªÜU MINIMAX:
Minimax l√† thu·∫≠t to√°n game theory c·ªï ƒëi·ªÉn, t√¨m ki·∫øm move t·ªëi ∆∞u b·∫±ng c√°ch
d·ª± ƒëo√°n tr∆∞·ªõc nhi·ªÅu moves. Alpha-Beta pruning t·ªëi ∆∞u hi·ªáu su·∫•t b·∫±ng c√°ch
lo·∫°i b·ªè c√°c nh√°nh kh√¥ng c·∫ßn thi·∫øt.

3.2 NGUY√äN L√ù HO·∫†T ƒê·ªòNG:
- Maximizing Player (AI): T·ªëi ƒëa h√≥a score
- Minimizing Player (Human): T·ªëi thi·ªÉu h√≥a score AI
- Search depth: 4 levels (configurable)
- Pruning: Lo·∫°i b·ªè nh√°nh kh√¥ng promising

3.3 IMPLEMENTATION:

```csharp
float Minimax(GameState state, int depth, bool isMaximizing, float alpha, float beta)
{
    // Base case: Depth = 0 ho·∫∑c game over
    if (depth == 0 || IsGameOver(state))
        return EvaluatePosition(state);
    
    if (isMaximizing) // AI turn
    {
        float maxEval = float.NegativeInfinity;
        foreach (var move in GeneratePossibleMoves(state, true))
        {
            GameState newState = ApplyMove(state, move, true);
            float eval = Minimax(newState, depth - 1, false, alpha, beta);
            maxEval = Mathf.Max(maxEval, eval);
            alpha = Mathf.Max(alpha, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return maxEval;
    }
    else // Human turn
    {
        float minEval = float.PositiveInfinity;
        foreach (var move in GeneratePossibleMoves(state, false))
        {
            GameState newState = ApplyMove(state, move, false);
            float eval = Minimax(newState, depth - 1, true, alpha, beta);
            minEval = Mathf.Min(minEval, eval);
            beta = Mathf.Min(beta, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return minEval;
    }
}
```

3.4 EVALUATION FUNCTION:
Position evaluation d·ª±a tr√™n multiple factors:

```csharp
float EvaluatePosition(GameState state)
{
    float score = 0f;
    
    // Distance to goal (c√†ng g·∫ßn c√†ng t·ªët)
    int aiDistance = GetShortestPath(state.aiPosition, goalRow);
    int humanDistance = GetShortestPath(state.humanPosition, goalRow);
    score += (humanDistance - aiDistance) * 100;
    
    // Wall advantage
    score += (state.aiWallsLeft - state.humanWallsLeft) * 50;
    
    // Mobility (s·ªë moves c√≥ th·ªÉ)
    int aiMobility = GetValidMoves(state.aiPosition).Count;
    int humanMobility = GetValidMoves(state.humanPosition).Count;
    score += (aiMobility - humanMobility) * 20;
    
    // Center control
    score += GetCenterControlScore(state) * 30;
    
    return score;
}
```

3.5 ALPHA-BETA PRUNING HI·ªÜU QU·∫¢:
- Reduces search space t·ª´ O(b^d) xu·ªëng O(b^(d/2))
- Best case: 50% reduction in nodes evaluated
- Ordering moves by promising gi√∫p pruning hi·ªáu qu·∫£ h∆°n

================================================================================
                        4. SPECIALIZED AI STYLES SYSTEM
================================================================================

4.1 CONCEPT:
6 AI personalities v·ªõi training v√† behavior ri√™ng bi·ªát, m·ªói style c√≥:
- Ri√™ng Q-table file
- Custom reward structure
- Specific move prioritization
- Unique decision making logic

4.2 AI STYLES CHI TI·∫æT:

üî• AGGRESSIVE STYLE:
- M·ª•c ti√™u: T·∫•n c√¥ng v√† ch·∫∑n ƒë·ªëi th·ªß
- Wall Usage: 70% chance
- Reward Structure:
  * blockingOpponentReward = 50 * intensity
  * wallPlacementReward = 30 * intensity
  * defensiveWallPenalty = -20 * intensity
- Q-table: qtable_aggressive.json

üõ°Ô∏è DEFENSIVE STYLE:
- M·ª•c ti√™u: Ph√≤ng th·ªß v√† di chuy·ªÉn an to√†n
- Wall Usage: 50% chance
- Reward Structure:
  * defensiveWallReward = 35 * intensity
  * movementReward = 20 * intensity
  * aggressiveMovePenalty = -15 * intensity
- Q-table: qtable_defensive.json

üß± WALL MASTER STYLE:
- M·ª•c ti√™u: Chuy√™n gia ƒë·∫∑t t∆∞·ªùng v√† t·∫°o m√™ cung
- Wall Usage: 90% chance
- Reward Structure:
  * wallPlacementReward = 60 * intensity
  * strategicWallReward = 40 * intensity
  * wallComboReward = 50 * intensity
  * movementWithoutWallPenalty = -25 * intensity
- Q-table: qtable_wallmaster.json

‚ö° SPEED RUNNER STYLE:
- M·ª•c ti√™u: ƒê·∫°t ƒë√≠ch nhanh nh·∫•t
- Wall Usage: 10% chance
- Reward Structure:
  * movementReward = 40 * intensity
  * progressToGoalReward = 50 * intensity
  * quickWinBonus = 100 * intensity
  * wallPlacementPenalty = -20 * intensity
- Q-table: qtable_speedrunner.json

üß† TACTICAL STYLE:
- M·ª•c ti√™u: L·ª£i th·∫ø v·ªã tr√≠ v√† k·∫ø ho·∫°ch d√†i h·∫°n
- Wall Usage: Variable (strategic)
- Reward Structure:
  * tacticalMoveReward = 45 * intensity
  * positionAdvantageReward = 35 * intensity
  * mobilityReward = 25 * intensity
  * patternRecognitionBonus = 40 * intensity
- Q-table: qtable_tactical.json

‚öñÔ∏è BALANCED STYLE:
- M·ª•c ti√™u: C√¢n b·∫±ng t·∫•t c·∫£ aspects
- Wall Usage: 30% chance
- Reward Structure: Default balanced values
- Q-table: qtable.json

4.3 STYLE SWITCHING MECHANISM:

```csharp
public void SwitchToStyle(PlayStyle newStyle)
{
    // Save current Q-table
    if (allowQTableSaving) qAgent.SaveQTable(GetStyleSpecificQTablePath());
    
    // Load new style Q-table
    specializedStyle = newStyle;
    qAgent.LoadQTable(GetStyleSpecificQTablePath());
    
    // Reconfigure parameters
    ConfigureStyleSpecificTraining();
}
```

4.4 STYLE INTENSITY:
Parameter t·ª´ 0.0 ‚Üí 1.0 control m·ª©c ƒë·ªô chuy√™n h√≥a:
- 0.0: Balanced behavior
- 0.5: Moderate specialization
- 1.0: Extreme specialization

================================================================================
                        5. DECISION TREE CLASSIFIER
================================================================================

5.1 PURPOSE:
Decision Tree ph√¢n t√≠ch game context ƒë·ªÉ ch·ªçn strategy ph√π h·ª£p, d·ª±a tr√™n:
- Game phase (Early/Mid/End)
- Position advantage
- Resource advantage
- Mobility situation

5.2 FEATURE EXTRACTION:

```csharp
public struct DecisionFeatures
{
    public int aiDistanceToGoal;      // AI distance to goal line
    public int humanDistanceToGoal;   // Human distance to goal line
    public int aiWallsLeft;           // AI walls remaining
    public int humanWallsLeft;        // Human walls remaining
    public int aiMobility;            // AI possible moves
    public int humanMobility;         // Human possible moves
    public float distanceAdvantage;   // Human - AI distance
    public float wallAdvantage;       // AI - Human walls
    public bool isEarlyGame;          // > 15 moves estimated remaining
    public bool isMidGame;            // 5-15 moves remaining
    public bool isEndGame;            // < 5 moves remaining
}
```

5.3 DECISION LOGIC:

```csharp
public AIStrategy Predict(DecisionFeatures features)
{
    // Early Game: Play conservatively
    if (features.isEarlyGame)
        return AIStrategy.Balanced;
    
    // AI has significant lead: Focus on blocking
    if (features.distanceAdvantage > 2)
        return AIStrategy.Blocking;
    
    // AI is behind: Be aggressive
    if (features.distanceAdvantage < -1)
        return AIStrategy.Aggressive;
    
    // End game with wall advantage: Play defensively
    if (features.isEndGame && features.wallAdvantage > 2)
        return AIStrategy.Defensive;
    
    // Close game: Balanced approach
    return AIStrategy.Balanced;
}
```

5.4 STRATEGY OUTCOMES:
- Balanced: Equal weight to movement v√† walls
- Blocking: Prioritize moves that increase opponent path
- Aggressive: Focus on attacking positions
- Defensive: Protect own position and path

================================================================================
                        6. ENTROPY ANALYSIS & SOFTMAX
================================================================================

6.1 ENTROPY CONCEPT:
Entropy ƒëo ƒë·ªô ƒëa d·∫°ng/uncertainty c·ªßa move scores. High entropy = nhi·ªÅu moves
c√≥ scores t∆∞∆°ng ƒë∆∞∆°ng, low entropy = m·ªôt move r√µ r√†ng t·ªët nh·∫•t.

6.2 ENTROPY CALCULATION:

```csharp
float CalculateEntropy(List<float> moveScores)
{
    // Normalize th√†nh probabilities
    float total = moveScores.Sum();
    var probabilities = moveScores.Select(s => s / total);
    
    // Calculate Shannon entropy
    float entropy = 0f;
    foreach (float p in probabilities)
    {
        if (p > 0f)
            entropy -= p * Mathf.Log(p, 2f);
    }
    
    return entropy; // Range: 0 (deterministic) to log2(n) (uniform)
}
```

6.3 SOFTMAX DISTRIBUTION:

```csharp
float[] CalculateSoftmax(List<float> scores, float temperature)
{
    float[] probabilities = new float[scores.Count];
    float sum = scores.Sum(s => Mathf.Exp(s / temperature));
    
    for (int i = 0; i < scores.Count; i++)
    {
        probabilities[i] = Mathf.Exp(scores[i] / temperature) / sum;
    }
    
    return probabilities;
}
```

6.4 TEMPERATURE PARAMETER:
- Low temperature (0.1): Sharp distribution, focus on best moves
- High temperature (2.0): Flat distribution, more exploration
- Default temperature (1.0): Balanced exploitation/exploration

6.5 ADAPTIVE MOVE SELECTION:

```csharp
public AIMove ChooseActionWithEntropy(GameState state, List<AIMove> possibleMoves)
{
    var moveScores = EvaluateAllMoves(state, possibleMoves);
    float entropy = CalculateEntropy(moveScores);
    
    if (entropy > entropyThreshold) // High diversity
    {
        // Use softmax for probabilistic selection
        var probabilities = CalculateSoftmax(moveScores, softmaxTemperature);
        return SelectMoveByProbability(possibleMoves, probabilities);
    }
    else // Clear best move
    {
        // Use deterministic best move
        return GetBestScoredMove(possibleMoves, moveScores);
    }
}
```

================================================================================
                        7. TRAINING SYSTEM & SELF-PLAY
================================================================================

7.1 SELF-PLAY CONCEPT:
AI h·ªçc b·∫±ng c√°ch ch∆°i v·ªõi ch√≠nh n√≥ h√†ng ng√†n l·∫ßn, gradually improve performance
th√¥ng qua trial v√† error.

7.2 TRAINING PIPELINE:

```csharp
IEnumerator RunSelfPlayTraining()
{
    for (int episode = 1; episode <= maxEpisodes; episode++)
    {
        // Reset game state
        ResetGameForTraining();
        
        while (!IsGameOver())
        {
            // Get current state
            GameState currentState = GetCurrentGameState();
            List<AIMove> possibleMoves = GetPossibleMoves();
            
            // Choose action using Œµ-greedy
            AIMove chosenMove = qLearningAgent.ChooseAction(currentState, possibleMoves);
            
            // Execute move
            ExecuteMove(chosenMove);
            
            // Calculate reward
            float reward = CalculateReward(currentState, chosenMove);
            
            // Update Q-table
            GameState newState = GetCurrentGameState();
            qLearningAgent.UpdateQValue(
                EncodeState(currentState),
                EncodeAction(chosenMove),
                reward,
                EncodeState(newState)
            );
            
            yield return new WaitForEndOfFrame();
        }
        
        // Epsilon decay
        qLearningAgent.DecayEpsilon();
        
        // Save progress periodically
        if (episode % 100 == 0)
        {
            qLearningAgent.SaveQTable();
        }
    }
}
```

7.3 REWARD ENGINEERING:
Critical cho Q-Learning success:

```csharp
float CalculateReward(GameState state, AIMove move)
{
    float reward = 0f;
    
    // Game outcome rewards
    if (IsWin(state)) reward += 1000f;
    if (IsLoss(state)) reward -= 1000f;
    
    // Progress rewards
    if (move.moveType == MoveType.Movement)
    {
        if (IsMoveTowardGoal(move)) reward += 20f;
        else reward -= 5f;
    }
    
    // Strategic rewards
    if (move.moveType == MoveType.WallPlacement)
    {
        if (IsWallBlockingOpponent(move)) reward += 50f;
        if (IsWallStrategic(move)) reward += 30f;
        if (IsWallWasted(move)) reward -= 25f;
    }
    
    // Style-specific rewards
    reward += GetStyleSpecificReward(state, move);
    
    return reward;
}
```

7.4 TRAINING METRICS:
- Episodes: 1000 standard
- Duration: ~15 minutes
- Convergence: Usually around episode 800
- Q-table size: 13,118+ states at completion
- Final epsilon: 0.1 (90% exploitation)

================================================================================
                        8. AI TESTING FRAMEWORK
================================================================================

8.1 TESTING PHILOSOPHY:
Comprehensive testing ensures AI quality v√† reliability across different
game scenarios v√† edge cases.

8.2 TEST CATEGORIES:

A) DECISION TREE TESTS:
```csharp
void TestDecisionTreeStrategies()
{
    // Test Early Game
    var earlyGameFeatures = new DecisionFeatures
    {
        aiDistanceToGoal = 8, humanDistanceToGoal = 8,
        isEarlyGame = true
    };
    var strategy = decisionTree.Predict(earlyGameFeatures);
    Assert.AreEqual(AIStrategy.Balanced, strategy);
    
    // Test AI Leading
    var leadingFeatures = new DecisionFeatures
    {
        distanceAdvantage = 3, // AI leading
        isMidGame = true
    };
    strategy = decisionTree.Predict(leadingFeatures);
    Assert.AreEqual(AIStrategy.Blocking, strategy);
}
```

B) ENTROPY ANALYSIS TESTS:
```csharp
void TestEntropyAnalysis()
{
    // High entropy case (diverse scores)
    var highEntropyScores = new List<float> { 100f, 98f, 97f, 95f, 94f };
    float entropy = CalculateTestEntropy(highEntropyScores);
    Assert.IsTrue(entropy > 0.5f);
    
    // Low entropy case (clear winner)
    var lowEntropyScores = new List<float> { 100f, 50f, 50f, 50f, 50f };
    entropy = CalculateTestEntropy(lowEntropyScores);
    Assert.IsTrue(entropy < 0.5f);
}
```

C) SOFTMAX DISTRIBUTION TESTS:
```csharp
void TestSoftmaxDistribution()
{
    var testScores = new List<float> { 100f, 98f, 95f, 90f };
    float temperature = 1.0f;
    
    var probabilities = CalculateSoftmax(testScores, temperature);
    
    // Verify probabilities sum to 1
    float sum = probabilities.Sum();
    Assert.AreEqual(1.0f, sum, 0.001f);
    
    // Verify highest score gets highest probability
    int maxIndex = Array.IndexOf(probabilities, probabilities.Max());
    Assert.AreEqual(0, maxIndex);
}
```

D) PERFORMANCE BENCHMARKS:
- Move generation time: <100ms
- Q-Learning decision: <50ms
- Minimax decision (depth 4): <200ms
- Memory usage: <256MB

8.3 AUTOMATED TESTING:
Tests run automatically ƒë·ªÉ ensure code quality:
- Unit tests cho individual algorithms
- Integration tests cho AI system interactions
- Performance tests cho response times
- Regression tests ƒë·ªÉ prevent quality degradation

================================================================================
                        9. HYBRID AI ARCHITECTURE
================================================================================

9.1 CONCEPT:
Hybrid system k·∫øt h·ª£p strengths c·ªßa multiple algorithms:
- Q-Learning: Data-driven, learns from experience
- Minimax: Logical, guaranteed optimal given evaluation
- Decision Tree: Fast, context-aware switching
- Entropy: Adaptive, handles uncertainty

9.2 ALGORITHM SELECTION LOGIC:

```csharp
public AIMove GetBestMove()
{
    UpdateGameState();
    List<AIMove> possibleMoves = GeneratePossibleMoves();
    
    // Primary: Use Q-Learning if trained model available
    if (useQLearning && qAgent.IsTrainedModel())
    {
        Debug.Log($"üß† Using {specializedStyle} Q-Learning");
        return qAgent.ChooseAction(currentGameState, possibleMoves);
    }
    
    // Fallback: Use Minimax for guaranteed reasonable play
    else
    {
        Debug.Log($"üîç Using {specializedStyle} Minimax");
        return FindBestMoveWithMinimax();
    }
}
```

9.3 SEAMLESS SWITCHING:
- No interruption in gameplay
- Maintains consistent API
- Preserves game state integrity
- Logs algorithm used for debugging

9.4 BENEFITS:
- Robustness: Fallback ensures AI never fails
- Performance: Q-Learning for trained scenarios, Minimax for edge cases
- Flexibility: Can switch algorithms based on situation
- Development: Easy to test individual components

================================================================================
                        10. PERFORMANCE OPTIMIZATION
================================================================================

10.1 Q-TABLE OPTIMIZATION:
- Sparse representation: Only store visited states
- Compression: String interning for state keys
- Lazy loading: Load Q-table on demand
- Periodic saving: Every 100 training episodes

10.2 MINIMAX OPTIMIZATION:
- Alpha-Beta pruning: 50% search reduction
- Move ordering: Promising moves first
- Iterative deepening: Gradual depth increase
- Transposition tables: Cache evaluated positions

10.3 MEMORY MANAGEMENT:
- Object pooling cho GameState objects
- String caching cho repeated state/action encodings
- Garbage collection optimization
- Memory profiling v√† monitoring

10.4 MULTITHREADING CONSIDERATIONS:
- Q-Learning training: Background thread
- Move calculation: Main thread (Unity requirement)
- File I/O: Async operations where possible
- Thread-safe collections cho shared data

10.5 CACHING STRATEGIES:
- Path finding results
- Move generation results
- Evaluation function results
- Feature extraction results

================================================================================
                            K·∫æT LU·∫¨N
================================================================================

D·ª± √°n AI Quoridor Project th·ªÉ hi·ªán m·ªôt implementation comprehensive c·ªßa modern
game AI techniques:

TECHNICAL INNOVATIONS:
‚úÖ Hybrid Q-Learning + Minimax architecture
‚úÖ 6 specialized AI personalities v·ªõi separate training
‚úÖ Advanced decision making v·ªõi entropy analysis
‚úÖ Comprehensive testing v√† validation framework
‚úÖ Professional development tools v√† debugging

EDUCATIONAL VALUE:
‚úÖ Practical machine learning implementation
‚úÖ Traditional game AI algorithms
‚úÖ Modern software engineering practices
‚úÖ Performance optimization techniques
‚úÖ Automated testing methodologies

FUTURE ENHANCEMENTS:
üöÄ Deep Q-Learning (DQN) integration
üöÄ Multi-agent reinforcement learning
üöÄ Neural network-based evaluation functions
üöÄ Online learning v√† adaptation
üöÄ Transfer learning between game variants

D·ª± √°n n√†y demonstrating successful integration c·ªßa classical AI methods v·ªõi
modern machine learning, t·∫°o ra m·ªôt intelligent, adaptive, v√† highly
interactive gaming experience.

================================================================================
                            APPENDIX
================================================================================

FILES LI√äN QUAN:
- Assets/scripts/QLearningAgent.cs - Q-Learning implementation
- Assets/scripts/QuoridorAI.cs - Main AI controller
- Assets/scripts/DecisionTreeClassifier.cs - Decision tree logic
- Assets/scripts/QuoridorSelfPlayTrainer.cs - Training system
- Assets/scripts/AITestScript.cs - Testing framework
- Assets/scripts/Editor/ForceRetrainAI.cs - Development tools

Q-TABLE FILES:
- StreamingAssets/qtable.json - Balanced style
- StreamingAssets/qtable_aggressive.json - Aggressive style
- StreamingAssets/qtable_defensive.json - Defensive style
- StreamingAssets/qtable_wallmaster.json - Wall Master style
- StreamingAssets/qtable_speedrunner.json - Speed Runner style
- StreamingAssets/qtable_tactical.json - Tactical style

DOCUMENTATION:
- AI_ALGORITHMS_DETAILED_DESCRIPTION.txt - This file
- README.md - Project overview
- API_INTEGRATION_GUIDE.md - Voice/Chat integration
- TRAINING_GUIDE.md - AI training instructions

================================================================================
================================================================================
                    AI QUORIDOR GAME PROJECT - THU·∫¨T TO√ÅN AI CHI TI·∫æT
================================================================================

D·ª± √°n: AI Quoridor Game v·ªõi Machine Learning v√† Multi-Platform Integration
Ng√†y t·∫°o: 25/07/2025
T√°c gi·∫£: [Student Name]
Platform: Unity 3D
Language: C#

================================================================================
                            M·ª§C L·ª§C
================================================================================

1. T·ªîNG QUAN KI·∫æN TR√öC AI
2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
4. SPECIALIZED AI STYLES SYSTEM
5. DECISION TREE CLASSIFIER
6. ENTROPY ANALYSIS & SOFTMAX
7. TRAINING SYSTEM & SELF-PLAY
8. AI TESTING FRAMEWORK
9. HYBRID AI ARCHITECTURE
10. PERFORMANCE OPTIMIZATION

================================================================================
                        1. T·ªîNG QUAN KI·∫æN TR√öC AI
================================================================================

D·ª± √°n AI Quoridor s·ª≠ d·ª•ng m·ªôt ki·∫øn tr√∫c AI hybrid ti√™n ti·∫øn, k·∫øt h·ª£p:

1.1 CORE ALGORITHMS:
- Q-Learning (Reinforcement Learning) - Thu·∫≠t to√°n ch√≠nh
- Minimax + Alpha-Beta Pruning - Thu·∫≠t to√°n fallback
- Decision Tree Classifier - Ph√¢n t√≠ch ng·ªØ c·∫£nh game
- Entropy Analysis - ƒê√°nh gi√° ƒëa d·∫°ng move

1.2 SPECIALIZED SYSTEMS:
- 6 AI Personality Styles v·ªõi training ri√™ng bi·ªát
- Self-Play Training Pipeline t·ª± ƒë·ªông
- Real-time Performance Testing Framework
- Dynamic Algorithm Switching

1.3 TECHNICAL FOUNDATION:
- State Space: 13,118+ unique game states
- Action Space: Movement + Wall placement combinations
- Training Method: Self-play v·ªõi 1000 episodes
- Learning Rate: Œ± = 0.1, Discount Factor: Œ≥ = 0.99

================================================================================
                    2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
================================================================================

2.1 GI·ªöI THI·ªÜU Q-LEARNING:
Q-Learning l√† thu·∫≠t to√°n Reinforcement Learning kh√¥ng c·∫ßn model, h·ªçc th√¥ng qua
t∆∞∆°ng t√°c v·ªõi m√¥i tr∆∞·ªùng ƒë·ªÉ t·ªëi ∆∞u h√≥a policy. Trong Quoridor, AI h·ªçc c√°ch
ch·ªçn moves t·ªëi ∆∞u d·ª±a tr√™n kinh nghi·ªám t·ª´ h√†ng ng√†n game.

2.2 C√îNG TH·ª®C TO√ÅN H·ªåC:
Q(s,a) = Q(s,a) + Œ±[r + Œ≥*max(Q(s',a')) - Q(s,a)]

Trong ƒë√≥:
- Q(s,a): Q-value c·ªßa state s v·ªõi action a
- Œ±: Learning rate (0.1) - t·ªëc ƒë·ªô h·ªçc
- r: Immediate reward t·ª´ action
- Œ≥: Discount factor (0.99) - t·∫ßm quan tr·ªçng future rewards
- s': Next state sau khi th·ª±c hi·ªán action
- max(Q(s',a')): Maximum Q-value c·ªßa next state

2.3 IMPLEMENTATION CHI TI·∫æT:

```csharp
public class QLearningAgent
{
    // Core Q-Table: Dictionary l∆∞u tr·ªØ Q-values
    private Dictionary<string, Dictionary<string, float>> qTable;
    
    // Hyperparameters
    private float learningRate = 0.1f;      // T·ªëc ƒë·ªô h·ªçc
    private float discountFactor = 0.99f;   // Discount future rewards
    private float epsilon = 0.995f;         // Exploration rate
    
    // Q-Learning Update Implementation
    public void UpdateQValue(string state, string action, float reward, string nextState)
    {
        // L·∫•y Q-value hi·ªán t·∫°i
        float currentQ = GetQValue(state, action);
        
        // T√¨m max Q-value c·ªßa next state
        float maxNextQ = GetMaxQValue(nextState);
        
        // √Åp d·ª•ng c√¥ng th·ª©c Q-Learning
        float newQ = currentQ + learningRate * (reward + discountFactor * maxNextQ - currentQ);
        
        // C·∫≠p nh·∫≠t Q-table
        SetQValue(state, action, newQ);
    }
}
```

2.4 STATE ENCODING:
Game state ƒë∆∞·ª£c encode th√†nh string duy nh·∫•t:
"aiPos_x,aiPos_y-humanPos_x,humanPos_y-aiWalls-humanWalls-wallsList"

V√≠ d·ª•: "4,8-4,0-9-10-7,4,False|5,6,True"
- AI ·ªü (4,8), Human ·ªü (4,0)
- AI c√≥ 9 walls, Human c√≥ 10 walls
- Walls ƒë√£ ƒë·∫∑t: (7,4,False), (5,6,True)

2.5 ACTION ENCODING:
Actions ƒë∆∞·ª£c encode th√†nh string:
- Movement: "M:x,y" (v√≠ d·ª•: "M:4,7")
- Wall: "W:x,y,horizontal" (v√≠ d·ª•: "W:3,5,True")

2.6 EPSILON-GREEDY STRATEGY:
```csharp
public AIMove ChooseAction(GameState state, List<AIMove> possibleMoves)
{
    // Exploration vs Exploitation trade-off
    if (Random.Range(0f, 1f) < epsilon)
    {
        // EXPLORATION: Ch·ªçn random ƒë·ªÉ kh√°m ph√°
        return possibleMoves[Random.Range(0, possibleMoves.Count)];
    }
    else
    {
        // EXPLOITATION: Ch·ªçn action c√≥ Q-value cao nh·∫•t
        return GetBestAction(state, possibleMoves);
    }
}
```

2.7 EPSILON DECAY:
Epsilon gi·∫£m d·∫ßn t·ª´ 0.995 ‚Üí 0.1 qua training:
- Ban ƒë·∫ßu: High exploration (99.5%)
- Cu·ªëi training: High exploitation (90%)

2.8 REWARD SYSTEM:
- Win game: +1000
- Lose game: -1000
- Move toward goal: +10
- Block opponent: +50
- Waste move: -5
- Strategic position: +15

================================================================================
                3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
================================================================================

3.1 GI·ªöI THI·ªÜU MINIMAX:
Minimax l√† thu·∫≠t to√°n game theory c·ªï ƒëi·ªÉn, t√¨m ki·∫øm move t·ªëi ∆∞u b·∫±ng c√°ch
d·ª± ƒëo√°n tr∆∞·ªõc nhi·ªÅu moves. Alpha-Beta pruning t·ªëi ∆∞u hi·ªáu su·∫•t b·∫±ng c√°ch
lo·∫°i b·ªè c√°c nh√°nh kh√¥ng c·∫ßn thi·∫øt.

3.2 NGUY√äN L√ù HO·∫†T ƒê·ªòNG:
- Maximizing Player (AI): T·ªëi ƒëa h√≥a score
- Minimizing Player (Human): T·ªëi thi·ªÉu h√≥a score AI
- Search depth: 4 levels (configurable)
- Pruning: Lo·∫°i b·ªè nh√°nh kh√¥ng promising

3.3 IMPLEMENTATION:

```csharp
float Minimax(GameState state, int depth, bool isMaximizing, float alpha, float beta)
{
    // Base case: Depth = 0 ho·∫∑c game over
    if (depth == 0 || IsGameOver(state))
        return EvaluatePosition(state);
    
    if (isMaximizing) // AI turn
    {
        float maxEval = float.NegativeInfinity;
        foreach (var move in GeneratePossibleMoves(state, true))
        {
            GameState newState = ApplyMove(state, move, true);
            float eval = Minimax(newState, depth - 1, false, alpha, beta);
            maxEval = Mathf.Max(maxEval, eval);
            alpha = Mathf.Max(alpha, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return maxEval;
    }
    else // Human turn
    {
        float minEval = float.PositiveInfinity;
        foreach (var move in GeneratePossibleMoves(state, false))
        {
            GameState newState = ApplyMove(state, move, false);
            float eval = Minimax(newState, depth - 1, true, alpha, beta);
            minEval = Mathf.Min(minEval, eval);
            beta = Mathf.Min(beta, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return minEval;
    }
}
```

3.4 EVALUATION FUNCTION:
Position evaluation d·ª±a tr√™n multiple factors:

```csharp
float EvaluatePosition(GameState state)
{
    float score = 0f;
    
    // Distance to goal (c√†ng g·∫ßn c√†ng t·ªët)
    int aiDistance = GetShortestPath(state.aiPosition, goalRow);
    int humanDistance = GetShortestPath(state.humanPosition, goalRow);
    score += (humanDistance - aiDistance) * 100;
    
    // Wall advantage
    score += (state.aiWallsLeft - state.humanWallsLeft) * 50;
    
    // Mobility (s·ªë moves c√≥ th·ªÉ)
    int aiMobility = GetValidMoves(state.aiPosition).Count;
    int humanMobility = GetValidMoves(state.humanPosition).Count;
    score += (aiMobility - humanMobility) * 20;
    
    // Center control
    score += GetCenterControlScore(state) * 30;
    
    return score;
}
```

3.5 ALPHA-BETA PRUNING HI·ªÜU QU·∫¢:
- Reduces search space t·ª´ O(b^d) xu·ªëng O(b^(d/2))
- Best case: 50% reduction in nodes evaluated
- Ordering moves by promising gi√∫p pruning hi·ªáu qu·∫£ h∆°n

================================================================================
                        4. SPECIALIZED AI STYLES SYSTEM
================================================================================

4.1 CONCEPT:
6 AI personalities v·ªõi training v√† behavior ri√™ng bi·ªát, m·ªói style c√≥:
- Ri√™ng Q-table file
- Custom reward structure
- Specific move prioritization
- Unique decision making logic

4.2 AI STYLES CHI TI·∫æT:

üî• AGGRESSIVE STYLE:
- M·ª•c ti√™u: T·∫•n c√¥ng v√† ch·∫∑n ƒë·ªëi th·ªß
- Wall Usage: 70% chance
- Reward Structure:
  * blockingOpponentReward = 50 * intensity
  * wallPlacementReward = 30 * intensity
  * defensiveWallPenalty = -20 * intensity
- Q-table: qtable_aggressive.json

üõ°Ô∏è DEFENSIVE STYLE:
- M·ª•c ti√™u: Ph√≤ng th·ªß v√† di chuy·ªÉn an to√†n
- Wall Usage: 50% chance
- Reward Structure:
  * defensiveWallReward = 35 * intensity
  * movementReward = 20 * intensity
  * aggressiveMovePenalty = -15 * intensity
- Q-table: qtable_defensive.json

üß± WALL MASTER STYLE:
- M·ª•c ti√™u: Chuy√™n gia ƒë·∫∑t t∆∞·ªùng v√† t·∫°o m√™ cung
- Wall Usage: 90% chance
- Reward Structure:
  * wallPlacementReward = 60 * intensity
  * strategicWallReward = 40 * intensity
  * wallComboReward = 50 * intensity
  * movementWithoutWallPenalty = -25 * intensity
- Q-table: qtable_wallmaster.json

‚ö° SPEED RUNNER STYLE:
- M·ª•c ti√™u: ƒê·∫°t ƒë√≠ch nhanh nh·∫•t
- Wall Usage: 10% chance
- Reward Structure:
  * movementReward = 40 * intensity
  * progressToGoalReward = 50 * intensity
  * quickWinBonus = 100 * intensity
  * wallPlacementPenalty = -20 * intensity
- Q-table: qtable_speedrunner.json

üß† TACTICAL STYLE:
- M·ª•c ti√™u: L·ª£i th·∫ø v·ªã tr√≠ v√† k·∫ø ho·∫°ch d√†i h·∫°n
- Wall Usage: Variable (strategic)
- Reward Structure:
  * tacticalMoveReward = 45 * intensity
  * positionAdvantageReward = 35 * intensity
  * mobilityReward = 25 * intensity
  * patternRecognitionBonus = 40 * intensity
- Q-table: qtable_tactical.json

‚öñÔ∏è BALANCED STYLE:
- M·ª•c ti√™u: C√¢n b·∫±ng t·∫•t c·∫£ aspects
- Wall Usage: 30% chance
- Reward Structure: Default balanced values
- Q-table: qtable.json

4.3 STYLE SWITCHING MECHANISM:

```csharp
public void SwitchToStyle(PlayStyle newStyle)
{
    // Save current Q-table
    if (allowQTableSaving) qAgent.SaveQTable(GetStyleSpecificQTablePath());
    
    // Load new style Q-table
    specializedStyle = newStyle;
    qAgent.LoadQTable(GetStyleSpecificQTablePath());
    
    // Reconfigure parameters
    ConfigureStyleSpecificTraining();
}
```

4.4 STYLE INTENSITY:
Parameter t·ª´ 0.0 ‚Üí 1.0 control m·ª©c ƒë·ªô chuy√™n h√≥a:
- 0.0: Balanced behavior
- 0.5: Moderate specialization
- 1.0: Extreme specialization

================================================================================
                        5. DECISION TREE CLASSIFIER
================================================================================

5.1 PURPOSE:
Decision Tree ph√¢n t√≠ch game context ƒë·ªÉ ch·ªçn strategy ph√π h·ª£p, d·ª±a tr√™n:
- Game phase (Early/Mid/End)
- Position advantage
- Resource advantage
- Mobility situation

5.2 FEATURE EXTRACTION:

```csharp
public struct DecisionFeatures
{
    public int aiDistanceToGoal;      // AI distance to goal line
    public int humanDistanceToGoal;   // Human distance to goal line
    public int aiWallsLeft;           // AI walls remaining
    public int humanWallsLeft;        // Human walls remaining
    public int aiMobility;            // AI possible moves
    public int humanMobility;         // Human possible moves
    public float distanceAdvantage;   // Human - AI distance
    public float wallAdvantage;       // AI - Human walls
    public bool isEarlyGame;          // > 15 moves estimated remaining
    public bool isMidGame;            // 5-15 moves remaining
    public bool isEndGame;            // < 5 moves remaining
}
```

5.3 DECISION LOGIC:

```csharp
public AIStrategy Predict(DecisionFeatures features)
{
    // Early Game: Play conservatively
    if (features.isEarlyGame)
        return AIStrategy.Balanced;
    
    // AI has significant lead: Focus on blocking
    if (features.distanceAdvantage > 2)
        return AIStrategy.Blocking;
    
    // AI is behind: Be aggressive
    if (features.distanceAdvantage < -1)
        return AIStrategy.Aggressive;
    
    // End game with wall advantage: Play defensively
    if (features.isEndGame && features.wallAdvantage > 2)
        return AIStrategy.Defensive;
    
    // Close game: Balanced approach
    return AIStrategy.Balanced;
}
```

5.4 STRATEGY OUTCOMES:
- Balanced: Equal weight to movement v√† walls
- Blocking: Prioritize moves that increase opponent path
- Aggressive: Focus on attacking positions
- Defensive: Protect own position and path

================================================================================
                        6. ENTROPY ANALYSIS & SOFTMAX
================================================================================

6.1 ENTROPY CONCEPT:
Entropy ƒëo ƒë·ªô ƒëa d·∫°ng/uncertainty c·ªßa move scores. High entropy = nhi·ªÅu moves
c√≥ scores t∆∞∆°ng ƒë∆∞∆°ng, low entropy = m·ªôt move r√µ r√†ng t·ªët nh·∫•t.

6.2 ENTROPY CALCULATION:

```csharp
float CalculateEntropy(List<float> moveScores)
{
    // Normalize th√†nh probabilities
    float total = moveScores.Sum();
    var probabilities = moveScores.Select(s => s / total);
    
    // Calculate Shannon entropy
    float entropy = 0f;
    foreach (float p in probabilities)
    {
        if (p > 0f)
            entropy -= p * Mathf.Log(p, 2f);
    }
    
    return entropy; // Range: 0 (deterministic) to log2(n) (uniform)
}
```

6.3 SOFTMAX DISTRIBUTION:

```csharp
float[] CalculateSoftmax(List<float> scores, float temperature)
{
    float[] probabilities = new float[scores.Count];
    float sum = scores.Sum(s => Mathf.Exp(s / temperature));
    
    for (int i = 0; i < scores.Count; i++)
    {
        probabilities[i] = Mathf.Exp(scores[i] / temperature) / sum;
    }
    
    return probabilities;
}
```

6.4 TEMPERATURE PARAMETER:
- Low temperature (0.1): Sharp distribution, focus on best moves
- High temperature (2.0): Flat distribution, more exploration
- Default temperature (1.0): Balanced exploitation/exploration

6.5 ADAPTIVE MOVE SELECTION:

```csharp
public AIMove ChooseActionWithEntropy(GameState state, List<AIMove> possibleMoves)
{
    var moveScores = EvaluateAllMoves(state, possibleMoves);
    float entropy = CalculateEntropy(moveScores);
    
    if (entropy > entropyThreshold) // High diversity
    {
        // Use softmax for probabilistic selection
        var probabilities = CalculateSoftmax(moveScores, softmaxTemperature);
        return SelectMoveByProbability(possibleMoves, probabilities);
    }
    else // Clear best move
    {
        // Use deterministic best move
        return GetBestScoredMove(possibleMoves, moveScores);
    }
}
```

================================================================================
                        7. TRAINING SYSTEM & SELF-PLAY
================================================================================

7.1 SELF-PLAY CONCEPT:
AI h·ªçc b·∫±ng c√°ch ch∆°i v·ªõi ch√≠nh n√≥ h√†ng ng√†n l·∫ßn, gradually improve performance
th√¥ng qua trial v√† error.

7.2 TRAINING PIPELINE:

```csharp
IEnumerator RunSelfPlayTraining()
{
    for (int episode = 1; episode <= maxEpisodes; episode++)
    {
        // Reset game state
        ResetGameForTraining();
        
        while (!IsGameOver())
        {
            // Get current state
            GameState currentState = GetCurrentGameState();
            List<AIMove> possibleMoves = GetPossibleMoves();
            
            // Choose action using Œµ-greedy
            AIMove chosenMove = qLearningAgent.ChooseAction(currentState, possibleMoves);
            
            // Execute move
            ExecuteMove(chosenMove);
            
            // Calculate reward
            float reward = CalculateReward(currentState, chosenMove);
            
            // Update Q-table
            GameState newState = GetCurrentGameState();
            qLearningAgent.UpdateQValue(
                EncodeState(currentState),
                EncodeAction(chosenMove),
                reward,
                EncodeState(newState)
            );
            
            yield return new WaitForEndOfFrame();
        }
        
        // Epsilon decay
        qLearningAgent.DecayEpsilon();
        
        // Save progress periodically
        if (episode % 100 == 0)
        {
            qLearningAgent.SaveQTable();
        }
    }
}
```

7.3 REWARD ENGINEERING:
Critical cho Q-Learning success:

```csharp
float CalculateReward(GameState state, AIMove move)
{
    float reward = 0f;
    
    // Game outcome rewards
    if (IsWin(state)) reward += 1000f;
    if (IsLoss(state)) reward -= 1000f;
    
    // Progress rewards
    if (move.moveType == MoveType.Movement)
    {
        if (IsMoveTowardGoal(move)) reward += 20f;
        else reward -= 5f;
    }
    
    // Strategic rewards
    if (move.moveType == MoveType.WallPlacement)
    {
        if (IsWallBlockingOpponent(move)) reward += 50f;
        if (IsWallStrategic(move)) reward += 30f;
        if (IsWallWasted(move)) reward -= 25f;
    }
    
    // Style-specific rewards
    reward += GetStyleSpecificReward(state, move);
    
    return reward;
}
```

7.4 TRAINING METRICS:
- Episodes: 1000 standard
- Duration: ~15 minutes
- Convergence: Usually around episode 800
- Q-table size: 13,118+ states at completion
- Final epsilon: 0.1 (90% exploitation)

================================================================================
                        8. AI TESTING FRAMEWORK
================================================================================

8.1 TESTING PHILOSOPHY:
Comprehensive testing ensures AI quality v√† reliability across different
game scenarios v√† edge cases.

8.2 TEST CATEGORIES:

A) DECISION TREE TESTS:
```csharp
void TestDecisionTreeStrategies()
{
    // Test Early Game
    var earlyGameFeatures = new DecisionFeatures
    {
        aiDistanceToGoal = 8, humanDistanceToGoal = 8,
        isEarlyGame = true
    };
    var strategy = decisionTree.Predict(earlyGameFeatures);
    Assert.AreEqual(AIStrategy.Balanced, strategy);
    
    // Test AI Leading
    var leadingFeatures = new DecisionFeatures
    {
        distanceAdvantage = 3, // AI leading
        isMidGame = true
    };
    strategy = decisionTree.Predict(leadingFeatures);
    Assert.AreEqual(AIStrategy.Blocking, strategy);
}
```

B) ENTROPY ANALYSIS TESTS:
```csharp
void TestEntropyAnalysis()
{
    // High entropy case (diverse scores)
    var highEntropyScores = new List<float> { 100f, 98f, 97f, 95f, 94f };
    float entropy = CalculateTestEntropy(highEntropyScores);
    Assert.IsTrue(entropy > 0.5f);
    
    // Low entropy case (clear winner)
    var lowEntropyScores = new List<float> { 100f, 50f, 50f, 50f, 50f };
    entropy = CalculateTestEntropy(lowEntropyScores);
    Assert.IsTrue(entropy < 0.5f);
}
```

C) SOFTMAX DISTRIBUTION TESTS:
```csharp
void TestSoftmaxDistribution()
{
    var testScores = new List<float> { 100f, 98f, 95f, 90f };
    float temperature = 1.0f;
    
    var probabilities = CalculateSoftmax(testScores, temperature);
    
    // Verify probabilities sum to 1
    float sum = probabilities.Sum();
    Assert.AreEqual(1.0f, sum, 0.001f);
    
    // Verify highest score gets highest probability
    int maxIndex = Array.IndexOf(probabilities, probabilities.Max());
    Assert.AreEqual(0, maxIndex);
}
```

D) PERFORMANCE BENCHMARKS:
- Move generation time: <100ms
- Q-Learning decision: <50ms
- Minimax decision (depth 4): <200ms
- Memory usage: <256MB

8.3 AUTOMATED TESTING:
Tests run automatically ƒë·ªÉ ensure code quality:
- Unit tests cho individual algorithms
- Integration tests cho AI system interactions
- Performance tests cho response times
- Regression tests ƒë·ªÉ prevent quality degradation

================================================================================
                        9. HYBRID AI ARCHITECTURE
================================================================================

9.1 CONCEPT:
Hybrid system k·∫øt h·ª£p strengths c·ªßa multiple algorithms:
- Q-Learning: Data-driven, learns from experience
- Minimax: Logical, guaranteed optimal given evaluation
- Decision Tree: Fast, context-aware switching
- Entropy: Adaptive, handles uncertainty

9.2 ALGORITHM SELECTION LOGIC:

```csharp
public AIMove GetBestMove()
{
    UpdateGameState();
    List<AIMove> possibleMoves = GeneratePossibleMoves();
    
    // Primary: Use Q-Learning if trained model available
    if (useQLearning && qAgent.IsTrainedModel())
    {
        Debug.Log($"üß† Using {specializedStyle} Q-Learning");
        return qAgent.ChooseAction(currentGameState, possibleMoves);
    }
    
    // Fallback: Use Minimax for guaranteed reasonable play
    else
    {
        Debug.Log($"üîç Using {specializedStyle} Minimax");
        return FindBestMoveWithMinimax();
    }
}
```

9.3 SEAMLESS SWITCHING:
- No interruption in gameplay
- Maintains consistent API
- Preserves game state integrity
- Logs algorithm used for debugging

9.4 BENEFITS:
- Robustness: Fallback ensures AI never fails
- Performance: Q-Learning for trained scenarios, Minimax for edge cases
- Flexibility: Can switch algorithms based on situation
- Development: Easy to test individual components

================================================================================
                        10. PERFORMANCE OPTIMIZATION
================================================================================

10.1 Q-TABLE OPTIMIZATION:
- Sparse representation: Only store visited states
- Compression: String interning for state keys
- Lazy loading: Load Q-table on demand
- Periodic saving: Every 100 training episodes

10.2 MINIMAX OPTIMIZATION:
- Alpha-Beta pruning: 50% search reduction
- Move ordering: Promising moves first
- Iterative deepening: Gradual depth increase
- Transposition tables: Cache evaluated positions

10.3 MEMORY MANAGEMENT:
- Object pooling cho GameState objects
- String caching cho repeated state/action encodings
- Garbage collection optimization
- Memory profiling v√† monitoring

10.4 MULTITHREADING CONSIDERATIONS:
- Q-Learning training: Background thread
- Move calculation: Main thread (Unity requirement)
- File I/O: Async operations where possible
- Thread-safe collections cho shared data

10.5 CACHING STRATEGIES:
- Path finding results
- Move generation results
- Evaluation function results
- Feature extraction results

================================================================================
                            K·∫æT LU·∫¨N
================================================================================

D·ª± √°n AI Quoridor Project th·ªÉ hi·ªán m·ªôt implementation comprehensive c·ªßa modern
game AI techniques:

TECHNICAL INNOVATIONS:
‚úÖ Hybrid Q-Learning + Minimax architecture
‚úÖ 6 specialized AI personalities v·ªõi separate training
‚úÖ Advanced decision making v·ªõi entropy analysis
‚úÖ Comprehensive testing v√† validation framework
‚úÖ Professional development tools v√† debugging

EDUCATIONAL VALUE:
‚úÖ Practical machine learning implementation
‚úÖ Traditional game AI algorithms
‚úÖ Modern software engineering practices
‚úÖ Performance optimization techniques
‚úÖ Automated testing methodologies

FUTURE ENHANCEMENTS:
üöÄ Deep Q-Learning (DQN) integration
üöÄ Multi-agent reinforcement learning
üöÄ Neural network-based evaluation functions
üöÄ Online learning v√† adaptation
üöÄ Transfer learning between game variants

D·ª± √°n n√†y demonstrating successful integration c·ªßa classical AI methods v·ªõi
modern machine learning, t·∫°o ra m·ªôt intelligent, adaptive, v√† highly
interactive gaming experience.

================================================================================
                            APPENDIX
================================================================================

FILES LI√äN QUAN:
- Assets/scripts/QLearningAgent.cs - Q-Learning implementation
- Assets/scripts/QuoridorAI.cs - Main AI controller
- Assets/scripts/DecisionTreeClassifier.cs - Decision tree logic
- Assets/scripts/QuoridorSelfPlayTrainer.cs - Training system
- Assets/scripts/AITestScript.cs - Testing framework
- Assets/scripts/Editor/ForceRetrainAI.cs - Development tools

Q-TABLE FILES:
- StreamingAssets/qtable.json - Balanced style
- StreamingAssets/qtable_aggressive.json - Aggressive style
- StreamingAssets/qtable_defensive.json - Defensive style
- StreamingAssets/qtable_wallmaster.json - Wall Master style
- StreamingAssets/qtable_speedrunner.json - Speed Runner style
- StreamingAssets/qtable_tactical.json - Tactical style

DOCUMENTATION:
- AI_ALGORITHMS_DETAILED_DESCRIPTION.txt - This file
- README.md - Project overview
- API_INTEGRATION_GUIDE.md - Voice/Chat integration
- TRAINING_GUIDE.md - AI training instructions

================================================================================
================================================================================
                    AI QUORIDOR GAME PROJECT - THU·∫¨T TO√ÅN AI CHI TI·∫æT
================================================================================

D·ª± √°n: AI Quoridor Game v·ªõi Machine Learning v√† Multi-Platform Integration
Ng√†y t·∫°o: 25/07/2025
T√°c gi·∫£: [Student Name]
Platform: Unity 3D
Language: C#

================================================================================
                            M·ª§C L·ª§C
================================================================================

1. T·ªîNG QUAN KI·∫æN TR√öC AI
2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
4. SPECIALIZED AI STYLES SYSTEM
5. DECISION TREE CLASSIFIER
6. ENTROPY ANALYSIS & SOFTMAX
7. TRAINING SYSTEM & SELF-PLAY
8. AI TESTING FRAMEWORK
9. HYBRID AI ARCHITECTURE
10. PERFORMANCE OPTIMIZATION

================================================================================
                        1. T·ªîNG QUAN KI·∫æN TR√öC AI
================================================================================

D·ª± √°n AI Quoridor s·ª≠ d·ª•ng m·ªôt ki·∫øn tr√∫c AI hybrid ti√™n ti·∫øn, k·∫øt h·ª£p:

1.1 CORE ALGORITHMS:
- Q-Learning (Reinforcement Learning) - Thu·∫≠t to√°n ch√≠nh
- Minimax + Alpha-Beta Pruning - Thu·∫≠t to√°n fallback
- Decision Tree Classifier - Ph√¢n t√≠ch ng·ªØ c·∫£nh game
- Entropy Analysis - ƒê√°nh gi√° ƒëa d·∫°ng move

1.2 SPECIALIZED SYSTEMS:
- 6 AI Personality Styles v·ªõi training ri√™ng bi·ªát
- Self-Play Training Pipeline t·ª± ƒë·ªông
- Real-time Performance Testing Framework
- Dynamic Algorithm Switching

1.3 TECHNICAL FOUNDATION:
- State Space: 13,118+ unique game states
- Action Space: Movement + Wall placement combinations
- Training Method: Self-play v·ªõi 1000 episodes
- Learning Rate: Œ± = 0.1, Discount Factor: Œ≥ = 0.99

================================================================================
                    2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
================================================================================

2.1 GI·ªöI THI·ªÜU Q-LEARNING:
Q-Learning l√† thu·∫≠t to√°n Reinforcement Learning kh√¥ng c·∫ßn model, h·ªçc th√¥ng qua
t∆∞∆°ng t√°c v·ªõi m√¥i tr∆∞·ªùng ƒë·ªÉ t·ªëi ∆∞u h√≥a policy. Trong Quoridor, AI h·ªçc c√°ch
ch·ªçn moves t·ªëi ∆∞u d·ª±a tr√™n kinh nghi·ªám t·ª´ h√†ng ng√†n game.

2.2 C√îNG TH·ª®C TO√ÅN H·ªåC:
Q(s,a) = Q(s,a) + Œ±[r + Œ≥*max(Q(s',a')) - Q(s,a)]

Trong ƒë√≥:
- Q(s,a): Q-value c·ªßa state s v·ªõi action a
- Œ±: Learning rate (0.1) - t·ªëc ƒë·ªô h·ªçc
- r: Immediate reward t·ª´ action
- Œ≥: Discount factor (0.99) - t·∫ßm quan tr·ªçng future rewards
- s': Next state sau khi th·ª±c hi·ªán action
- max(Q(s',a')): Maximum Q-value c·ªßa next state

2.3 IMPLEMENTATION CHI TI·∫æT:

```csharp
public class QLearningAgent
{
    // Core Q-Table: Dictionary l∆∞u tr·ªØ Q-values
    private Dictionary<string, Dictionary<string, float>> qTable;
    
    // Hyperparameters
    private float learningRate = 0.1f;      // T·ªëc ƒë·ªô h·ªçc
    private float discountFactor = 0.99f;   // Discount future rewards
    private float epsilon = 0.995f;         // Exploration rate
    
    // Q-Learning Update Implementation
    public void UpdateQValue(string state, string action, float reward, string nextState)
    {
        // L·∫•y Q-value hi·ªán t·∫°i
        float currentQ = GetQValue(state, action);
        
        // T√¨m max Q-value c·ªßa next state
        float maxNextQ = GetMaxQValue(nextState);
        
        // √Åp d·ª•ng c√¥ng th·ª©c Q-Learning
        float newQ = currentQ + learningRate * (reward + discountFactor * maxNextQ - currentQ);
        
        // C·∫≠p nh·∫≠t Q-table
        SetQValue(state, action, newQ);
    }
}
```

2.4 STATE ENCODING:
Game state ƒë∆∞·ª£c encode th√†nh string duy nh·∫•t:
"aiPos_x,aiPos_y-humanPos_x,humanPos_y-aiWalls-humanWalls-wallsList"

V√≠ d·ª•: "4,8-4,0-9-10-7,4,False|5,6,True"
- AI ·ªü (4,8), Human ·ªü (4,0)
- AI c√≥ 9 walls, Human c√≥ 10 walls
- Walls ƒë√£ ƒë·∫∑t: (7,4,False), (5,6,True)

2.5 ACTION ENCODING:
Actions ƒë∆∞·ª£c encode th√†nh string:
- Movement: "M:x,y" (v√≠ d·ª•: "M:4,7")
- Wall: "W:x,y,horizontal" (v√≠ d·ª•: "W:3,5,True")

2.6 EPSILON-GREEDY STRATEGY:
```csharp
public AIMove ChooseAction(GameState state, List<AIMove> possibleMoves)
{
    // Exploration vs Exploitation trade-off
    if (Random.Range(0f, 1f) < epsilon)
    {
        // EXPLORATION: Ch·ªçn random ƒë·ªÉ kh√°m ph√°
        return possibleMoves[Random.Range(0, possibleMoves.Count)];
    }
    else
    {
        // EXPLOITATION: Ch·ªçn action c√≥ Q-value cao nh·∫•t
        return GetBestAction(state, possibleMoves);
    }
}
```

2.7 EPSILON DECAY:
Epsilon gi·∫£m d·∫ßn t·ª´ 0.995 ‚Üí 0.1 qua training:
- Ban ƒë·∫ßu: High exploration (99.5%)
- Cu·ªëi training: High exploitation (90%)

2.8 REWARD SYSTEM:
- Win game: +1000
- Lose game: -1000
- Move toward goal: +10
- Block opponent: +50
- Waste move: -5
- Strategic position: +15

================================================================================
                3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
================================================================================

3.1 GI·ªöI THI·ªÜU MINIMAX:
Minimax l√† thu·∫≠t to√°n game theory c·ªï ƒëi·ªÉn, t√¨m ki·∫øm move t·ªëi ∆∞u b·∫±ng c√°ch
d·ª± ƒëo√°n tr∆∞·ªõc nhi·ªÅu moves. Alpha-Beta pruning t·ªëi ∆∞u hi·ªáu su·∫•t b·∫±ng c√°ch
lo·∫°i b·ªè c√°c nh√°nh kh√¥ng c·∫ßn thi·∫øt.

3.2 NGUY√äN L√ù HO·∫†T ƒê·ªòNG:
- Maximizing Player (AI): T·ªëi ƒëa h√≥a score
- Minimizing Player (Human): T·ªëi thi·ªÉu h√≥a score AI
- Search depth: 4 levels (configurable)
- Pruning: Lo·∫°i b·ªè nh√°nh kh√¥ng promising

3.3 IMPLEMENTATION:

```csharp
float Minimax(GameState state, int depth, bool isMaximizing, float alpha, float beta)
{
    // Base case: Depth = 0 ho·∫∑c game over
    if (depth == 0 || IsGameOver(state))
        return EvaluatePosition(state);
    
    if (isMaximizing) // AI turn
    {
        float maxEval = float.NegativeInfinity;
        foreach (var move in GeneratePossibleMoves(state, true))
        {
            GameState newState = ApplyMove(state, move, true);
            float eval = Minimax(newState, depth - 1, false, alpha, beta);
            maxEval = Mathf.Max(maxEval, eval);
            alpha = Mathf.Max(alpha, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return maxEval;
    }
    else // Human turn
    {
        float minEval = float.PositiveInfinity;
        foreach (var move in GeneratePossibleMoves(state, false))
        {
            GameState newState = ApplyMove(state, move, false);
            float eval = Minimax(newState, depth - 1, true, alpha, beta);
            minEval = Mathf.Min(minEval, eval);
            beta = Mathf.Min(beta, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return minEval;
    }
}
```

3.4 EVALUATION FUNCTION:
Position evaluation d·ª±a tr√™n multiple factors:

```csharp
float EvaluatePosition(GameState state)
{
    float score = 0f;
    
    // Distance to goal (c√†ng g·∫ßn c√†ng t·ªët)
    int aiDistance = GetShortestPath(state.aiPosition, goalRow);
    int humanDistance = GetShortestPath(state.humanPosition, goalRow);
    score += (humanDistance - aiDistance) * 100;
    
    // Wall advantage
    score += (state.aiWallsLeft - state.humanWallsLeft) * 50;
    
    // Mobility (s·ªë moves c√≥ th·ªÉ)
    int aiMobility = GetValidMoves(state.aiPosition).Count;
    int humanMobility = GetValidMoves(state.humanPosition).Count;
    score += (aiMobility - humanMobility) * 20;
    
    // Center control
    score += GetCenterControlScore(state) * 30;
    
    return score;
}
```

3.5 ALPHA-BETA PRUNING HI·ªÜU QU·∫¢:
- Reduces search space t·ª´ O(b^d) xu·ªëng O(b^(d/2))
- Best case: 50% reduction in nodes evaluated
- Ordering moves by promising gi√∫p pruning hi·ªáu qu·∫£ h∆°n

================================================================================
                        4. SPECIALIZED AI STYLES SYSTEM
================================================================================

4.1 CONCEPT:
6 AI personalities v·ªõi training v√† behavior ri√™ng bi·ªát, m·ªói style c√≥:
- Ri√™ng Q-table file
- Custom reward structure
- Specific move prioritization
- Unique decision making logic

4.2 AI STYLES CHI TI·∫æT:

üî• AGGRESSIVE STYLE:
- M·ª•c ti√™u: T·∫•n c√¥ng v√† ch·∫∑n ƒë·ªëi th·ªß
- Wall Usage: 70% chance
- Reward Structure:
  * blockingOpponentReward = 50 * intensity
  * wallPlacementReward = 30 * intensity
  * defensiveWallPenalty = -20 * intensity
- Q-table: qtable_aggressive.json

üõ°Ô∏è DEFENSIVE STYLE:
- M·ª•c ti√™u: Ph√≤ng th·ªß v√† di chuy·ªÉn an to√†n
- Wall Usage: 50% chance
- Reward Structure:
  * defensiveWallReward = 35 * intensity
  * movementReward = 20 * intensity
  * aggressiveMovePenalty = -15 * intensity
- Q-table: qtable_defensive.json

üß± WALL MASTER STYLE:
- M·ª•c ti√™u: Chuy√™n gia ƒë·∫∑t t∆∞·ªùng v√† t·∫°o m√™ cung
- Wall Usage: 90% chance
- Reward Structure:
  * wallPlacementReward = 60 * intensity
  * strategicWallReward = 40 * intensity
  * wallComboReward = 50 * intensity
  * movementWithoutWallPenalty = -25 * intensity
- Q-table: qtable_wallmaster.json

‚ö° SPEED RUNNER STYLE:
- M·ª•c ti√™u: ƒê·∫°t ƒë√≠ch nhanh nh·∫•t
- Wall Usage: 10% chance
- Reward Structure:
  * movementReward = 40 * intensity
  * progressToGoalReward = 50 * intensity
  * quickWinBonus = 100 * intensity
  * wallPlacementPenalty = -20 * intensity
- Q-table: qtable_speedrunner.json

üß† TACTICAL STYLE:
- M·ª•c ti√™u: L·ª£i th·∫ø v·ªã tr√≠ v√† k·∫ø ho·∫°ch d√†i h·∫°n
- Wall Usage: Variable (strategic)
- Reward Structure:
  * tacticalMoveReward = 45 * intensity
  * positionAdvantageReward = 35 * intensity
  * mobilityReward = 25 * intensity
  * patternRecognitionBonus = 40 * intensity
- Q-table: qtable_tactical.json

‚öñÔ∏è BALANCED STYLE:
- M·ª•c ti√™u: C√¢n b·∫±ng t·∫•t c·∫£ aspects
- Wall Usage: 30% chance
- Reward Structure: Default balanced values
- Q-table: qtable.json

4.3 STYLE SWITCHING MECHANISM:

```csharp
public void SwitchToStyle(PlayStyle newStyle)
{
    // Save current Q-table
    if (allowQTableSaving) qAgent.SaveQTable(GetStyleSpecificQTablePath());
    
    // Load new style Q-table
    specializedStyle = newStyle;
    qAgent.LoadQTable(GetStyleSpecificQTablePath());
    
    // Reconfigure parameters
    ConfigureStyleSpecificTraining();
}
```

4.4 STYLE INTENSITY:
Parameter t·ª´ 0.0 ‚Üí 1.0 control m·ª©c ƒë·ªô chuy√™n h√≥a:
- 0.0: Balanced behavior
- 0.5: Moderate specialization
- 1.0: Extreme specialization

================================================================================
                        5. DECISION TREE CLASSIFIER
================================================================================

5.1 PURPOSE:
Decision Tree ph√¢n t√≠ch game context ƒë·ªÉ ch·ªçn strategy ph√π h·ª£p, d·ª±a tr√™n:
- Game phase (Early/Mid/End)
- Position advantage
- Resource advantage
- Mobility situation

5.2 FEATURE EXTRACTION:

```csharp
public struct DecisionFeatures
{
    public int aiDistanceToGoal;      // AI distance to goal line
    public int humanDistanceToGoal;   // Human distance to goal line
    public int aiWallsLeft;           // AI walls remaining
    public int humanWallsLeft;        // Human walls remaining
    public int aiMobility;            // AI possible moves
    public int humanMobility;         // Human possible moves
    public float distanceAdvantage;   // Human - AI distance
    public float wallAdvantage;       // AI - Human walls
    public bool isEarlyGame;          // > 15 moves estimated remaining
    public bool isMidGame;            // 5-15 moves remaining
    public bool isEndGame;            // < 5 moves remaining
}
```

5.3 DECISION LOGIC:

```csharp
public AIStrategy Predict(DecisionFeatures features)
{
    // Early Game: Play conservatively
    if (features.isEarlyGame)
        return AIStrategy.Balanced;
    
    // AI has significant lead: Focus on blocking
    if (features.distanceAdvantage > 2)
        return AIStrategy.Blocking;
    
    // AI is behind: Be aggressive
    if (features.distanceAdvantage < -1)
        return AIStrategy.Aggressive;
    
    // End game with wall advantage: Play defensively
    if (features.isEndGame && features.wallAdvantage > 2)
        return AIStrategy.Defensive;
    
    // Close game: Balanced approach
    return AIStrategy.Balanced;
}
```

5.4 STRATEGY OUTCOMES:
- Balanced: Equal weight to movement v√† walls
- Blocking: Prioritize moves that increase opponent path
- Aggressive: Focus on attacking positions
- Defensive: Protect own position and path

================================================================================
                        6. ENTROPY ANALYSIS & SOFTMAX
================================================================================

6.1 ENTROPY CONCEPT:
Entropy ƒëo ƒë·ªô ƒëa d·∫°ng/uncertainty c·ªßa move scores. High entropy = nhi·ªÅu moves
c√≥ scores t∆∞∆°ng ƒë∆∞∆°ng, low entropy = m·ªôt move r√µ r√†ng t·ªët nh·∫•t.

6.2 ENTROPY CALCULATION:

```csharp
float CalculateEntropy(List<float> moveScores)
{
    // Normalize th√†nh probabilities
    float total = moveScores.Sum();
    var probabilities = moveScores.Select(s => s / total);
    
    // Calculate Shannon entropy
    float entropy = 0f;
    foreach (float p in probabilities)
    {
        if (p > 0f)
            entropy -= p * Mathf.Log(p, 2f);
    }
    
    return entropy; // Range: 0 (deterministic) to log2(n) (uniform)
}
```

6.3 SOFTMAX DISTRIBUTION:

```csharp
float[] CalculateSoftmax(List<float> scores, float temperature)
{
    float[] probabilities = new float[scores.Count];
    float sum = scores.Sum(s => Mathf.Exp(s / temperature));
    
    for (int i = 0; i < scores.Count; i++)
    {
        probabilities[i] = Mathf.Exp(scores[i] / temperature) / sum;
    }
    
    return probabilities;
}
```

6.4 TEMPERATURE PARAMETER:
- Low temperature (0.1): Sharp distribution, focus on best moves
- High temperature (2.0): Flat distribution, more exploration
- Default temperature (1.0): Balanced exploitation/exploration

6.5 ADAPTIVE MOVE SELECTION:

```csharp
public AIMove ChooseActionWithEntropy(GameState state, List<AIMove> possibleMoves)
{
    var moveScores = EvaluateAllMoves(state, possibleMoves);
    float entropy = CalculateEntropy(moveScores);
    
    if (entropy > entropyThreshold) // High diversity
    {
        // Use softmax for probabilistic selection
        var probabilities = CalculateSoftmax(moveScores, softmaxTemperature);
        return SelectMoveByProbability(possibleMoves, probabilities);
    }
    else // Clear best move
    {
        // Use deterministic best move
        return GetBestScoredMove(possibleMoves, moveScores);
    }
}
```

================================================================================
                        7. TRAINING SYSTEM & SELF-PLAY
================================================================================

7.1 SELF-PLAY CONCEPT:
AI h·ªçc b·∫±ng c√°ch ch∆°i v·ªõi ch√≠nh n√≥ h√†ng ng√†n l·∫ßn, gradually improve performance
th√¥ng qua trial v√† error.

7.2 TRAINING PIPELINE:

```csharp
IEnumerator RunSelfPlayTraining()
{
    for (int episode = 1; episode <= maxEpisodes; episode++)
    {
        // Reset game state
        ResetGameForTraining();
        
        while (!IsGameOver())
        {
            // Get current state
            GameState currentState = GetCurrentGameState();
            List<AIMove> possibleMoves = GetPossibleMoves();
            
            // Choose action using Œµ-greedy
            AIMove chosenMove = qLearningAgent.ChooseAction(currentState, possibleMoves);
            
            // Execute move
            ExecuteMove(chosenMove);
            
            // Calculate reward
            float reward = CalculateReward(currentState, chosenMove);
            
            // Update Q-table
            GameState newState = GetCurrentGameState();
            qLearningAgent.UpdateQValue(
                EncodeState(currentState),
                EncodeAction(chosenMove),
                reward,
                EncodeState(newState)
            );
            
            yield return new WaitForEndOfFrame();
        }
        
        // Epsilon decay
        qLearningAgent.DecayEpsilon();
        
        // Save progress periodically
        if (episode % 100 == 0)
        {
            qLearningAgent.SaveQTable();
        }
    }
}
```

7.3 REWARD ENGINEERING:
Critical cho Q-Learning success:

```csharp
float CalculateReward(GameState state, AIMove move)
{
    float reward = 0f;
    
    // Game outcome rewards
    if (IsWin(state)) reward += 1000f;
    if (IsLoss(state)) reward -= 1000f;
    
    // Progress rewards
    if (move.moveType == MoveType.Movement)
    {
        if (IsMoveTowardGoal(move)) reward += 20f;
        else reward -= 5f;
    }
    
    // Strategic rewards
    if (move.moveType == MoveType.WallPlacement)
    {
        if (IsWallBlockingOpponent(move)) reward += 50f;
        if (IsWallStrategic(move)) reward += 30f;
        if (IsWallWasted(move)) reward -= 25f;
    }
    
    // Style-specific rewards
    reward += GetStyleSpecificReward(state, move);
    
    return reward;
}
```

7.4 TRAINING METRICS:
- Episodes: 1000 standard
- Duration: ~15 minutes
- Convergence: Usually around episode 800
- Q-table size: 13,118+ states at completion
- Final epsilon: 0.1 (90% exploitation)

================================================================================
                        8. AI TESTING FRAMEWORK
================================================================================

8.1 TESTING PHILOSOPHY:
Comprehensive testing ensures AI quality v√† reliability across different
game scenarios v√† edge cases.

8.2 TEST CATEGORIES:

A) DECISION TREE TESTS:
```csharp
void TestDecisionTreeStrategies()
{
    // Test Early Game
    var earlyGameFeatures = new DecisionFeatures
    {
        aiDistanceToGoal = 8, humanDistanceToGoal = 8,
        isEarlyGame = true
    };
    var strategy = decisionTree.Predict(earlyGameFeatures);
    Assert.AreEqual(AIStrategy.Balanced, strategy);
    
    // Test AI Leading
    var leadingFeatures = new DecisionFeatures
    {
        distanceAdvantage = 3, // AI leading
        isMidGame = true
    };
    strategy = decisionTree.Predict(leadingFeatures);
    Assert.AreEqual(AIStrategy.Blocking, strategy);
}
```

B) ENTROPY ANALYSIS TESTS:
```csharp
void TestEntropyAnalysis()
{
    // High entropy case (diverse scores)
    var highEntropyScores = new List<float> { 100f, 98f, 97f, 95f, 94f };
    float entropy = CalculateTestEntropy(highEntropyScores);
    Assert.IsTrue(entropy > 0.5f);
    
    // Low entropy case (clear winner)
    var lowEntropyScores = new List<float> { 100f, 50f, 50f, 50f, 50f };
    entropy = CalculateTestEntropy(lowEntropyScores);
    Assert.IsTrue(entropy < 0.5f);
}
```

C) SOFTMAX DISTRIBUTION TESTS:
```csharp
void TestSoftmaxDistribution()
{
    var testScores = new List<float> { 100f, 98f, 95f, 90f };
    float temperature = 1.0f;
    
    var probabilities = CalculateSoftmax(testScores, temperature);
    
    // Verify probabilities sum to 1
    float sum = probabilities.Sum();
    Assert.AreEqual(1.0f, sum, 0.001f);
    
    // Verify highest score gets highest probability
    int maxIndex = Array.IndexOf(probabilities, probabilities.Max());
    Assert.AreEqual(0, maxIndex);
}
```

D) PERFORMANCE BENCHMARKS:
- Move generation time: <100ms
- Q-Learning decision: <50ms
- Minimax decision (depth 4): <200ms
- Memory usage: <256MB

8.3 AUTOMATED TESTING:
Tests run automatically ƒë·ªÉ ensure code quality:
- Unit tests cho individual algorithms
- Integration tests cho AI system interactions
- Performance tests cho response times
- Regression tests ƒë·ªÉ prevent quality degradation

================================================================================
                        9. HYBRID AI ARCHITECTURE
================================================================================

9.1 CONCEPT:
Hybrid system k·∫øt h·ª£p strengths c·ªßa multiple algorithms:
- Q-Learning: Data-driven, learns from experience
- Minimax: Logical, guaranteed optimal given evaluation
- Decision Tree: Fast, context-aware switching
- Entropy: Adaptive, handles uncertainty

9.2 ALGORITHM SELECTION LOGIC:

```csharp
public AIMove GetBestMove()
{
    UpdateGameState();
    List<AIMove> possibleMoves = GeneratePossibleMoves();
    
    // Primary: Use Q-Learning if trained model available
    if (useQLearning && qAgent.IsTrainedModel())
    {
        Debug.Log($"üß† Using {specializedStyle} Q-Learning");
        return qAgent.ChooseAction(currentGameState, possibleMoves);
    }
    
    // Fallback: Use Minimax for guaranteed reasonable play
    else
    {
        Debug.Log($"üîç Using {specializedStyle} Minimax");
        return FindBestMoveWithMinimax();
    }
}
```

9.3 SEAMLESS SWITCHING:
- No interruption in gameplay
- Maintains consistent API
- Preserves game state integrity
- Logs algorithm used for debugging

9.4 BENEFITS:
- Robustness: Fallback ensures AI never fails
- Performance: Q-Learning for trained scenarios, Minimax for edge cases
- Flexibility: Can switch algorithms based on situation
- Development: Easy to test individual components

================================================================================
                        10. PERFORMANCE OPTIMIZATION
================================================================================

10.1 Q-TABLE OPTIMIZATION:
- Sparse representation: Only store visited states
- Compression: String interning for state keys
- Lazy loading: Load Q-table on demand
- Periodic saving: Every 100 training episodes

10.2 MINIMAX OPTIMIZATION:
- Alpha-Beta pruning: 50% search reduction
- Move ordering: Promising moves first
- Iterative deepening: Gradual depth increase
- Transposition tables: Cache evaluated positions

10.3 MEMORY MANAGEMENT:
- Object pooling cho GameState objects
- String caching cho repeated state/action encodings
- Garbage collection optimization
- Memory profiling v√† monitoring

10.4 MULTITHREADING CONSIDERATIONS:
- Q-Learning training: Background thread
- Move calculation: Main thread (Unity requirement)
- File I/O: Async operations where possible
- Thread-safe collections cho shared data

10.5 CACHING STRATEGIES:
- Path finding results
- Move generation results
- Evaluation function results
- Feature extraction results

================================================================================
                            K·∫æT LU·∫¨N
================================================================================

D·ª± √°n AI Quoridor Project th·ªÉ hi·ªán m·ªôt implementation comprehensive c·ªßa modern
game AI techniques:

TECHNICAL INNOVATIONS:
‚úÖ Hybrid Q-Learning + Minimax architecture
‚úÖ 6 specialized AI personalities v·ªõi separate training
‚úÖ Advanced decision making v·ªõi entropy analysis
‚úÖ Comprehensive testing v√† validation framework
‚úÖ Professional development tools v√† debugging

EDUCATIONAL VALUE:
‚úÖ Practical machine learning implementation
‚úÖ Traditional game AI algorithms
‚úÖ Modern software engineering practices
‚úÖ Performance optimization techniques
‚úÖ Automated testing methodologies

FUTURE ENHANCEMENTS:
üöÄ Deep Q-Learning (DQN) integration
üöÄ Multi-agent reinforcement learning
üöÄ Neural network-based evaluation functions
üöÄ Online learning v√† adaptation
üöÄ Transfer learning between game variants

D·ª± √°n n√†y demonstrating successful integration c·ªßa classical AI methods v·ªõi
modern machine learning, t·∫°o ra m·ªôt intelligent, adaptive, v√† highly
interactive gaming experience.

================================================================================
                            APPENDIX
================================================================================

FILES LI√äN QUAN:
- Assets/scripts/QLearningAgent.cs - Q-Learning implementation
- Assets/scripts/QuoridorAI.cs - Main AI controller
- Assets/scripts/DecisionTreeClassifier.cs - Decision tree logic
- Assets/scripts/QuoridorSelfPlayTrainer.cs - Training system
- Assets/scripts/AITestScript.cs - Testing framework
- Assets/scripts/Editor/ForceRetrainAI.cs - Development tools

Q-TABLE FILES:
- StreamingAssets/qtable.json - Balanced style
- StreamingAssets/qtable_aggressive.json - Aggressive style
- StreamingAssets/qtable_defensive.json - Defensive style
- StreamingAssets/qtable_wallmaster.json - Wall Master style
- StreamingAssets/qtable_speedrunner.json - Speed Runner style
- StreamingAssets/qtable_tactical.json - Tactical style

DOCUMENTATION:
- AI_ALGORITHMS_DETAILED_DESCRIPTION.txt - This file
- README.md - Project overview
- API_INTEGRATION_GUIDE.md - Voice/Chat integration
- TRAINING_GUIDE.md - AI training instructions

================================================================================
================================================================================
                    AI QUORIDOR GAME PROJECT - THU·∫¨T TO√ÅN AI CHI TI·∫æT
================================================================================

D·ª± √°n: AI Quoridor Game v·ªõi Machine Learning v√† Multi-Platform Integration
Ng√†y t·∫°o: 25/07/2025
T√°c gi·∫£: [Student Name]
Platform: Unity 3D
Language: C#

================================================================================
                            M·ª§C L·ª§C
================================================================================

1. T·ªîNG QUAN KI·∫æN TR√öC AI
2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
4. SPECIALIZED AI STYLES SYSTEM
5. DECISION TREE CLASSIFIER
6. ENTROPY ANALYSIS & SOFTMAX
7. TRAINING SYSTEM & SELF-PLAY
8. AI TESTING FRAMEWORK
9. HYBRID AI ARCHITECTURE
10. PERFORMANCE OPTIMIZATION

================================================================================
                        1. T·ªîNG QUAN KI·∫æN TR√öC AI
================================================================================

D·ª± √°n AI Quoridor s·ª≠ d·ª•ng m·ªôt ki·∫øn tr√∫c AI hybrid ti√™n ti·∫øn, k·∫øt h·ª£p:

1.1 CORE ALGORITHMS:
- Q-Learning (Reinforcement Learning) - Thu·∫≠t to√°n ch√≠nh
- Minimax + Alpha-Beta Pruning - Thu·∫≠t to√°n fallback
- Decision Tree Classifier - Ph√¢n t√≠ch ng·ªØ c·∫£nh game
- Entropy Analysis - ƒê√°nh gi√° ƒëa d·∫°ng move

1.2 SPECIALIZED SYSTEMS:
- 6 AI Personality Styles v·ªõi training ri√™ng bi·ªát
- Self-Play Training Pipeline t·ª± ƒë·ªông
- Real-time Performance Testing Framework
- Dynamic Algorithm Switching

1.3 TECHNICAL FOUNDATION:
- State Space: 13,118+ unique game states
- Action Space: Movement + Wall placement combinations
- Training Method: Self-play v·ªõi 1000 episodes
- Learning Rate: Œ± = 0.1, Discount Factor: Œ≥ = 0.99

================================================================================
                    2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
================================================================================

2.1 GI·ªöI THI·ªÜU Q-LEARNING:
Q-Learning l√† thu·∫≠t to√°n Reinforcement Learning kh√¥ng c·∫ßn model, h·ªçc th√¥ng qua
t∆∞∆°ng t√°c v·ªõi m√¥i tr∆∞·ªùng ƒë·ªÉ t·ªëi ∆∞u h√≥a policy. Trong Quoridor, AI h·ªçc c√°ch
ch·ªçn moves t·ªëi ∆∞u d·ª±a tr√™n kinh nghi·ªám t·ª´ h√†ng ng√†n game.

2.2 C√îNG TH·ª®C TO√ÅN H·ªåC:
Q(s,a) = Q(s,a) + Œ±[r + Œ≥*max(Q(s',a')) - Q(s,a)]

Trong ƒë√≥:
- Q(s,a): Q-value c·ªßa state s v·ªõi action a
- Œ±: Learning rate (0.1) - t·ªëc ƒë·ªô h·ªçc
- r: Immediate reward t·ª´ action
- Œ≥: Discount factor (0.99) - t·∫ßm quan tr·ªçng future rewards
- s': Next state sau khi th·ª±c hi·ªán action
- max(Q(s',a')): Maximum Q-value c·ªßa next state

2.3 IMPLEMENTATION CHI TI·∫æT:

```csharp
public class QLearningAgent
{
    // Core Q-Table: Dictionary l∆∞u tr·ªØ Q-values
    private Dictionary<string, Dictionary<string, float>> qTable;
    
    // Hyperparameters
    private float learningRate = 0.1f;      // T·ªëc ƒë·ªô h·ªçc
    private float discountFactor = 0.99f;   // Discount future rewards
    private float epsilon = 0.995f;         // Exploration rate
    
    // Q-Learning Update Implementation
    public void UpdateQValue(string state, string action, float reward, string nextState)
    {
        // L·∫•y Q-value hi·ªán t·∫°i
        float currentQ = GetQValue(state, action);
        
        // T√¨m max Q-value c·ªßa next state
        float maxNextQ = GetMaxQValue(nextState);
        
        // √Åp d·ª•ng c√¥ng th·ª©c Q-Learning
        float newQ = currentQ + learningRate * (reward + discountFactor * maxNextQ - currentQ);
        
        // C·∫≠p nh·∫≠t Q-table
        SetQValue(state, action, newQ);
    }
}
```

2.4 STATE ENCODING:
Game state ƒë∆∞·ª£c encode th√†nh string duy nh·∫•t:
"aiPos_x,aiPos_y-humanPos_x,humanPos_y-aiWalls-humanWalls-wallsList"

V√≠ d·ª•: "4,8-4,0-9-10-7,4,False|5,6,True"
- AI ·ªü (4,8), Human ·ªü (4,0)
- AI c√≥ 9 walls, Human c√≥ 10 walls
- Walls ƒë√£ ƒë·∫∑t: (7,4,False), (5,6,True)

2.5 ACTION ENCODING:
Actions ƒë∆∞·ª£c encode th√†nh string:
- Movement: "M:x,y" (v√≠ d·ª•: "M:4,7")
- Wall: "W:x,y,horizontal" (v√≠ d·ª•: "W:3,5,True")

2.6 EPSILON-GREEDY STRATEGY:
```csharp
public AIMove ChooseAction(GameState state, List<AIMove> possibleMoves)
{
    // Exploration vs Exploitation trade-off
    if (Random.Range(0f, 1f) < epsilon)
    {
        // EXPLORATION: Ch·ªçn random ƒë·ªÉ kh√°m ph√°
        return possibleMoves[Random.Range(0, possibleMoves.Count)];
    }
    else
    {
        // EXPLOITATION: Ch·ªçn action c√≥ Q-value cao nh·∫•t
        return GetBestAction(state, possibleMoves);
    }
}
```

2.7 EPSILON DECAY:
Epsilon gi·∫£m d·∫ßn t·ª´ 0.995 ‚Üí 0.1 qua training:
- Ban ƒë·∫ßu: High exploration (99.5%)
- Cu·ªëi training: High exploitation (90%)

2.8 REWARD SYSTEM:
- Win game: +1000
- Lose game: -1000
- Move toward goal: +10
- Block opponent: +50
- Waste move: -5
- Strategic position: +15

================================================================================
                3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
================================================================================

3.1 GI·ªöI THI·ªÜU MINIMAX:
Minimax l√† thu·∫≠t to√°n game theory c·ªï ƒëi·ªÉn, t√¨m ki·∫øm move t·ªëi ∆∞u b·∫±ng c√°ch
d·ª± ƒëo√°n tr∆∞·ªõc nhi·ªÅu moves. Alpha-Beta pruning t·ªëi ∆∞u hi·ªáu su·∫•t b·∫±ng c√°ch
lo·∫°i b·ªè c√°c nh√°nh kh√¥ng c·∫ßn thi·∫øt.

3.2 NGUY√äN L√ù HO·∫†T ƒê·ªòNG:
- Maximizing Player (AI): T·ªëi ƒëa h√≥a score
- Minimizing Player (Human): T·ªëi thi·ªÉu h√≥a score AI
- Search depth: 4 levels (configurable)
- Pruning: Lo·∫°i b·ªè nh√°nh kh√¥ng promising

3.3 IMPLEMENTATION:

```csharp
float Minimax(GameState state, int depth, bool isMaximizing, float alpha, float beta)
{
    // Base case: Depth = 0 ho·∫∑c game over
    if (depth == 0 || IsGameOver(state))
        return EvaluatePosition(state);
    
    if (isMaximizing) // AI turn
    {
        float maxEval = float.NegativeInfinity;
        foreach (var move in GeneratePossibleMoves(state, true))
        {
            GameState newState = ApplyMove(state, move, true);
            float eval = Minimax(newState, depth - 1, false, alpha, beta);
            maxEval = Mathf.Max(maxEval, eval);
            alpha = Mathf.Max(alpha, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return maxEval;
    }
    else // Human turn
    {
        float minEval = float.PositiveInfinity;
        foreach (var move in GeneratePossibleMoves(state, false))
        {
            GameState newState = ApplyMove(state, move, false);
            float eval = Minimax(newState, depth - 1, true, alpha, beta);
            minEval = Mathf.Min(minEval, eval);
            beta = Mathf.Min(beta, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return minEval;
    }
}
```

3.4 EVALUATION FUNCTION:
Position evaluation d·ª±a tr√™n multiple factors:

```csharp
float EvaluatePosition(GameState state)
{
    float score = 0f;
    
    // Distance to goal (c√†ng g·∫ßn c√†ng t·ªët)
    int aiDistance = GetShortestPath(state.aiPosition, goalRow);
    int humanDistance = GetShortestPath(state.humanPosition, goalRow);
    score += (humanDistance - aiDistance) * 100;
    
    // Wall advantage
    score += (state.aiWallsLeft - state.humanWallsLeft) * 50;
    
    // Mobility (s·ªë moves c√≥ th·ªÉ)
    int aiMobility = GetValidMoves(state.aiPosition).Count;
    int humanMobility = GetValidMoves(state.humanPosition).Count;
    score += (aiMobility - humanMobility) * 20;
    
    // Center control
    score += GetCenterControlScore(state) * 30;
    
    return score;
}
```

3.5 ALPHA-BETA PRUNING HI·ªÜU QU·∫¢:
- Reduces search space t·ª´ O(b^d) xu·ªëng O(b^(d/2))
- Best case: 50% reduction in nodes evaluated
- Ordering moves by promising gi√∫p pruning hi·ªáu qu·∫£ h∆°n

================================================================================
                        4. SPECIALIZED AI STYLES SYSTEM
================================================================================

4.1 CONCEPT:
6 AI personalities v·ªõi training v√† behavior ri√™ng bi·ªát, m·ªói style c√≥:
- Ri√™ng Q-table file
- Custom reward structure
- Specific move prioritization
- Unique decision making logic

4.2 AI STYLES CHI TI·∫æT:

üî• AGGRESSIVE STYLE:
- M·ª•c ti√™u: T·∫•n c√¥ng v√† ch·∫∑n ƒë·ªëi th·ªß
- Wall Usage: 70% chance
- Reward Structure:
  * blockingOpponentReward = 50 * intensity
  * wallPlacementReward = 30 * intensity
  * defensiveWallPenalty = -20 * intensity
- Q-table: qtable_aggressive.json

üõ°Ô∏è DEFENSIVE STYLE:
- M·ª•c ti√™u: Ph√≤ng th·ªß v√† di chuy·ªÉn an to√†n
- Wall Usage: 50% chance
- Reward Structure:
  * defensiveWallReward = 35 * intensity
  * movementReward = 20 * intensity
  * aggressiveMovePenalty = -15 * intensity
- Q-table: qtable_defensive.json

üß± WALL MASTER STYLE:
- M·ª•c ti√™u: Chuy√™n gia ƒë·∫∑t t∆∞·ªùng v√† t·∫°o m√™ cung
- Wall Usage: 90% chance
- Reward Structure:
  * wallPlacementReward = 60 * intensity
  * strategicWallReward = 40 * intensity
  * wallComboReward = 50 * intensity
  * movementWithoutWallPenalty = -25 * intensity
- Q-table: qtable_wallmaster.json

‚ö° SPEED RUNNER STYLE:
- M·ª•c ti√™u: ƒê·∫°t ƒë√≠ch nhanh nh·∫•t
- Wall Usage: 10% chance
- Reward Structure:
  * movementReward = 40 * intensity
  * progressToGoalReward = 50 * intensity
  * quickWinBonus = 100 * intensity
  * wallPlacementPenalty = -20 * intensity
- Q-table: qtable_speedrunner.json

üß† TACTICAL STYLE:
- M·ª•c ti√™u: L·ª£i th·∫ø v·ªã tr√≠ v√† k·∫ø ho·∫°ch d√†i h·∫°n
- Wall Usage: Variable (strategic)
- Reward Structure:
  * tacticalMoveReward = 45 * intensity
  * positionAdvantageReward = 35 * intensity
  * mobilityReward = 25 * intensity
  * patternRecognitionBonus = 40 * intensity
- Q-table: qtable_tactical.json

‚öñÔ∏è BALANCED STYLE:
- M·ª•c ti√™u: C√¢n b·∫±ng t·∫•t c·∫£ aspects
- Wall Usage: 30% chance
- Reward Structure: Default balanced values
- Q-table: qtable.json

4.3 STYLE SWITCHING MECHANISM:

```csharp
public void SwitchToStyle(PlayStyle newStyle)
{
    // Save current Q-table
    if (allowQTableSaving) qAgent.SaveQTable(GetStyleSpecificQTablePath());
    
    // Load new style Q-table
    specializedStyle = newStyle;
    qAgent.LoadQTable(GetStyleSpecificQTablePath());
    
    // Reconfigure parameters
    ConfigureStyleSpecificTraining();
}
```

4.4 STYLE INTENSITY:
Parameter t·ª´ 0.0 ‚Üí 1.0 control m·ª©c ƒë·ªô chuy√™n h√≥a:
- 0.0: Balanced behavior
- 0.5: Moderate specialization
- 1.0: Extreme specialization

================================================================================
                        5. DECISION TREE CLASSIFIER
================================================================================

5.1 PURPOSE:
Decision Tree ph√¢n t√≠ch game context ƒë·ªÉ ch·ªçn strategy ph√π h·ª£p, d·ª±a tr√™n:
- Game phase (Early/Mid/End)
- Position advantage
- Resource advantage
- Mobility situation

5.2 FEATURE EXTRACTION:

```csharp
public struct DecisionFeatures
{
    public int aiDistanceToGoal;      // AI distance to goal line
    public int humanDistanceToGoal;   // Human distance to goal line
    public int aiWallsLeft;           // AI walls remaining
    public int humanWallsLeft;        // Human walls remaining
    public int aiMobility;            // AI possible moves
    public int humanMobility;         // Human possible moves
    public float distanceAdvantage;   // Human - AI distance
    public float wallAdvantage;       // AI - Human walls
    public bool isEarlyGame;          // > 15 moves estimated remaining
    public bool isMidGame;            // 5-15 moves remaining
    public bool isEndGame;            // < 5 moves remaining
}
```

5.3 DECISION LOGIC:

```csharp
public AIStrategy Predict(DecisionFeatures features)
{
    // Early Game: Play conservatively
    if (features.isEarlyGame)
        return AIStrategy.Balanced;
    
    // AI has significant lead: Focus on blocking
    if (features.distanceAdvantage > 2)
        return AIStrategy.Blocking;
    
    // AI is behind: Be aggressive
    if (features.distanceAdvantage < -1)
        return AIStrategy.Aggressive;
    
    // End game with wall advantage: Play defensively
    if (features.isEndGame && features.wallAdvantage > 2)
        return AIStrategy.Defensive;
    
    // Close game: Balanced approach
    return AIStrategy.Balanced;
}
```

5.4 STRATEGY OUTCOMES:
- Balanced: Equal weight to movement v√† walls
- Blocking: Prioritize moves that increase opponent path
- Aggressive: Focus on attacking positions
- Defensive: Protect own position and path

================================================================================
                        6. ENTROPY ANALYSIS & SOFTMAX
================================================================================

6.1 ENTROPY CONCEPT:
Entropy ƒëo ƒë·ªô ƒëa d·∫°ng/uncertainty c·ªßa move scores. High entropy = nhi·ªÅu moves
c√≥ scores t∆∞∆°ng ƒë∆∞∆°ng, low entropy = m·ªôt move r√µ r√†ng t·ªët nh·∫•t.

6.2 ENTROPY CALCULATION:

```csharp
float CalculateEntropy(List<float> moveScores)
{
    // Normalize th√†nh probabilities
    float total = moveScores.Sum();
    var probabilities = moveScores.Select(s => s / total);
    
    // Calculate Shannon entropy
    float entropy = 0f;
    foreach (float p in probabilities)
    {
        if (p > 0f)
            entropy -= p * Mathf.Log(p, 2f);
    }
    
    return entropy; // Range: 0 (deterministic) to log2(n) (uniform)
}
```

6.3 SOFTMAX DISTRIBUTION:

```csharp
float[] CalculateSoftmax(List<float> scores, float temperature)
{
    float[] probabilities = new float[scores.Count];
    float sum = scores.Sum(s => Mathf.Exp(s / temperature));
    
    for (int i = 0; i < scores.Count; i++)
    {
        probabilities[i] = Mathf.Exp(scores[i] / temperature) / sum;
    }
    
    return probabilities;
}
```

6.4 TEMPERATURE PARAMETER:
- Low temperature (0.1): Sharp distribution, focus on best moves
- High temperature (2.0): Flat distribution, more exploration
- Default temperature (1.0): Balanced exploitation/exploration

6.5 ADAPTIVE MOVE SELECTION:

```csharp
public AIMove ChooseActionWithEntropy(GameState state, List<AIMove> possibleMoves)
{
    var moveScores = EvaluateAllMoves(state, possibleMoves);
    float entropy = CalculateEntropy(moveScores);
    
    if (entropy > entropyThreshold) // High diversity
    {
        // Use softmax for probabilistic selection
        var probabilities = CalculateSoftmax(moveScores, softmaxTemperature);
        return SelectMoveByProbability(possibleMoves, probabilities);
    }
    else // Clear best move
    {
        // Use deterministic best move
        return GetBestScoredMove(possibleMoves, moveScores);
    }
}
```

================================================================================
                        7. TRAINING SYSTEM & SELF-PLAY
================================================================================

7.1 SELF-PLAY CONCEPT:
AI h·ªçc b·∫±ng c√°ch ch∆°i v·ªõi ch√≠nh n√≥ h√†ng ng√†n l·∫ßn, gradually improve performance
th√¥ng qua trial v√† error.

7.2 TRAINING PIPELINE:

```csharp
IEnumerator RunSelfPlayTraining()
{
    for (int episode = 1; episode <= maxEpisodes; episode++)
    {
        // Reset game state
        ResetGameForTraining();
        
        while (!IsGameOver())
        {
            // Get current state
            GameState currentState = GetCurrentGameState();
            List<AIMove> possibleMoves = GetPossibleMoves();
            
            // Choose action using Œµ-greedy
            AIMove chosenMove = qLearningAgent.ChooseAction(currentState, possibleMoves);
            
            // Execute move
            ExecuteMove(chosenMove);
            
            // Calculate reward
            float reward = CalculateReward(currentState, chosenMove);
            
            // Update Q-table
            GameState newState = GetCurrentGameState();
            qLearningAgent.UpdateQValue(
                EncodeState(currentState),
                EncodeAction(chosenMove),
                reward,
                EncodeState(newState)
            );
            
            yield return new WaitForEndOfFrame();
        }
        
        // Epsilon decay
        qLearningAgent.DecayEpsilon();
        
        // Save progress periodically
        if (episode % 100 == 0)
        {
            qLearningAgent.SaveQTable();
        }
    }
}
```

7.3 REWARD ENGINEERING:
Critical cho Q-Learning success:

```csharp
float CalculateReward(GameState state, AIMove move)
{
    float reward = 0f;
    
    // Game outcome rewards
    if (IsWin(state)) reward += 1000f;
    if (IsLoss(state)) reward -= 1000f;
    
    // Progress rewards
    if (move.moveType == MoveType.Movement)
    {
        if (IsMoveTowardGoal(move)) reward += 20f;
        else reward -= 5f;
    }
    
    // Strategic rewards
    if (move.moveType == MoveType.WallPlacement)
    {
        if (IsWallBlockingOpponent(move)) reward += 50f;
        if (IsWallStrategic(move)) reward += 30f;
        if (IsWallWasted(move)) reward -= 25f;
    }
    
    // Style-specific rewards
    reward += GetStyleSpecificReward(state, move);
    
    return reward;
}
```

7.4 TRAINING METRICS:
- Episodes: 1000 standard
- Duration: ~15 minutes
- Convergence: Usually around episode 800
- Q-table size: 13,118+ states at completion
- Final epsilon: 0.1 (90% exploitation)

================================================================================
                        8. AI TESTING FRAMEWORK
================================================================================

8.1 TESTING PHILOSOPHY:
Comprehensive testing ensures AI quality v√† reliability across different
game scenarios v√† edge cases.

8.2 TEST CATEGORIES:

A) DECISION TREE TESTS:
```csharp
void TestDecisionTreeStrategies()
{
    // Test Early Game
    var earlyGameFeatures = new DecisionFeatures
    {
        aiDistanceToGoal = 8, humanDistanceToGoal = 8,
        isEarlyGame = true
    };
    var strategy = decisionTree.Predict(earlyGameFeatures);
    Assert.AreEqual(AIStrategy.Balanced, strategy);
    
    // Test AI Leading
    var leadingFeatures = new DecisionFeatures
    {
        distanceAdvantage = 3, // AI leading
        isMidGame = true
    };
    strategy = decisionTree.Predict(leadingFeatures);
    Assert.AreEqual(AIStrategy.Blocking, strategy);
}
```

B) ENTROPY ANALYSIS TESTS:
```csharp
void TestEntropyAnalysis()
{
    // High entropy case (diverse scores)
    var highEntropyScores = new List<float> { 100f, 98f, 97f, 95f, 94f };
    float entropy = CalculateTestEntropy(highEntropyScores);
    Assert.IsTrue(entropy > 0.5f);
    
    // Low entropy case (clear winner)
    var lowEntropyScores = new List<float> { 100f, 50f, 50f, 50f, 50f };
    entropy = CalculateTestEntropy(lowEntropyScores);
    Assert.IsTrue(entropy < 0.5f);
}
```

C) SOFTMAX DISTRIBUTION TESTS:
```csharp
void TestSoftmaxDistribution()
{
    var testScores = new List<float> { 100f, 98f, 95f, 90f };
    float temperature = 1.0f;
    
    var probabilities = CalculateSoftmax(testScores, temperature);
    
    // Verify probabilities sum to 1
    float sum = probabilities.Sum();
    Assert.AreEqual(1.0f, sum, 0.001f);
    
    // Verify highest score gets highest probability
    int maxIndex = Array.IndexOf(probabilities, probabilities.Max());
    Assert.AreEqual(0, maxIndex);
}
```

D) PERFORMANCE BENCHMARKS:
- Move generation time: <100ms
- Q-Learning decision: <50ms
- Minimax decision (depth 4): <200ms
- Memory usage: <256MB

8.3 AUTOMATED TESTING:
Tests run automatically ƒë·ªÉ ensure code quality:
- Unit tests cho individual algorithms
- Integration tests cho AI system interactions
- Performance tests cho response times
- Regression tests ƒë·ªÉ prevent quality degradation

================================================================================
                        9. HYBRID AI ARCHITECTURE
================================================================================

9.1 CONCEPT:
Hybrid system k·∫øt h·ª£p strengths c·ªßa multiple algorithms:
- Q-Learning: Data-driven, learns from experience
- Minimax: Logical, guaranteed optimal given evaluation
- Decision Tree: Fast, context-aware switching
- Entropy: Adaptive, handles uncertainty

9.2 ALGORITHM SELECTION LOGIC:

```csharp
public AIMove GetBestMove()
{
    UpdateGameState();
    List<AIMove> possibleMoves = GeneratePossibleMoves();
    
    // Primary: Use Q-Learning if trained model available
    if (useQLearning && qAgent.IsTrainedModel())
    {
        Debug.Log($"üß† Using {specializedStyle} Q-Learning");
        return qAgent.ChooseAction(currentGameState, possibleMoves);
    }
    
    // Fallback: Use Minimax for guaranteed reasonable play
    else
    {
        Debug.Log($"üîç Using {specializedStyle} Minimax");
        return FindBestMoveWithMinimax();
    }
}
```

9.3 SEAMLESS SWITCHING:
- No interruption in gameplay
- Maintains consistent API
- Preserves game state integrity
- Logs algorithm used for debugging

9.4 BENEFITS:
- Robustness: Fallback ensures AI never fails
- Performance: Q-Learning for trained scenarios, Minimax for edge cases
- Flexibility: Can switch algorithms based on situation
- Development: Easy to test individual components

================================================================================
                        10. PERFORMANCE OPTIMIZATION
================================================================================

10.1 Q-TABLE OPTIMIZATION:
- Sparse representation: Only store visited states
- Compression: String interning for state keys
- Lazy loading: Load Q-table on demand
- Periodic saving: Every 100 training episodes

10.2 MINIMAX OPTIMIZATION:
- Alpha-Beta pruning: 50% search reduction
- Move ordering: Promising moves first
- Iterative deepening: Gradual depth increase
- Transposition tables: Cache evaluated positions

10.3 MEMORY MANAGEMENT:
- Object pooling cho GameState objects
- String caching cho repeated state/action encodings
- Garbage collection optimization
- Memory profiling v√† monitoring

10.4 MULTITHREADING CONSIDERATIONS:
- Q-Learning training: Background thread
- Move calculation: Main thread (Unity requirement)
- File I/O: Async operations where possible
- Thread-safe collections cho shared data

10.5 CACHING STRATEGIES:
- Path finding results
- Move generation results
- Evaluation function results
- Feature extraction results

================================================================================
                            K·∫æT LU·∫¨N
================================================================================

D·ª± √°n AI Quoridor Project th·ªÉ hi·ªán m·ªôt implementation comprehensive c·ªßa modern
game AI techniques:

TECHNICAL INNOVATIONS:
‚úÖ Hybrid Q-Learning + Minimax architecture
‚úÖ 6 specialized AI personalities v·ªõi separate training
‚úÖ Advanced decision making v·ªõi entropy analysis
‚úÖ Comprehensive testing v√† validation framework
‚úÖ Professional development tools v√† debugging

EDUCATIONAL VALUE:
‚úÖ Practical machine learning implementation
‚úÖ Traditional game AI algorithms
‚úÖ Modern software engineering practices
‚úÖ Performance optimization techniques
‚úÖ Automated testing methodologies

FUTURE ENHANCEMENTS:
üöÄ Deep Q-Learning (DQN) integration
üöÄ Multi-agent reinforcement learning
üöÄ Neural network-based evaluation functions
üöÄ Online learning v√† adaptation
üöÄ Transfer learning between game variants

D·ª± √°n n√†y demonstrating successful integration c·ªßa classical AI methods v·ªõi
modern machine learning, t·∫°o ra m·ªôt intelligent, adaptive, v√† highly
interactive gaming experience.

================================================================================
                            APPENDIX
================================================================================

FILES LI√äN QUAN:
- Assets/scripts/QLearningAgent.cs - Q-Learning implementation
- Assets/scripts/QuoridorAI.cs - Main AI controller
- Assets/scripts/DecisionTreeClassifier.cs - Decision tree logic
- Assets/scripts/QuoridorSelfPlayTrainer.cs - Training system
- Assets/scripts/AITestScript.cs - Testing framework
- Assets/scripts/Editor/ForceRetrainAI.cs - Development tools

Q-TABLE FILES:
- StreamingAssets/qtable.json - Balanced style
- StreamingAssets/qtable_aggressive.json - Aggressive style
- StreamingAssets/qtable_defensive.json - Defensive style
- StreamingAssets/qtable_wallmaster.json - Wall Master style
- StreamingAssets/qtable_speedrunner.json - Speed Runner style
- StreamingAssets/qtable_tactical.json - Tactical style

DOCUMENTATION:
- AI_ALGORITHMS_DETAILED_DESCRIPTION.txt - This file
- README.md - Project overview
- API_INTEGRATION_GUIDE.md - Voice/Chat integration
- TRAINING_GUIDE.md - AI training instructions

================================================================================
================================================================================
                    AI QUORIDOR GAME PROJECT - THU·∫¨T TO√ÅN AI CHI TI·∫æT
================================================================================

D·ª± √°n: AI Quoridor Game v·ªõi Machine Learning v√† Multi-Platform Integration
Ng√†y t·∫°o: 25/07/2025
T√°c gi·∫£: [Student Name]
Platform: Unity 3D
Language: C#

================================================================================
                            M·ª§C L·ª§C
================================================================================

1. T·ªîNG QUAN KI·∫æN TR√öC AI
2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
4. SPECIALIZED AI STYLES SYSTEM
5. DECISION TREE CLASSIFIER
6. ENTROPY ANALYSIS & SOFTMAX
7. TRAINING SYSTEM & SELF-PLAY
8. AI TESTING FRAMEWORK
9. HYBRID AI ARCHITECTURE
10. PERFORMANCE OPTIMIZATION

================================================================================
                        1. T·ªîNG QUAN KI·∫æN TR√öC AI
================================================================================

D·ª± √°n AI Quoridor s·ª≠ d·ª•ng m·ªôt ki·∫øn tr√∫c AI hybrid ti√™n ti·∫øn, k·∫øt h·ª£p:

1.1 CORE ALGORITHMS:
- Q-Learning (Reinforcement Learning) - Thu·∫≠t to√°n ch√≠nh
- Minimax + Alpha-Beta Pruning - Thu·∫≠t to√°n fallback
- Decision Tree Classifier - Ph√¢n t√≠ch ng·ªØ c·∫£nh game
- Entropy Analysis - ƒê√°nh gi√° ƒëa d·∫°ng move

1.2 SPECIALIZED SYSTEMS:
- 6 AI Personality Styles v·ªõi training ri√™ng bi·ªát
- Self-Play Training Pipeline t·ª± ƒë·ªông
- Real-time Performance Testing Framework
- Dynamic Algorithm Switching

1.3 TECHNICAL FOUNDATION:
- State Space: 13,118+ unique game states
- Action Space: Movement + Wall placement combinations
- Training Method: Self-play v·ªõi 1000 episodes
- Learning Rate: Œ± = 0.1, Discount Factor: Œ≥ = 0.99

================================================================================
                    2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
================================================================================

2.1 GI·ªöI THI·ªÜU Q-LEARNING:
Q-Learning l√† thu·∫≠t to√°n Reinforcement Learning kh√¥ng c·∫ßn model, h·ªçc th√¥ng qua
t∆∞∆°ng t√°c v·ªõi m√¥i tr∆∞·ªùng ƒë·ªÉ t·ªëi ∆∞u h√≥a policy. Trong Quoridor, AI h·ªçc c√°ch
ch·ªçn moves t·ªëi ∆∞u d·ª±a tr√™n kinh nghi·ªám t·ª´ h√†ng ng√†n game.

2.2 C√îNG TH·ª®C TO√ÅN H·ªåC:
Q(s,a) = Q(s,a) + Œ±[r + Œ≥*max(Q(s',a')) - Q(s,a)]

Trong ƒë√≥:
- Q(s,a): Q-value c·ªßa state s v·ªõi action a
- Œ±: Learning rate (0.1) - t·ªëc ƒë·ªô h·ªçc
- r: Immediate reward t·ª´ action
- Œ≥: Discount factor (0.99) - t·∫ßm quan tr·ªçng future rewards
- s': Next state sau khi th·ª±c hi·ªán action
- max(Q(s',a')): Maximum Q-value c·ªßa next state

2.3 IMPLEMENTATION CHI TI·∫æT:

```csharp
public class QLearningAgent
{
    // Core Q-Table: Dictionary l∆∞u tr·ªØ Q-values
    private Dictionary<string, Dictionary<string, float>> qTable;
    
    // Hyperparameters
    private float learningRate = 0.1f;      // T·ªëc ƒë·ªô h·ªçc
    private float discountFactor = 0.99f;   // Discount future rewards
    private float epsilon = 0.995f;         // Exploration rate
    
    // Q-Learning Update Implementation
    public void UpdateQValue(string state, string action, float reward, string nextState)
    {
        // L·∫•y Q-value hi·ªán t·∫°i
        float currentQ = GetQValue(state, action);
        
        // T√¨m max Q-value c·ªßa next state
        float maxNextQ = GetMaxQValue(nextState);
        
        // √Åp d·ª•ng c√¥ng th·ª©c Q-Learning
        float newQ = currentQ + learningRate * (reward + discountFactor * maxNextQ - currentQ);
        
        // C·∫≠p nh·∫≠t Q-table
        SetQValue(state, action, newQ);
    }
}
```

2.4 STATE ENCODING:
Game state ƒë∆∞·ª£c encode th√†nh string duy nh·∫•t:
"aiPos_x,aiPos_y-humanPos_x,humanPos_y-aiWalls-humanWalls-wallsList"

V√≠ d·ª•: "4,8-4,0-9-10-7,4,False|5,6,True"
- AI ·ªü (4,8), Human ·ªü (4,0)
- AI c√≥ 9 walls, Human c√≥ 10 walls
- Walls ƒë√£ ƒë·∫∑t: (7,4,False), (5,6,True)

2.5 ACTION ENCODING:
Actions ƒë∆∞·ª£c encode th√†nh string:
- Movement: "M:x,y" (v√≠ d·ª•: "M:4,7")
- Wall: "W:x,y,horizontal" (v√≠ d·ª•: "W:3,5,True")

2.6 EPSILON-GREEDY STRATEGY:
```csharp
public AIMove ChooseAction(GameState state, List<AIMove> possibleMoves)
{
    // Exploration vs Exploitation trade-off
    if (Random.Range(0f, 1f) < epsilon)
    {
        // EXPLORATION: Ch·ªçn random ƒë·ªÉ kh√°m ph√°
        return possibleMoves[Random.Range(0, possibleMoves.Count)];
    }
    else
    {
        // EXPLOITATION: Ch·ªçn action c√≥ Q-value cao nh·∫•t
        return GetBestAction(state, possibleMoves);
    }
}
```

2.7 EPSILON DECAY:
Epsilon gi·∫£m d·∫ßn t·ª´ 0.995 ‚Üí 0.1 qua training:
- Ban ƒë·∫ßu: High exploration (99.5%)
- Cu·ªëi training: High exploitation (90%)

2.8 REWARD SYSTEM:
- Win game: +1000
- Lose game: -1000
- Move toward goal: +10
- Block opponent: +50
- Waste move: -5
- Strategic position: +15

================================================================================
                3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
================================================================================

3.1 GI·ªöI THI·ªÜU MINIMAX:
Minimax l√† thu·∫≠t to√°n game theory c·ªï ƒëi·ªÉn, t√¨m ki·∫øm move t·ªëi ∆∞u b·∫±ng c√°ch
d·ª± ƒëo√°n tr∆∞·ªõc nhi·ªÅu moves. Alpha-Beta pruning t·ªëi ∆∞u hi·ªáu su·∫•t b·∫±ng c√°ch
lo·∫°i b·ªè c√°c nh√°nh kh√¥ng c·∫ßn thi·∫øt.

3.2 NGUY√äN L√ù HO·∫†T ƒê·ªòNG:
- Maximizing Player (AI): T·ªëi ƒëa h√≥a score
- Minimizing Player (Human): T·ªëi thi·ªÉu h√≥a score AI
- Search depth: 4 levels (configurable)
- Pruning: Lo·∫°i b·ªè nh√°nh kh√¥ng promising

3.3 IMPLEMENTATION:

```csharp
float Minimax(GameState state, int depth, bool isMaximizing, float alpha, float beta)
{
    // Base case: Depth = 0 ho·∫∑c game over
    if (depth == 0 || IsGameOver(state))
        return EvaluatePosition(state);
    
    if (isMaximizing) // AI turn
    {
        float maxEval = float.NegativeInfinity;
        foreach (var move in GeneratePossibleMoves(state, true))
        {
            GameState newState = ApplyMove(state, move, true);
            float eval = Minimax(newState, depth - 1, false, alpha, beta);
            maxEval = Mathf.Max(maxEval, eval);
            alpha = Mathf.Max(alpha, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return maxEval;
    }
    else // Human turn
    {
        float minEval = float.PositiveInfinity;
        foreach (var move in GeneratePossibleMoves(state, false))
        {
            GameState newState = ApplyMove(state, move, false);
            float eval = Minimax(newState, depth - 1, true, alpha, beta);
            minEval = Mathf.Min(minEval, eval);
            beta = Mathf.Min(beta, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return minEval;
    }
}
```

3.4 EVALUATION FUNCTION:
Position evaluation d·ª±a tr√™n multiple factors:

```csharp
float EvaluatePosition(GameState state)
{
    float score = 0f;
    
    // Distance to goal (c√†ng g·∫ßn c√†ng t·ªët)
    int aiDistance = GetShortestPath(state.aiPosition, goalRow);
    int humanDistance = GetShortestPath(state.humanPosition, goalRow);
    score += (humanDistance - aiDistance) * 100;
    
    // Wall advantage
    score += (state.aiWallsLeft - state.humanWallsLeft) * 50;
    
    // Mobility (s·ªë moves c√≥ th·ªÉ)
    int aiMobility = GetValidMoves(state.aiPosition).Count;
    int humanMobility = GetValidMoves(state.humanPosition).Count;
    score += (aiMobility - humanMobility) * 20;
    
    // Center control
    score += GetCenterControlScore(state) * 30;
    
    return score;
}
```

3.5 ALPHA-BETA PRUNING HI·ªÜU QU·∫¢:
- Reduces search space t·ª´ O(b^d) xu·ªëng O(b^(d/2))
- Best case: 50% reduction in nodes evaluated
- Ordering moves by promising gi√∫p pruning hi·ªáu qu·∫£ h∆°n

================================================================================
                        4. SPECIALIZED AI STYLES SYSTEM
================================================================================

4.1 CONCEPT:
6 AI personalities v·ªõi training v√† behavior ri√™ng bi·ªát, m·ªói style c√≥:
- Ri√™ng Q-table file
- Custom reward structure
- Specific move prioritization
- Unique decision making logic

4.2 AI STYLES CHI TI·∫æT:

üî• AGGRESSIVE STYLE:
- M·ª•c ti√™u: T·∫•n c√¥ng v√† ch·∫∑n ƒë·ªëi th·ªß
- Wall Usage: 70% chance
- Reward Structure:
  * blockingOpponentReward = 50 * intensity
  * wallPlacementReward = 30 * intensity
  * defensiveWallPenalty = -20 * intensity
- Q-table: qtable_aggressive.json

üõ°Ô∏è DEFENSIVE STYLE:
- M·ª•c ti√™u: Ph√≤ng th·ªß v√† di chuy·ªÉn an to√†n
- Wall Usage: 50% chance
- Reward Structure:
  * defensiveWallReward = 35 * intensity
  * movementReward = 20 * intensity
  * aggressiveMovePenalty = -15 * intensity
- Q-table: qtable_defensive.json

üß± WALL MASTER STYLE:
- M·ª•c ti√™u: Chuy√™n gia ƒë·∫∑t t∆∞·ªùng v√† t·∫°o m√™ cung
- Wall Usage: 90% chance
- Reward Structure:
  * wallPlacementReward = 60 * intensity
  * strategicWallReward = 40 * intensity
  * wallComboReward = 50 * intensity
  * movementWithoutWallPenalty = -25 * intensity
- Q-table: qtable_wallmaster.json

‚ö° SPEED RUNNER STYLE:
- M·ª•c ti√™u: ƒê·∫°t ƒë√≠ch nhanh nh·∫•t
- Wall Usage: 10% chance
- Reward Structure:
  * movementReward = 40 * intensity
  * progressToGoalReward = 50 * intensity
  * quickWinBonus = 100 * intensity
  * wallPlacementPenalty = -20 * intensity
- Q-table: qtable_speedrunner.json

üß† TACTICAL STYLE:
- M·ª•c ti√™u: L·ª£i th·∫ø v·ªã tr√≠ v√† k·∫ø ho·∫°ch d√†i h·∫°n
- Wall Usage: Variable (strategic)
- Reward Structure:
  * tacticalMoveReward = 45 * intensity
  * positionAdvantageReward = 35 * intensity
  * mobilityReward = 25 * intensity
  * patternRecognitionBonus = 40 * intensity
- Q-table: qtable_tactical.json

‚öñÔ∏è BALANCED STYLE:
- M·ª•c ti√™u: C√¢n b·∫±ng t·∫•t c·∫£ aspects
- Wall Usage: 30% chance
- Reward Structure: Default balanced values
- Q-table: qtable.json

4.3 STYLE SWITCHING MECHANISM:

```csharp
public void SwitchToStyle(PlayStyle newStyle)
{
    // Save current Q-table
    if (allowQTableSaving) qAgent.SaveQTable(GetStyleSpecificQTablePath());
    
    // Load new style Q-table
    specializedStyle = newStyle;
    qAgent.LoadQTable(GetStyleSpecificQTablePath());
    
    // Reconfigure parameters
    ConfigureStyleSpecificTraining();
}
```

4.4 STYLE INTENSITY:
Parameter t·ª´ 0.0 ‚Üí 1.0 control m·ª©c ƒë·ªô chuy√™n h√≥a:
- 0.0: Balanced behavior
- 0.5: Moderate specialization
- 1.0: Extreme specialization

================================================================================
                        5. DECISION TREE CLASSIFIER
================================================================================

5.1 PURPOSE:
Decision Tree ph√¢n t√≠ch game context ƒë·ªÉ ch·ªçn strategy ph√π h·ª£p, d·ª±a tr√™n:
- Game phase (Early/Mid/End)
- Position advantage
- Resource advantage
- Mobility situation

5.2 FEATURE EXTRACTION:

```csharp
public struct DecisionFeatures
{
    public int aiDistanceToGoal;      // AI distance to goal line
    public int humanDistanceToGoal;   // Human distance to goal line
    public int aiWallsLeft;           // AI walls remaining
    public int humanWallsLeft;        // Human walls remaining
    public int aiMobility;            // AI possible moves
    public int humanMobility;         // Human possible moves
    public float distanceAdvantage;   // Human - AI distance
    public float wallAdvantage;       // AI - Human walls
    public bool isEarlyGame;          // > 15 moves estimated remaining
    public bool isMidGame;            // 5-15 moves remaining
    public bool isEndGame;            // < 5 moves remaining
}
```

5.3 DECISION LOGIC:

```csharp
public AIStrategy Predict(DecisionFeatures features)
{
    // Early Game: Play conservatively
    if (features.isEarlyGame)
        return AIStrategy.Balanced;
    
    // AI has significant lead: Focus on blocking
    if (features.distanceAdvantage > 2)
        return AIStrategy.Blocking;
    
    // AI is behind: Be aggressive
    if (features.distanceAdvantage < -1)
        return AIStrategy.Aggressive;
    
    // End game with wall advantage: Play defensively
    if (features.isEndGame && features.wallAdvantage > 2)
        return AIStrategy.Defensive;
    
    // Close game: Balanced approach
    return AIStrategy.Balanced;
}
```

5.4 STRATEGY OUTCOMES:
- Balanced: Equal weight to movement v√† walls
- Blocking: Prioritize moves that increase opponent path
- Aggressive: Focus on attacking positions
- Defensive: Protect own position and path

================================================================================
                        6. ENTROPY ANALYSIS & SOFTMAX
================================================================================

6.1 ENTROPY CONCEPT:
Entropy ƒëo ƒë·ªô ƒëa d·∫°ng/uncertainty c·ªßa move scores. High entropy = nhi·ªÅu moves
c√≥ scores t∆∞∆°ng ƒë∆∞∆°ng, low entropy = m·ªôt move r√µ r√†ng t·ªët nh·∫•t.

6.2 ENTROPY CALCULATION:

```csharp
float CalculateEntropy(List<float> moveScores)
{
    // Normalize th√†nh probabilities
    float total = moveScores.Sum();
    var probabilities = moveScores.Select(s => s / total);
    
    // Calculate Shannon entropy
    float entropy = 0f;
    foreach (float p in probabilities)
    {
        if (p > 0f)
            entropy -= p * Mathf.Log(p, 2f);
    }
    
    return entropy; // Range: 0 (deterministic) to log2(n) (uniform)
}
```

6.3 SOFTMAX DISTRIBUTION:

```csharp
float[] CalculateSoftmax(List<float> scores, float temperature)
{
    float[] probabilities = new float[scores.Count];
    float sum = scores.Sum(s => Mathf.Exp(s / temperature));
    
    for (int i = 0; i < scores.Count; i++)
    {
        probabilities[i] = Mathf.Exp(scores[i] / temperature) / sum;
    }
    
    return probabilities;
}
```

6.4 TEMPERATURE PARAMETER:
- Low temperature (0.1): Sharp distribution, focus on best moves
- High temperature (2.0): Flat distribution, more exploration
- Default temperature (1.0): Balanced exploitation/exploration

6.5 ADAPTIVE MOVE SELECTION:

```csharp
public AIMove ChooseActionWithEntropy(GameState state, List<AIMove> possibleMoves)
{
    var moveScores = EvaluateAllMoves(state, possibleMoves);
    float entropy = CalculateEntropy(moveScores);
    
    if (entropy > entropyThreshold) // High diversity
    {
        // Use softmax for probabilistic selection
        var probabilities = CalculateSoftmax(moveScores, softmaxTemperature);
        return SelectMoveByProbability(possibleMoves, probabilities);
    }
    else // Clear best move
    {
        // Use deterministic best move
        return GetBestScoredMove(possibleMoves, moveScores);
    }
}
```

================================================================================
                        7. TRAINING SYSTEM & SELF-PLAY
================================================================================

7.1 SELF-PLAY CONCEPT:
AI h·ªçc b·∫±ng c√°ch ch∆°i v·ªõi ch√≠nh n√≥ h√†ng ng√†n l·∫ßn, gradually improve performance
th√¥ng qua trial v√† error.

7.2 TRAINING PIPELINE:

```csharp
IEnumerator RunSelfPlayTraining()
{
    for (int episode = 1; episode <= maxEpisodes; episode++)
    {
        // Reset game state
        ResetGameForTraining();
        
        while (!IsGameOver())
        {
            // Get current state
            GameState currentState = GetCurrentGameState();
            List<AIMove> possibleMoves = GetPossibleMoves();
            
            // Choose action using Œµ-greedy
            AIMove chosenMove = qLearningAgent.ChooseAction(currentState, possibleMoves);
            
            // Execute move
            ExecuteMove(chosenMove);
            
            // Calculate reward
            float reward = CalculateReward(currentState, chosenMove);
            
            // Update Q-table
            GameState newState = GetCurrentGameState();
            qLearningAgent.UpdateQValue(
                EncodeState(currentState),
                EncodeAction(chosenMove),
                reward,
                EncodeState(newState)
            );
            
            yield return new WaitForEndOfFrame();
        }
        
        // Epsilon decay
        qLearningAgent.DecayEpsilon();
        
        // Save progress periodically
        if (episode % 100 == 0)
        {
            qLearningAgent.SaveQTable();
        }
    }
}
```

7.3 REWARD ENGINEERING:
Critical cho Q-Learning success:

```csharp
float CalculateReward(GameState state, AIMove move)
{
    float reward = 0f;
    
    // Game outcome rewards
    if (IsWin(state)) reward += 1000f;
    if (IsLoss(state)) reward -= 1000f;
    
    // Progress rewards
    if (move.moveType == MoveType.Movement)
    {
        if (IsMoveTowardGoal(move)) reward += 20f;
        else reward -= 5f;
    }
    
    // Strategic rewards
    if (move.moveType == MoveType.WallPlacement)
    {
        if (IsWallBlockingOpponent(move)) reward += 50f;
        if (IsWallStrategic(move)) reward += 30f;
        if (IsWallWasted(move)) reward -= 25f;
    }
    
    // Style-specific rewards
    reward += GetStyleSpecificReward(state, move);
    
    return reward;
}
```

7.4 TRAINING METRICS:
- Episodes: 1000 standard
- Duration: ~15 minutes
- Convergence: Usually around episode 800
- Q-table size: 13,118+ states at completion
- Final epsilon: 0.1 (90% exploitation)

================================================================================
                        8. AI TESTING FRAMEWORK
================================================================================

8.1 TESTING PHILOSOPHY:
Comprehensive testing ensures AI quality v√† reliability across different
game scenarios v√† edge cases.

8.2 TEST CATEGORIES:

A) DECISION TREE TESTS:
```csharp
void TestDecisionTreeStrategies()
{
    // Test Early Game
    var earlyGameFeatures = new DecisionFeatures
    {
        aiDistanceToGoal = 8, humanDistanceToGoal = 8,
        isEarlyGame = true
    };
    var strategy = decisionTree.Predict(earlyGameFeatures);
    Assert.AreEqual(AIStrategy.Balanced, strategy);
    
    // Test AI Leading
    var leadingFeatures = new DecisionFeatures
    {
        distanceAdvantage = 3, // AI leading
        isMidGame = true
    };
    strategy = decisionTree.Predict(leadingFeatures);
    Assert.AreEqual(AIStrategy.Blocking, strategy);
}
```

B) ENTROPY ANALYSIS TESTS:
```csharp
void TestEntropyAnalysis()
{
    // High entropy case (diverse scores)
    var highEntropyScores = new List<float> { 100f, 98f, 97f, 95f, 94f };
    float entropy = CalculateTestEntropy(highEntropyScores);
    Assert.IsTrue(entropy > 0.5f);
    
    // Low entropy case (clear winner)
    var lowEntropyScores = new List<float> { 100f, 50f, 50f, 50f, 50f };
    entropy = CalculateTestEntropy(lowEntropyScores);
    Assert.IsTrue(entropy < 0.5f);
}
```

C) SOFTMAX DISTRIBUTION TESTS:
```csharp
void TestSoftmaxDistribution()
{
    var testScores = new List<float> { 100f, 98f, 95f, 90f };
    float temperature = 1.0f;
    
    var probabilities = CalculateSoftmax(testScores, temperature);
    
    // Verify probabilities sum to 1
    float sum = probabilities.Sum();
    Assert.AreEqual(1.0f, sum, 0.001f);
    
    // Verify highest score gets highest probability
    int maxIndex = Array.IndexOf(probabilities, probabilities.Max());
    Assert.AreEqual(0, maxIndex);
}
```

D) PERFORMANCE BENCHMARKS:
- Move generation time: <100ms
- Q-Learning decision: <50ms
- Minimax decision (depth 4): <200ms
- Memory usage: <256MB

8.3 AUTOMATED TESTING:
Tests run automatically ƒë·ªÉ ensure code quality:
- Unit tests cho individual algorithms
- Integration tests cho AI system interactions
- Performance tests cho response times
- Regression tests ƒë·ªÉ prevent quality degradation

================================================================================
                        9. HYBRID AI ARCHITECTURE
================================================================================

9.1 CONCEPT:
Hybrid system k·∫øt h·ª£p strengths c·ªßa multiple algorithms:
- Q-Learning: Data-driven, learns from experience
- Minimax: Logical, guaranteed optimal given evaluation
- Decision Tree: Fast, context-aware switching
- Entropy: Adaptive, handles uncertainty

9.2 ALGORITHM SELECTION LOGIC:

```csharp
public AIMove GetBestMove()
{
    UpdateGameState();
    List<AIMove> possibleMoves = GeneratePossibleMoves();
    
    // Primary: Use Q-Learning if trained model available
    if (useQLearning && qAgent.IsTrainedModel())
    {
        Debug.Log($"üß† Using {specializedStyle} Q-Learning");
        return qAgent.ChooseAction(currentGameState, possibleMoves);
    }
    
    // Fallback: Use Minimax for guaranteed reasonable play
    else
    {
        Debug.Log($"üîç Using {specializedStyle} Minimax");
        return FindBestMoveWithMinimax();
    }
}
```

9.3 SEAMLESS SWITCHING:
- No interruption in gameplay
- Maintains consistent API
- Preserves game state integrity
- Logs algorithm used for debugging

9.4 BENEFITS:
- Robustness: Fallback ensures AI never fails
- Performance: Q-Learning for trained scenarios, Minimax for edge cases
- Flexibility: Can switch algorithms based on situation
- Development: Easy to test individual components

================================================================================
                        10. PERFORMANCE OPTIMIZATION
================================================================================

10.1 Q-TABLE OPTIMIZATION:
- Sparse representation: Only store visited states
- Compression: String interning for state keys
- Lazy loading: Load Q-table on demand
- Periodic saving: Every 100 training episodes

10.2 MINIMAX OPTIMIZATION:
- Alpha-Beta pruning: 50% search reduction
- Move ordering: Promising moves first
- Iterative deepening: Gradual depth increase
- Transposition tables: Cache evaluated positions

10.3 MEMORY MANAGEMENT:
- Object pooling cho GameState objects
- String caching cho repeated state/action encodings
- Garbage collection optimization
- Memory profiling v√† monitoring

10.4 MULTITHREADING CONSIDERATIONS:
- Q-Learning training: Background thread
- Move calculation: Main thread (Unity requirement)
- File I/O: Async operations where possible
- Thread-safe collections cho shared data

10.5 CACHING STRATEGIES:
- Path finding results
- Move generation results
- Evaluation function results
- Feature extraction results

================================================================================
                            K·∫æT LU·∫¨N
================================================================================

D·ª± √°n AI Quoridor Project th·ªÉ hi·ªán m·ªôt implementation comprehensive c·ªßa modern
game AI techniques:

TECHNICAL INNOVATIONS:
‚úÖ Hybrid Q-Learning + Minimax architecture
‚úÖ 6 specialized AI personalities v·ªõi separate training
‚úÖ Advanced decision making v·ªõi entropy analysis
‚úÖ Comprehensive testing v√† validation framework
‚úÖ Professional development tools v√† debugging

EDUCATIONAL VALUE:
‚úÖ Practical machine learning implementation
‚úÖ Traditional game AI algorithms
‚úÖ Modern software engineering practices
‚úÖ Performance optimization techniques
‚úÖ Automated testing methodologies

FUTURE ENHANCEMENTS:
üöÄ Deep Q-Learning (DQN) integration
üöÄ Multi-agent reinforcement learning
üöÄ Neural network-based evaluation functions
üöÄ Online learning v√† adaptation
üöÄ Transfer learning between game variants

D·ª± √°n n√†y demonstrating successful integration c·ªßa classical AI methods v·ªõi
modern machine learning, t·∫°o ra m·ªôt intelligent, adaptive, v√† highly
interactive gaming experience.

================================================================================
                            APPENDIX
================================================================================

FILES LI√äN QUAN:
- Assets/scripts/QLearningAgent.cs - Q-Learning implementation
- Assets/scripts/QuoridorAI.cs - Main AI controller
- Assets/scripts/DecisionTreeClassifier.cs - Decision tree logic
- Assets/scripts/QuoridorSelfPlayTrainer.cs - Training system
- Assets/scripts/AITestScript.cs - Testing framework
- Assets/scripts/Editor/ForceRetrainAI.cs - Development tools

Q-TABLE FILES:
- StreamingAssets/qtable.json - Balanced style
- StreamingAssets/qtable_aggressive.json - Aggressive style
- StreamingAssets/qtable_defensive.json - Defensive style
- StreamingAssets/qtable_wallmaster.json - Wall Master style
- StreamingAssets/qtable_speedrunner.json - Speed Runner style
- StreamingAssets/qtable_tactical.json - Tactical style

DOCUMENTATION:
- AI_ALGORITHMS_DETAILED_DESCRIPTION.txt - This file
- README.md - Project overview
- API_INTEGRATION_GUIDE.md - Voice/Chat integration
- TRAINING_GUIDE.md - AI training instructions

================================================================================
================================================================================
                    AI QUORIDOR GAME PROJECT - THU·∫¨T TO√ÅN AI CHI TI·∫æT
================================================================================

D·ª± √°n: AI Quoridor Game v·ªõi Machine Learning v√† Multi-Platform Integration
Ng√†y t·∫°o: 25/07/2025
T√°c gi·∫£: [Student Name]
Platform: Unity 3D
Language: C#

================================================================================
                            M·ª§C L·ª§C
================================================================================

1. T·ªîNG QUAN KI·∫æN TR√öC AI
2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
4. SPECIALIZED AI STYLES SYSTEM
5. DECISION TREE CLASSIFIER
6. ENTROPY ANALYSIS & SOFTMAX
7. TRAINING SYSTEM & SELF-PLAY
8. AI TESTING FRAMEWORK
9. HYBRID AI ARCHITECTURE
10. PERFORMANCE OPTIMIZATION

================================================================================
                        1. T·ªîNG QUAN KI·∫æN TR√öC AI
================================================================================

D·ª± √°n AI Quoridor s·ª≠ d·ª•ng m·ªôt ki·∫øn tr√∫c AI hybrid ti√™n ti·∫øn, k·∫øt h·ª£p:

1.1 CORE ALGORITHMS:
- Q-Learning (Reinforcement Learning) - Thu·∫≠t to√°n ch√≠nh
- Minimax + Alpha-Beta Pruning - Thu·∫≠t to√°n fallback
- Decision Tree Classifier - Ph√¢n t√≠ch ng·ªØ c·∫£nh game
- Entropy Analysis - ƒê√°nh gi√° ƒëa d·∫°ng move

1.2 SPECIALIZED SYSTEMS:
- 6 AI Personality Styles v·ªõi training ri√™ng bi·ªát
- Self-Play Training Pipeline t·ª± ƒë·ªông
- Real-time Performance Testing Framework
- Dynamic Algorithm Switching

1.3 TECHNICAL FOUNDATION:
- State Space: 13,118+ unique game states
- Action Space: Movement + Wall placement combinations
- Training Method: Self-play v·ªõi 1000 episodes
- Learning Rate: Œ± = 0.1, Discount Factor: Œ≥ = 0.99

================================================================================
                    2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
================================================================================

2.1 GI·ªöI THI·ªÜU Q-LEARNING:
Q-Learning l√† thu·∫≠t to√°n Reinforcement Learning kh√¥ng c·∫ßn model, h·ªçc th√¥ng qua
t∆∞∆°ng t√°c v·ªõi m√¥i tr∆∞·ªùng ƒë·ªÉ t·ªëi ∆∞u h√≥a policy. Trong Quoridor, AI h·ªçc c√°ch
ch·ªçn moves t·ªëi ∆∞u d·ª±a tr√™n kinh nghi·ªám t·ª´ h√†ng ng√†n game.

2.2 C√îNG TH·ª®C TO√ÅN H·ªåC:
Q(s,a) = Q(s,a) + Œ±[r + Œ≥*max(Q(s',a')) - Q(s,a)]

Trong ƒë√≥:
- Q(s,a): Q-value c·ªßa state s v·ªõi action a
- Œ±: Learning rate (0.1) - t·ªëc ƒë·ªô h·ªçc
- r: Immediate reward t·ª´ action
- Œ≥: Discount factor (0.99) - t·∫ßm quan tr·ªçng future rewards
- s': Next state sau khi th·ª±c hi·ªán action
- max(Q(s',a')): Maximum Q-value c·ªßa next state

2.3 IMPLEMENTATION CHI TI·∫æT:

```csharp
public class QLearningAgent
{
    // Core Q-Table: Dictionary l∆∞u tr·ªØ Q-values
    private Dictionary<string, Dictionary<string, float>> qTable;
    
    // Hyperparameters
    private float learningRate = 0.1f;      // T·ªëc ƒë·ªô h·ªçc
    private float discountFactor = 0.99f;   // Discount future rewards
    private float epsilon = 0.995f;         // Exploration rate
    
    // Q-Learning Update Implementation
    public void UpdateQValue(string state, string action, float reward, string nextState)
    {
        // L·∫•y Q-value hi·ªán t·∫°i
        float currentQ = GetQValue(state, action);
        
        // T√¨m max Q-value c·ªßa next state
        float maxNextQ = GetMaxQValue(nextState);
        
        // √Åp d·ª•ng c√¥ng th·ª©c Q-Learning
        float newQ = currentQ + learningRate * (reward + discountFactor * maxNextQ - currentQ);
        
        // C·∫≠p nh·∫≠t Q-table
        SetQValue(state, action, newQ);
    }
}
```

2.4 STATE ENCODING:
Game state ƒë∆∞·ª£c encode th√†nh string duy nh·∫•t:
"aiPos_x,aiPos_y-humanPos_x,humanPos_y-aiWalls-humanWalls-wallsList"

V√≠ d·ª•: "4,8-4,0-9-10-7,4,False|5,6,True"
- AI ·ªü (4,8), Human ·ªü (4,0)
- AI c√≥ 9 walls, Human c√≥ 10 walls
- Walls ƒë√£ ƒë·∫∑t: (7,4,False), (5,6,True)

2.5 ACTION ENCODING:
Actions ƒë∆∞·ª£c encode th√†nh string:
- Movement: "M:x,y" (v√≠ d·ª•: "M:4,7")
- Wall: "W:x,y,horizontal" (v√≠ d·ª•: "W:3,5,True")

2.6 EPSILON-GREEDY STRATEGY:
```csharp
public AIMove ChooseAction(GameState state, List<AIMove> possibleMoves)
{
    // Exploration vs Exploitation trade-off
    if (Random.Range(0f, 1f) < epsilon)
    {
        // EXPLORATION: Ch·ªçn random ƒë·ªÉ kh√°m ph√°
        return possibleMoves[Random.Range(0, possibleMoves.Count)];
    }
    else
    {
        // EXPLOITATION: Ch·ªçn action c√≥ Q-value cao nh·∫•t
        return GetBestAction(state, possibleMoves);
    }
}
```

2.7 EPSILON DECAY:
Epsilon gi·∫£m d·∫ßn t·ª´ 0.995 ‚Üí 0.1 qua training:
- Ban ƒë·∫ßu: High exploration (99.5%)
- Cu·ªëi training: High exploitation (90%)

2.8 REWARD SYSTEM:
- Win game: +1000
- Lose game: -1000
- Move toward goal: +10
- Block opponent: +50
- Waste move: -5
- Strategic position: +15

================================================================================
                3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
================================================================================

3.1 GI·ªöI THI·ªÜU MINIMAX:
Minimax l√† thu·∫≠t to√°n game theory c·ªï ƒëi·ªÉn, t√¨m ki·∫øm move t·ªëi ∆∞u b·∫±ng c√°ch
d·ª± ƒëo√°n tr∆∞·ªõc nhi·ªÅu moves. Alpha-Beta pruning t·ªëi ∆∞u hi·ªáu su·∫•t b·∫±ng c√°ch
lo·∫°i b·ªè c√°c nh√°nh kh√¥ng c·∫ßn thi·∫øt.

3.2 NGUY√äN L√ù HO·∫†T ƒê·ªòNG:
- Maximizing Player (AI): T·ªëi ƒëa h√≥a score
- Minimizing Player (Human): T·ªëi thi·ªÉu h√≥a score AI
- Search depth: 4 levels (configurable)
- Pruning: Lo·∫°i b·ªè nh√°nh kh√¥ng promising

3.3 IMPLEMENTATION:

```csharp
float Minimax(GameState state, int depth, bool isMaximizing, float alpha, float beta)
{
    // Base case: Depth = 0 ho·∫∑c game over
    if (depth == 0 || IsGameOver(state))
        return EvaluatePosition(state);
    
    if (isMaximizing) // AI turn
    {
        float maxEval = float.NegativeInfinity;
        foreach (var move in GeneratePossibleMoves(state, true))
        {
            GameState newState = ApplyMove(state, move, true);
            float eval = Minimax(newState, depth - 1, false, alpha, beta);
            maxEval = Mathf.Max(maxEval, eval);
            alpha = Mathf.Max(alpha, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return maxEval;
    }
    else // Human turn
    {
        float minEval = float.PositiveInfinity;
        foreach (var move in GeneratePossibleMoves(state, false))
        {
            GameState newState = ApplyMove(state, move, false);
            float eval = Minimax(newState, depth - 1, true, alpha, beta);
            minEval = Mathf.Min(minEval, eval);
            beta = Mathf.Min(beta, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return minEval;
    }
}
```

3.4 EVALUATION FUNCTION:
Position evaluation d·ª±a tr√™n multiple factors:

```csharp
float EvaluatePosition(GameState state)
{
    float score = 0f;
    
    // Distance to goal (c√†ng g·∫ßn c√†ng t·ªët)
    int aiDistance = GetShortestPath(state.aiPosition, goalRow);
    int humanDistance = GetShortestPath(state.humanPosition, goalRow);
    score += (humanDistance - aiDistance) * 100;
    
    // Wall advantage
    score += (state.aiWallsLeft - state.humanWallsLeft) * 50;
    
    // Mobility (s·ªë moves c√≥ th·ªÉ)
    int aiMobility = GetValidMoves(state.aiPosition).Count;
    int humanMobility = GetValidMoves(state.humanPosition).Count;
    score += (aiMobility - humanMobility) * 20;
    
    // Center control
    score += GetCenterControlScore(state) * 30;
    
    return score;
}
```

3.5 ALPHA-BETA PRUNING HI·ªÜU QU·∫¢:
- Reduces search space t·ª´ O(b^d) xu·ªëng O(b^(d/2))
- Best case: 50% reduction in nodes evaluated
- Ordering moves by promising gi√∫p pruning hi·ªáu qu·∫£ h∆°n

================================================================================
                        4. SPECIALIZED AI STYLES SYSTEM
================================================================================

4.1 CONCEPT:
6 AI personalities v·ªõi training v√† behavior ri√™ng bi·ªát, m·ªói style c√≥:
- Ri√™ng Q-table file
- Custom reward structure
- Specific move prioritization
- Unique decision making logic

4.2 AI STYLES CHI TI·∫æT:

üî• AGGRESSIVE STYLE:
- M·ª•c ti√™u: T·∫•n c√¥ng v√† ch·∫∑n ƒë·ªëi th·ªß
- Wall Usage: 70% chance
- Reward Structure:
  * blockingOpponentReward = 50 * intensity
  * wallPlacementReward = 30 * intensity
  * defensiveWallPenalty = -20 * intensity
- Q-table: qtable_aggressive.json

üõ°Ô∏è DEFENSIVE STYLE:
- M·ª•c ti√™u: Ph√≤ng th·ªß v√† di chuy·ªÉn an to√†n
- Wall Usage: 50% chance
- Reward Structure:
  * defensiveWallReward = 35 * intensity
  * movementReward = 20 * intensity
  * aggressiveMovePenalty = -15 * intensity
- Q-table: qtable_defensive.json

üß± WALL MASTER STYLE:
- M·ª•c ti√™u: Chuy√™n gia ƒë·∫∑t t∆∞·ªùng v√† t·∫°o m√™ cung
- Wall Usage: 90% chance
- Reward Structure:
  * wallPlacementReward = 60 * intensity
  * strategicWallReward = 40 * intensity
  * wallComboReward = 50 * intensity
  * movementWithoutWallPenalty = -25 * intensity
- Q-table: qtable_wallmaster.json

‚ö° SPEED RUNNER STYLE:
- M·ª•c ti√™u: ƒê·∫°t ƒë√≠ch nhanh nh·∫•t
- Wall Usage: 10% chance
- Reward Structure:
  * movementReward = 40 * intensity
  * progressToGoalReward = 50 * intensity
  * quickWinBonus = 100 * intensity
  * wallPlacementPenalty = -20 * intensity
- Q-table: qtable_speedrunner.json

üß† TACTICAL STYLE:
- M·ª•c ti√™u: L·ª£i th·∫ø v·ªã tr√≠ v√† k·∫ø ho·∫°ch d√†i h·∫°n
- Wall Usage: Variable (strategic)
- Reward Structure:
  * tacticalMoveReward = 45 * intensity
  * positionAdvantageReward = 35 * intensity
  * mobilityReward = 25 * intensity
  * patternRecognitionBonus = 40 * intensity
- Q-table: qtable_tactical.json

‚öñÔ∏è BALANCED STYLE:
- M·ª•c ti√™u: C√¢n b·∫±ng t·∫•t c·∫£ aspects
- Wall Usage: 30% chance
- Reward Structure: Default balanced values
- Q-table: qtable.json

4.3 STYLE SWITCHING MECHANISM:

```csharp
public void SwitchToStyle(PlayStyle newStyle)
{
    // Save current Q-table
    if (allowQTableSaving) qAgent.SaveQTable(GetStyleSpecificQTablePath());
    
    // Load new style Q-table
    specializedStyle = newStyle;
    qAgent.LoadQTable(GetStyleSpecificQTablePath());
    
    // Reconfigure parameters
    ConfigureStyleSpecificTraining();
}
```

4.4 STYLE INTENSITY:
Parameter t·ª´ 0.0 ‚Üí 1.0 control m·ª©c ƒë·ªô chuy√™n h√≥a:
- 0.0: Balanced behavior
- 0.5: Moderate specialization
- 1.0: Extreme specialization

================================================================================
                        5. DECISION TREE CLASSIFIER
================================================================================

5.1 PURPOSE:
Decision Tree ph√¢n t√≠ch game context ƒë·ªÉ ch·ªçn strategy ph√π h·ª£p, d·ª±a tr√™n:
- Game phase (Early/Mid/End)
- Position advantage
- Resource advantage
- Mobility situation

5.2 FEATURE EXTRACTION:

```csharp
public struct DecisionFeatures
{
    public int aiDistanceToGoal;      // AI distance to goal line
    public int humanDistanceToGoal;   // Human distance to goal line
    public int aiWallsLeft;           // AI walls remaining
    public int humanWallsLeft;        // Human walls remaining
    public int aiMobility;            // AI possible moves
    public int humanMobility;         // Human possible moves
    public float distanceAdvantage;   // Human - AI distance
    public float wallAdvantage;       // AI - Human walls
    public bool isEarlyGame;          // > 15 moves estimated remaining
    public bool isMidGame;            // 5-15 moves remaining
    public bool isEndGame;            // < 5 moves remaining
}
```

5.3 DECISION LOGIC:

```csharp
public AIStrategy Predict(DecisionFeatures features)
{
    // Early Game: Play conservatively
    if (features.isEarlyGame)
        return AIStrategy.Balanced;
    
    // AI has significant lead: Focus on blocking
    if (features.distanceAdvantage > 2)
        return AIStrategy.Blocking;
    
    // AI is behind: Be aggressive
    if (features.distanceAdvantage < -1)
        return AIStrategy.Aggressive;
    
    // End game with wall advantage: Play defensively
    if (features.isEndGame && features.wallAdvantage > 2)
        return AIStrategy.Defensive;
    
    // Close game: Balanced approach
    return AIStrategy.Balanced;
}
```

5.4 STRATEGY OUTCOMES:
- Balanced: Equal weight to movement v√† walls
- Blocking: Prioritize moves that increase opponent path
- Aggressive: Focus on attacking positions
- Defensive: Protect own position and path

================================================================================
                        6. ENTROPY ANALYSIS & SOFTMAX
================================================================================

6.1 ENTROPY CONCEPT:
Entropy ƒëo ƒë·ªô ƒëa d·∫°ng/uncertainty c·ªßa move scores. High entropy = nhi·ªÅu moves
c√≥ scores t∆∞∆°ng ƒë∆∞∆°ng, low entropy = m·ªôt move r√µ r√†ng t·ªët nh·∫•t.

6.2 ENTROPY CALCULATION:

```csharp
float CalculateEntropy(List<float> moveScores)
{
    // Normalize th√†nh probabilities
    float total = moveScores.Sum();
    var probabilities = moveScores.Select(s => s / total);
    
    // Calculate Shannon entropy
    float entropy = 0f;
    foreach (float p in probabilities)
    {
        if (p > 0f)
            entropy -= p * Mathf.Log(p, 2f);
    }
    
    return entropy; // Range: 0 (deterministic) to log2(n) (uniform)
}
```

6.3 SOFTMAX DISTRIBUTION:

```csharp
float[] CalculateSoftmax(List<float> scores, float temperature)
{
    float[] probabilities = new float[scores.Count];
    float sum = scores.Sum(s => Mathf.Exp(s / temperature));
    
    for (int i = 0; i < scores.Count; i++)
    {
        probabilities[i] = Mathf.Exp(scores[i] / temperature) / sum;
    }
    
    return probabilities;
}
```

6.4 TEMPERATURE PARAMETER:
- Low temperature (0.1): Sharp distribution, focus on best moves
- High temperature (2.0): Flat distribution, more exploration
- Default temperature (1.0): Balanced exploitation/exploration

6.5 ADAPTIVE MOVE SELECTION:

```csharp
public AIMove ChooseActionWithEntropy(GameState state, List<AIMove> possibleMoves)
{
    var moveScores = EvaluateAllMoves(state, possibleMoves);
    float entropy = CalculateEntropy(moveScores);
    
    if (entropy > entropyThreshold) // High diversity
    {
        // Use softmax for probabilistic selection
        var probabilities = CalculateSoftmax(moveScores, softmaxTemperature);
        return SelectMoveByProbability(possibleMoves, probabilities);
    }
    else // Clear best move
    {
        // Use deterministic best move
        return GetBestScoredMove(possibleMoves, moveScores);
    }
}
```

================================================================================
                        7. TRAINING SYSTEM & SELF-PLAY
================================================================================

7.1 SELF-PLAY CONCEPT:
AI h·ªçc b·∫±ng c√°ch ch∆°i v·ªõi ch√≠nh n√≥ h√†ng ng√†n l·∫ßn, gradually improve performance
th√¥ng qua trial v√† error.

7.2 TRAINING PIPELINE:

```csharp
IEnumerator RunSelfPlayTraining()
{
    for (int episode = 1; episode <= maxEpisodes; episode++)
    {
        // Reset game state
        ResetGameForTraining();
        
        while (!IsGameOver())
        {
            // Get current state
            GameState currentState = GetCurrentGameState();
            List<AIMove> possibleMoves = GetPossibleMoves();
            
            // Choose action using Œµ-greedy
            AIMove chosenMove = qLearningAgent.ChooseAction(currentState, possibleMoves);
            
            // Execute move
            ExecuteMove(chosenMove);
            
            // Calculate reward
            float reward = CalculateReward(currentState, chosenMove);
            
            // Update Q-table
            GameState newState = GetCurrentGameState();
            qLearningAgent.UpdateQValue(
                EncodeState(currentState),
                EncodeAction(chosenMove),
                reward,
                EncodeState(newState)
            );
            
            yield return new WaitForEndOfFrame();
        }
        
        // Epsilon decay
        qLearningAgent.DecayEpsilon();
        
        // Save progress periodically
        if (episode % 100 == 0)
        {
            qLearningAgent.SaveQTable();
        }
    }
}
```

7.3 REWARD ENGINEERING:
Critical cho Q-Learning success:

```csharp
float CalculateReward(GameState state, AIMove move)
{
    float reward = 0f;
    
    // Game outcome rewards
    if (IsWin(state)) reward += 1000f;
    if (IsLoss(state)) reward -= 1000f;
    
    // Progress rewards
    if (move.moveType == MoveType.Movement)
    {
        if (IsMoveTowardGoal(move)) reward += 20f;
        else reward -= 5f;
    }
    
    // Strategic rewards
    if (move.moveType == MoveType.WallPlacement)
    {
        if (IsWallBlockingOpponent(move)) reward += 50f;
        if (IsWallStrategic(move)) reward += 30f;
        if (IsWallWasted(move)) reward -= 25f;
    }
    
    // Style-specific rewards
    reward += GetStyleSpecificReward(state, move);
    
    return reward;
}
```

7.4 TRAINING METRICS:
- Episodes: 1000 standard
- Duration: ~15 minutes
- Convergence: Usually around episode 800
- Q-table size: 13,118+ states at completion
- Final epsilon: 0.1 (90% exploitation)

================================================================================
                        8. AI TESTING FRAMEWORK
================================================================================

8.1 TESTING PHILOSOPHY:
Comprehensive testing ensures AI quality v√† reliability across different
game scenarios v√† edge cases.

8.2 TEST CATEGORIES:

A) DECISION TREE TESTS:
```csharp
void TestDecisionTreeStrategies()
{
    // Test Early Game
    var earlyGameFeatures = new DecisionFeatures
    {
        aiDistanceToGoal = 8, humanDistanceToGoal = 8,
        isEarlyGame = true
    };
    var strategy = decisionTree.Predict(earlyGameFeatures);
    Assert.AreEqual(AIStrategy.Balanced, strategy);
    
    // Test AI Leading
    var leadingFeatures = new DecisionFeatures
    {
        distanceAdvantage = 3, // AI leading
        isMidGame = true
    };
    strategy = decisionTree.Predict(leadingFeatures);
    Assert.AreEqual(AIStrategy.Blocking, strategy);
}
```

B) ENTROPY ANALYSIS TESTS:
```csharp
void TestEntropyAnalysis()
{
    // High entropy case (diverse scores)
    var highEntropyScores = new List<float> { 100f, 98f, 97f, 95f, 94f };
    float entropy = CalculateTestEntropy(highEntropyScores);
    Assert.IsTrue(entropy > 0.5f);
    
    // Low entropy case (clear winner)
    var lowEntropyScores = new List<float> { 100f, 50f, 50f, 50f, 50f };
    entropy = CalculateTestEntropy(lowEntropyScores);
    Assert.IsTrue(entropy < 0.5f);
}
```

C) SOFTMAX DISTRIBUTION TESTS:
```csharp
void TestSoftmaxDistribution()
{
    var testScores = new List<float> { 100f, 98f, 95f, 90f };
    float temperature = 1.0f;
    
    var probabilities = CalculateSoftmax(testScores, temperature);
    
    // Verify probabilities sum to 1
    float sum = probabilities.Sum();
    Assert.AreEqual(1.0f, sum, 0.001f);
    
    // Verify highest score gets highest probability
    int maxIndex = Array.IndexOf(probabilities, probabilities.Max());
    Assert.AreEqual(0, maxIndex);
}
```

D) PERFORMANCE BENCHMARKS:
- Move generation time: <100ms
- Q-Learning decision: <50ms
- Minimax decision (depth 4): <200ms
- Memory usage: <256MB

8.3 AUTOMATED TESTING:
Tests run automatically ƒë·ªÉ ensure code quality:
- Unit tests cho individual algorithms
- Integration tests cho AI system interactions
- Performance tests cho response times
- Regression tests ƒë·ªÉ prevent quality degradation

================================================================================
                        9. HYBRID AI ARCHITECTURE
================================================================================

9.1 CONCEPT:
Hybrid system k·∫øt h·ª£p strengths c·ªßa multiple algorithms:
- Q-Learning: Data-driven, learns from experience
- Minimax: Logical, guaranteed optimal given evaluation
- Decision Tree: Fast, context-aware switching
- Entropy: Adaptive, handles uncertainty

9.2 ALGORITHM SELECTION LOGIC:

```csharp
public AIMove GetBestMove()
{
    UpdateGameState();
    List<AIMove> possibleMoves = GeneratePossibleMoves();
    
    // Primary: Use Q-Learning if trained model available
    if (useQLearning && qAgent.IsTrainedModel())
    {
        Debug.Log($"üß† Using {specializedStyle} Q-Learning");
        return qAgent.ChooseAction(currentGameState, possibleMoves);
    }
    
    // Fallback: Use Minimax for guaranteed reasonable play
    else
    {
        Debug.Log($"üîç Using {specializedStyle} Minimax");
        return FindBestMoveWithMinimax();
    }
}
```

9.3 SEAMLESS SWITCHING:
- No interruption in gameplay
- Maintains consistent API
- Preserves game state integrity
- Logs algorithm used for debugging

9.4 BENEFITS:
- Robustness: Fallback ensures AI never fails
- Performance: Q-Learning for trained scenarios, Minimax for edge cases
- Flexibility: Can switch algorithms based on situation
- Development: Easy to test individual components

================================================================================
                        10. PERFORMANCE OPTIMIZATION
================================================================================

10.1 Q-TABLE OPTIMIZATION:
- Sparse representation: Only store visited states
- Compression: String interning for state keys
- Lazy loading: Load Q-table on demand
- Periodic saving: Every 100 training episodes

10.2 MINIMAX OPTIMIZATION:
- Alpha-Beta pruning: 50% search reduction
- Move ordering: Promising moves first
- Iterative deepening: Gradual depth increase
- Transposition tables: Cache evaluated positions

10.3 MEMORY MANAGEMENT:
- Object pooling cho GameState objects
- String caching cho repeated state/action encodings
- Garbage collection optimization
- Memory profiling v√† monitoring

10.4 MULTITHREADING CONSIDERATIONS:
- Q-Learning training: Background thread
- Move calculation: Main thread (Unity requirement)
- File I/O: Async operations where possible
- Thread-safe collections cho shared data

10.5 CACHING STRATEGIES:
- Path finding results
- Move generation results
- Evaluation function results
- Feature extraction results

================================================================================
                            K·∫æT LU·∫¨N
================================================================================

D·ª± √°n AI Quoridor Project th·ªÉ hi·ªán m·ªôt implementation comprehensive c·ªßa modern
game AI techniques:

TECHNICAL INNOVATIONS:
‚úÖ Hybrid Q-Learning + Minimax architecture
‚úÖ 6 specialized AI personalities v·ªõi separate training
‚úÖ Advanced decision making v·ªõi entropy analysis
‚úÖ Comprehensive testing v√† validation framework
‚úÖ Professional development tools v√† debugging

EDUCATIONAL VALUE:
‚úÖ Practical machine learning implementation
‚úÖ Traditional game AI algorithms
‚úÖ Modern software engineering practices
‚úÖ Performance optimization techniques
‚úÖ Automated testing methodologies

FUTURE ENHANCEMENTS:
üöÄ Deep Q-Learning (DQN) integration
üöÄ Multi-agent reinforcement learning
üöÄ Neural network-based evaluation functions
üöÄ Online learning v√† adaptation
üöÄ Transfer learning between game variants

D·ª± √°n n√†y demonstrating successful integration c·ªßa classical AI methods v·ªõi
modern machine learning, t·∫°o ra m·ªôt intelligent, adaptive, v√† highly
interactive gaming experience.

================================================================================
                            APPENDIX
================================================================================

FILES LI√äN QUAN:
- Assets/scripts/QLearningAgent.cs - Q-Learning implementation
- Assets/scripts/QuoridorAI.cs - Main AI controller
- Assets/scripts/DecisionTreeClassifier.cs - Decision tree logic
- Assets/scripts/QuoridorSelfPlayTrainer.cs - Training system
- Assets/scripts/AITestScript.cs - Testing framework
- Assets/scripts/Editor/ForceRetrainAI.cs - Development tools

Q-TABLE FILES:
- StreamingAssets/qtable.json - Balanced style
- StreamingAssets/qtable_aggressive.json - Aggressive style
- StreamingAssets/qtable_defensive.json - Defensive style
- StreamingAssets/qtable_wallmaster.json - Wall Master style
- StreamingAssets/qtable_speedrunner.json - Speed Runner style
- StreamingAssets/qtable_tactical.json - Tactical style

DOCUMENTATION:
- AI_ALGORITHMS_DETAILED_DESCRIPTION.txt - This file
- README.md - Project overview
- API_INTEGRATION_GUIDE.md - Voice/Chat integration
- TRAINING_GUIDE.md - AI training instructions

================================================================================
================================================================================
                    AI QUORIDOR GAME PROJECT - THU·∫¨T TO√ÅN AI CHI TI·∫æT
================================================================================

D·ª± √°n: AI Quoridor Game v·ªõi Machine Learning v√† Multi-Platform Integration
Ng√†y t·∫°o: 25/07/2025
T√°c gi·∫£: [Student Name]
Platform: Unity 3D
Language: C#

================================================================================
                            M·ª§C L·ª§C
================================================================================

1. T·ªîNG QUAN KI·∫æN TR√öC AI
2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
4. SPECIALIZED AI STYLES SYSTEM
5. DECISION TREE CLASSIFIER
6. ENTROPY ANALYSIS & SOFTMAX
7. TRAINING SYSTEM & SELF-PLAY
8. AI TESTING FRAMEWORK
9. HYBRID AI ARCHITECTURE
10. PERFORMANCE OPTIMIZATION

================================================================================
                        1. T·ªîNG QUAN KI·∫æN TR√öC AI
================================================================================

D·ª± √°n AI Quoridor s·ª≠ d·ª•ng m·ªôt ki·∫øn tr√∫c AI hybrid ti√™n ti·∫øn, k·∫øt h·ª£p:

1.1 CORE ALGORITHMS:
- Q-Learning (Reinforcement Learning) - Thu·∫≠t to√°n ch√≠nh
- Minimax + Alpha-Beta Pruning - Thu·∫≠t to√°n fallback
- Decision Tree Classifier - Ph√¢n t√≠ch ng·ªØ c·∫£nh game
- Entropy Analysis - ƒê√°nh gi√° ƒëa d·∫°ng move

1.2 SPECIALIZED SYSTEMS:
- 6 AI Personality Styles v·ªõi training ri√™ng bi·ªát
- Self-Play Training Pipeline t·ª± ƒë·ªông
- Real-time Performance Testing Framework
- Dynamic Algorithm Switching

1.3 TECHNICAL FOUNDATION:
- State Space: 13,118+ unique game states
- Action Space: Movement + Wall placement combinations
- Training Method: Self-play v·ªõi 1000 episodes
- Learning Rate: Œ± = 0.1, Discount Factor: Œ≥ = 0.99

================================================================================
                    2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
================================================================================

2.1 GI·ªöI THI·ªÜU Q-LEARNING:
Q-Learning l√† thu·∫≠t to√°n Reinforcement Learning kh√¥ng c·∫ßn model, h·ªçc th√¥ng qua
t∆∞∆°ng t√°c v·ªõi m√¥i tr∆∞·ªùng ƒë·ªÉ t·ªëi ∆∞u h√≥a policy. Trong Quoridor, AI h·ªçc c√°ch
ch·ªçn moves t·ªëi ∆∞u d·ª±a tr√™n kinh nghi·ªám t·ª´ h√†ng ng√†n game.

2.2 C√îNG TH·ª®C TO√ÅN H·ªåC:
Q(s,a) = Q(s,a) + Œ±[r + Œ≥*max(Q(s',a')) - Q(s,a)]

Trong ƒë√≥:
- Q(s,a): Q-value c·ªßa state s v·ªõi action a
- Œ±: Learning rate (0.1) - t·ªëc ƒë·ªô h·ªçc
- r: Immediate reward t·ª´ action
- Œ≥: Discount factor (0.99) - t·∫ßm quan tr·ªçng future rewards
- s': Next state sau khi th·ª±c hi·ªán action
- max(Q(s',a')): Maximum Q-value c·ªßa next state

2.3 IMPLEMENTATION CHI TI·∫æT:

```csharp
public class QLearningAgent
{
    // Core Q-Table: Dictionary l∆∞u tr·ªØ Q-values
    private Dictionary<string, Dictionary<string, float>> qTable;
    
    // Hyperparameters
    private float learningRate = 0.1f;      // T·ªëc ƒë·ªô h·ªçc
    private float discountFactor = 0.99f;   // Discount future rewards
    private float epsilon = 0.995f;         // Exploration rate
    
    // Q-Learning Update Implementation
    public void UpdateQValue(string state, string action, float reward, string nextState)
    {
        // L·∫•y Q-value hi·ªán t·∫°i
        float currentQ = GetQValue(state, action);
        
        // T√¨m max Q-value c·ªßa next state
        float maxNextQ = GetMaxQValue(nextState);
        
        // √Åp d·ª•ng c√¥ng th·ª©c Q-Learning
        float newQ = currentQ + learningRate * (reward + discountFactor * maxNextQ - currentQ);
        
        // C·∫≠p nh·∫≠t Q-table
        SetQValue(state, action, newQ);
    }
}
```

2.4 STATE ENCODING:
Game state ƒë∆∞·ª£c encode th√†nh string duy nh·∫•t:
"aiPos_x,aiPos_y-humanPos_x,humanPos_y-aiWalls-humanWalls-wallsList"

V√≠ d·ª•: "4,8-4,0-9-10-7,4,False|5,6,True"
- AI ·ªü (4,8), Human ·ªü (4,0)
- AI c√≥ 9 walls, Human c√≥ 10 walls
- Walls ƒë√£ ƒë·∫∑t: (7,4,False), (5,6,True)

2.5 ACTION ENCODING:
Actions ƒë∆∞·ª£c encode th√†nh string:
- Movement: "M:x,y" (v√≠ d·ª•: "M:4,7")
- Wall: "W:x,y,horizontal" (v√≠ d·ª•: "W:3,5,True")

2.6 EPSILON-GREEDY STRATEGY:
```csharp
public AIMove ChooseAction(GameState state, List<AIMove> possibleMoves)
{
    // Exploration vs Exploitation trade-off
    if (Random.Range(0f, 1f) < epsilon)
    {
        // EXPLORATION: Ch·ªçn random ƒë·ªÉ kh√°m ph√°
        return possibleMoves[Random.Range(0, possibleMoves.Count)];
    }
    else
    {
        // EXPLOITATION: Ch·ªçn action c√≥ Q-value cao nh·∫•t
        return GetBestAction(state, possibleMoves);
    }
}
```

2.7 EPSILON DECAY:
Epsilon gi·∫£m d·∫ßn t·ª´ 0.995 ‚Üí 0.1 qua training:
- Ban ƒë·∫ßu: High exploration (99.5%)
- Cu·ªëi training: High exploitation (90%)

2.8 REWARD SYSTEM:
- Win game: +1000
- Lose game: -1000
- Move toward goal: +10
- Block opponent: +50
- Waste move: -5
- Strategic position: +15

================================================================================
                3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
================================================================================

3.1 GI·ªöI THI·ªÜU MINIMAX:
Minimax l√† thu·∫≠t to√°n game theory c·ªï ƒëi·ªÉn, t√¨m ki·∫øm move t·ªëi ∆∞u b·∫±ng c√°ch
d·ª± ƒëo√°n tr∆∞·ªõc nhi·ªÅu moves. Alpha-Beta pruning t·ªëi ∆∞u hi·ªáu su·∫•t b·∫±ng c√°ch
lo·∫°i b·ªè c√°c nh√°nh kh√¥ng c·∫ßn thi·∫øt.

3.2 NGUY√äN L√ù HO·∫†T ƒê·ªòNG:
- Maximizing Player (AI): T·ªëi ƒëa h√≥a score
- Minimizing Player (Human): T·ªëi thi·ªÉu h√≥a score AI
- Search depth: 4 levels (configurable)
- Pruning: Lo·∫°i b·ªè nh√°nh kh√¥ng promising

3.3 IMPLEMENTATION:

```csharp
float Minimax(GameState state, int depth, bool isMaximizing, float alpha, float beta)
{
    // Base case: Depth = 0 ho·∫∑c game over
    if (depth == 0 || IsGameOver(state))
        return EvaluatePosition(state);
    
    if (isMaximizing) // AI turn
    {
        float maxEval = float.NegativeInfinity;
        foreach (var move in GeneratePossibleMoves(state, true))
        {
            GameState newState = ApplyMove(state, move, true);
            float eval = Minimax(newState, depth - 1, false, alpha, beta);
            maxEval = Mathf.Max(maxEval, eval);
            alpha = Mathf.Max(alpha, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return maxEval;
    }
    else // Human turn
    {
        float minEval = float.PositiveInfinity;
        foreach (var move in GeneratePossibleMoves(state, false))
        {
            GameState newState = ApplyMove(state, move, false);
            float eval = Minimax(newState, depth - 1, true, alpha, beta);
            minEval = Mathf.Min(minEval, eval);
            beta = Mathf.Min(beta, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return minEval;
    }
}
```

3.4 EVALUATION FUNCTION:
Position evaluation d·ª±a tr√™n multiple factors:

```csharp
float EvaluatePosition(GameState state)
{
    float score = 0f;
    
    // Distance to goal (c√†ng g·∫ßn c√†ng t·ªët)
    int aiDistance = GetShortestPath(state.aiPosition, goalRow);
    int humanDistance = GetShortestPath(state.humanPosition, goalRow);
    score += (humanDistance - aiDistance) * 100;
    
    // Wall advantage
    score += (state.aiWallsLeft - state.humanWallsLeft) * 50;
    
    // Mobility (s·ªë moves c√≥ th·ªÉ)
    int aiMobility = GetValidMoves(state.aiPosition).Count;
    int humanMobility = GetValidMoves(state.humanPosition).Count;
    score += (aiMobility - humanMobility) * 20;
    
    // Center control
    score += GetCenterControlScore(state) * 30;
    
    return score;
}
```

3.5 ALPHA-BETA PRUNING HI·ªÜU QU·∫¢:
- Reduces search space t·ª´ O(b^d) xu·ªëng O(b^(d/2))
- Best case: 50% reduction in nodes evaluated
- Ordering moves by promising gi√∫p pruning hi·ªáu qu·∫£ h∆°n

================================================================================
                        4. SPECIALIZED AI STYLES SYSTEM
================================================================================

4.1 CONCEPT:
6 AI personalities v·ªõi training v√† behavior ri√™ng bi·ªát, m·ªói style c√≥:
- Ri√™ng Q-table file
- Custom reward structure
- Specific move prioritization
- Unique decision making logic

4.2 AI STYLES CHI TI·∫æT:

üî• AGGRESSIVE STYLE:
- M·ª•c ti√™u: T·∫•n c√¥ng v√† ch·∫∑n ƒë·ªëi th·ªß
- Wall Usage: 70% chance
- Reward Structure:
  * blockingOpponentReward = 50 * intensity
  * wallPlacementReward = 30 * intensity
  * defensiveWallPenalty = -20 * intensity
- Q-table: qtable_aggressive.json

üõ°Ô∏è DEFENSIVE STYLE:
- M·ª•c ti√™u: Ph√≤ng th·ªß v√† di chuy·ªÉn an to√†n
- Wall Usage: 50% chance
- Reward Structure:
  * defensiveWallReward = 35 * intensity
  * movementReward = 20 * intensity
  * aggressiveMovePenalty = -15 * intensity
- Q-table: qtable_defensive.json

üß± WALL MASTER STYLE:
- M·ª•c ti√™u: Chuy√™n gia ƒë·∫∑t t∆∞·ªùng v√† t·∫°o m√™ cung
- Wall Usage: 90% chance
- Reward Structure:
  * wallPlacementReward = 60 * intensity
  * strategicWallReward = 40 * intensity
  * wallComboReward = 50 * intensity
  * movementWithoutWallPenalty = -25 * intensity
- Q-table: qtable_wallmaster.json

‚ö° SPEED RUNNER STYLE:
- M·ª•c ti√™u: ƒê·∫°t ƒë√≠ch nhanh nh·∫•t
- Wall Usage: 10% chance
- Reward Structure:
  * movementReward = 40 * intensity
  * progressToGoalReward = 50 * intensity
  * quickWinBonus = 100 * intensity
  * wallPlacementPenalty = -20 * intensity
- Q-table: qtable_speedrunner.json

üß† TACTICAL STYLE:
- M·ª•c ti√™u: L·ª£i th·∫ø v·ªã tr√≠ v√† k·∫ø ho·∫°ch d√†i h·∫°n
- Wall Usage: Variable (strategic)
- Reward Structure:
  * tacticalMoveReward = 45 * intensity
  * positionAdvantageReward = 35 * intensity
  * mobilityReward = 25 * intensity
  * patternRecognitionBonus = 40 * intensity
- Q-table: qtable_tactical.json

‚öñÔ∏è BALANCED STYLE:
- M·ª•c ti√™u: C√¢n b·∫±ng t·∫•t c·∫£ aspects
- Wall Usage: 30% chance
- Reward Structure: Default balanced values
- Q-table: qtable.json

4.3 STYLE SWITCHING MECHANISM:

```csharp
public void SwitchToStyle(PlayStyle newStyle)
{
    // Save current Q-table
    if (allowQTableSaving) qAgent.SaveQTable(GetStyleSpecificQTablePath());
    
    // Load new style Q-table
    specializedStyle = newStyle;
    qAgent.LoadQTable(GetStyleSpecificQTablePath());
    
    // Reconfigure parameters
    ConfigureStyleSpecificTraining();
}
```

4.4 STYLE INTENSITY:
Parameter t·ª´ 0.0 ‚Üí 1.0 control m·ª©c ƒë·ªô chuy√™n h√≥a:
- 0.0: Balanced behavior
- 0.5: Moderate specialization
- 1.0: Extreme specialization

================================================================================
                        5. DECISION TREE CLASSIFIER
================================================================================

5.1 PURPOSE:
Decision Tree ph√¢n t√≠ch game context ƒë·ªÉ ch·ªçn strategy ph√π h·ª£p, d·ª±a tr√™n:
- Game phase (Early/Mid/End)
- Position advantage
- Resource advantage
- Mobility situation

5.2 FEATURE EXTRACTION:

```csharp
public struct DecisionFeatures
{
    public int aiDistanceToGoal;      // AI distance to goal line
    public int humanDistanceToGoal;   // Human distance to goal line
    public int aiWallsLeft;           // AI walls remaining
    public int humanWallsLeft;        // Human walls remaining
    public int aiMobility;            // AI possible moves
    public int humanMobility;         // Human possible moves
    public float distanceAdvantage;   // Human - AI distance
    public float wallAdvantage;       // AI - Human walls
    public bool isEarlyGame;          // > 15 moves estimated remaining
    public bool isMidGame;            // 5-15 moves remaining
    public bool isEndGame;            // < 5 moves remaining
}
```

5.3 DECISION LOGIC:

```csharp
public AIStrategy Predict(DecisionFeatures features)
{
    // Early Game: Play conservatively
    if (features.isEarlyGame)
        return AIStrategy.Balanced;
    
    // AI has significant lead: Focus on blocking
    if (features.distanceAdvantage > 2)
        return AIStrategy.Blocking;
    
    // AI is behind: Be aggressive
    if (features.distanceAdvantage < -1)
        return AIStrategy.Aggressive;
    
    // End game with wall advantage: Play defensively
    if (features.isEndGame && features.wallAdvantage > 2)
        return AIStrategy.Defensive;
    
    // Close game: Balanced approach
    return AIStrategy.Balanced;
}
```

5.4 STRATEGY OUTCOMES:
- Balanced: Equal weight to movement v√† walls
- Blocking: Prioritize moves that increase opponent path
- Aggressive: Focus on attacking positions
- Defensive: Protect own position and path

================================================================================
                        6. ENTROPY ANALYSIS & SOFTMAX
================================================================================

6.1 ENTROPY CONCEPT:
Entropy ƒëo ƒë·ªô ƒëa d·∫°ng/uncertainty c·ªßa move scores. High entropy = nhi·ªÅu moves
c√≥ scores t∆∞∆°ng ƒë∆∞∆°ng, low entropy = m·ªôt move r√µ r√†ng t·ªët nh·∫•t.

6.2 ENTROPY CALCULATION:

```csharp
float CalculateEntropy(List<float> moveScores)
{
    // Normalize th√†nh probabilities
    float total = moveScores.Sum();
    var probabilities = moveScores.Select(s => s / total);
    
    // Calculate Shannon entropy
    float entropy = 0f;
    foreach (float p in probabilities)
    {
        if (p > 0f)
            entropy -= p * Mathf.Log(p, 2f);
    }
    
    return entropy; // Range: 0 (deterministic) to log2(n) (uniform)
}
```

6.3 SOFTMAX DISTRIBUTION:

```csharp
float[] CalculateSoftmax(List<float> scores, float temperature)
{
    float[] probabilities = new float[scores.Count];
    float sum = scores.Sum(s => Mathf.Exp(s / temperature));
    
    for (int i = 0; i < scores.Count; i++)
    {
        probabilities[i] = Mathf.Exp(scores[i] / temperature) / sum;
    }
    
    return probabilities;
}
```

6.4 TEMPERATURE PARAMETER:
- Low temperature (0.1): Sharp distribution, focus on best moves
- High temperature (2.0): Flat distribution, more exploration
- Default temperature (1.0): Balanced exploitation/exploration

6.5 ADAPTIVE MOVE SELECTION:

```csharp
public AIMove ChooseActionWithEntropy(GameState state, List<AIMove> possibleMoves)
{
    var moveScores = EvaluateAllMoves(state, possibleMoves);
    float entropy = CalculateEntropy(moveScores);
    
    if (entropy > entropyThreshold) // High diversity
    {
        // Use softmax for probabilistic selection
        var probabilities = CalculateSoftmax(moveScores, softmaxTemperature);
        return SelectMoveByProbability(possibleMoves, probabilities);
    }
    else // Clear best move
    {
        // Use deterministic best move
        return GetBestScoredMove(possibleMoves, moveScores);
    }
}
```

================================================================================
                        7. TRAINING SYSTEM & SELF-PLAY
================================================================================

7.1 SELF-PLAY CONCEPT:
AI h·ªçc b·∫±ng c√°ch ch∆°i v·ªõi ch√≠nh n√≥ h√†ng ng√†n l·∫ßn, gradually improve performance
th√¥ng qua trial v√† error.

7.2 TRAINING PIPELINE:

```csharp
IEnumerator RunSelfPlayTraining()
{
    for (int episode = 1; episode <= maxEpisodes; episode++)
    {
        // Reset game state
        ResetGameForTraining();
        
        while (!IsGameOver())
        {
            // Get current state
            GameState currentState = GetCurrentGameState();
            List<AIMove> possibleMoves = GetPossibleMoves();
            
            // Choose action using Œµ-greedy
            AIMove chosenMove = qLearningAgent.ChooseAction(currentState, possibleMoves);
            
            // Execute move
            ExecuteMove(chosenMove);
            
            // Calculate reward
            float reward = CalculateReward(currentState, chosenMove);
            
            // Update Q-table
            GameState newState = GetCurrentGameState();
            qLearningAgent.UpdateQValue(
                EncodeState(currentState),
                EncodeAction(chosenMove),
                reward,
                EncodeState(newState)
            );
            
            yield return new WaitForEndOfFrame();
        }
        
        // Epsilon decay
        qLearningAgent.DecayEpsilon();
        
        // Save progress periodically
        if (episode % 100 == 0)
        {
            qLearningAgent.SaveQTable();
        }
    }
}
```

7.3 REWARD ENGINEERING:
Critical cho Q-Learning success:

```csharp
float CalculateReward(GameState state, AIMove move)
{
    float reward = 0f;
    
    // Game outcome rewards
    if (IsWin(state)) reward += 1000f;
    if (IsLoss(state)) reward -= 1000f;
    
    // Progress rewards
    if (move.moveType == MoveType.Movement)
    {
        if (IsMoveTowardGoal(move)) reward += 20f;
        else reward -= 5f;
    }
    
    // Strategic rewards
    if (move.moveType == MoveType.WallPlacement)
    {
        if (IsWallBlockingOpponent(move)) reward += 50f;
        if (IsWallStrategic(move)) reward += 30f;
        if (IsWallWasted(move)) reward -= 25f;
    }
    
    // Style-specific rewards
    reward += GetStyleSpecificReward(state, move);
    
    return reward;
}
```

7.4 TRAINING METRICS:
- Episodes: 1000 standard
- Duration: ~15 minutes
- Convergence: Usually around episode 800
- Q-table size: 13,118+ states at completion
- Final epsilon: 0.1 (90% exploitation)

================================================================================
                        8. AI TESTING FRAMEWORK
================================================================================

8.1 TESTING PHILOSOPHY:
Comprehensive testing ensures AI quality v√† reliability across different
game scenarios v√† edge cases.

8.2 TEST CATEGORIES:

A) DECISION TREE TESTS:
```csharp
void TestDecisionTreeStrategies()
{
    // Test Early Game
    var earlyGameFeatures = new DecisionFeatures
    {
        aiDistanceToGoal = 8, humanDistanceToGoal = 8,
        isEarlyGame = true
    };
    var strategy = decisionTree.Predict(earlyGameFeatures);
    Assert.AreEqual(AIStrategy.Balanced, strategy);
    
    // Test AI Leading
    var leadingFeatures = new DecisionFeatures
    {
        distanceAdvantage = 3, // AI leading
        isMidGame = true
    };
    strategy = decisionTree.Predict(leadingFeatures);
    Assert.AreEqual(AIStrategy.Blocking, strategy);
}
```

B) ENTROPY ANALYSIS TESTS:
```csharp
void TestEntropyAnalysis()
{
    // High entropy case (diverse scores)
    var highEntropyScores = new List<float> { 100f, 98f, 97f, 95f, 94f };
    float entropy = CalculateTestEntropy(highEntropyScores);
    Assert.IsTrue(entropy > 0.5f);
    
    // Low entropy case (clear winner)
    var lowEntropyScores = new List<float> { 100f, 50f, 50f, 50f, 50f };
    entropy = CalculateTestEntropy(lowEntropyScores);
    Assert.IsTrue(entropy < 0.5f);
}
```

C) SOFTMAX DISTRIBUTION TESTS:
```csharp
void TestSoftmaxDistribution()
{
    var testScores = new List<float> { 100f, 98f, 95f, 90f };
    float temperature = 1.0f;
    
    var probabilities = CalculateSoftmax(testScores, temperature);
    
    // Verify probabilities sum to 1
    float sum = probabilities.Sum();
    Assert.AreEqual(1.0f, sum, 0.001f);
    
    // Verify highest score gets highest probability
    int maxIndex = Array.IndexOf(probabilities, probabilities.Max());
    Assert.AreEqual(0, maxIndex);
}
```

D) PERFORMANCE BENCHMARKS:
- Move generation time: <100ms
- Q-Learning decision: <50ms
- Minimax decision (depth 4): <200ms
- Memory usage: <256MB

8.3 AUTOMATED TESTING:
Tests run automatically ƒë·ªÉ ensure code quality:
- Unit tests cho individual algorithms
- Integration tests cho AI system interactions
- Performance tests cho response times
- Regression tests ƒë·ªÉ prevent quality degradation

================================================================================
                        9. HYBRID AI ARCHITECTURE
================================================================================

9.1 CONCEPT:
Hybrid system k·∫øt h·ª£p strengths c·ªßa multiple algorithms:
- Q-Learning: Data-driven, learns from experience
- Minimax: Logical, guaranteed optimal given evaluation
- Decision Tree: Fast, context-aware switching
- Entropy: Adaptive, handles uncertainty

9.2 ALGORITHM SELECTION LOGIC:

```csharp
public AIMove GetBestMove()
{
    UpdateGameState();
    List<AIMove> possibleMoves = GeneratePossibleMoves();
    
    // Primary: Use Q-Learning if trained model available
    if (useQLearning && qAgent.IsTrainedModel())
    {
        Debug.Log($"üß† Using {specializedStyle} Q-Learning");
        return qAgent.ChooseAction(currentGameState, possibleMoves);
    }
    
    // Fallback: Use Minimax for guaranteed reasonable play
    else
    {
        Debug.Log($"üîç Using {specializedStyle} Minimax");
        return FindBestMoveWithMinimax();
    }
}
```

9.3 SEAMLESS SWITCHING:
- No interruption in gameplay
- Maintains consistent API
- Preserves game state integrity
- Logs algorithm used for debugging

9.4 BENEFITS:
- Robustness: Fallback ensures AI never fails
- Performance: Q-Learning for trained scenarios, Minimax for edge cases
- Flexibility: Can switch algorithms based on situation
- Development: Easy to test individual components

================================================================================
                        10. PERFORMANCE OPTIMIZATION
================================================================================

10.1 Q-TABLE OPTIMIZATION:
- Sparse representation: Only store visited states
- Compression: String interning for state keys
- Lazy loading: Load Q-table on demand
- Periodic saving: Every 100 training episodes

10.2 MINIMAX OPTIMIZATION:
- Alpha-Beta pruning: 50% search reduction
- Move ordering: Promising moves first
- Iterative deepening: Gradual depth increase
- Transposition tables: Cache evaluated positions

10.3 MEMORY MANAGEMENT:
- Object pooling cho GameState objects
- String caching cho repeated state/action encodings
- Garbage collection optimization
- Memory profiling v√† monitoring

10.4 MULTITHREADING CONSIDERATIONS:
- Q-Learning training: Background thread
- Move calculation: Main thread (Unity requirement)
- File I/O: Async operations where possible
- Thread-safe collections cho shared data

10.5 CACHING STRATEGIES:
- Path finding results
- Move generation results
- Evaluation function results
- Feature extraction results

================================================================================
                            K·∫æT LU·∫¨N
================================================================================

D·ª± √°n AI Quoridor Project th·ªÉ hi·ªán m·ªôt implementation comprehensive c·ªßa modern
game AI techniques:

TECHNICAL INNOVATIONS:
‚úÖ Hybrid Q-Learning + Minimax architecture
‚úÖ 6 specialized AI personalities v·ªõi separate training
‚úÖ Advanced decision making v·ªõi entropy analysis
‚úÖ Comprehensive testing v√† validation framework
‚úÖ Professional development tools v√† debugging

EDUCATIONAL VALUE:
‚úÖ Practical machine learning implementation
‚úÖ Traditional game AI algorithms
‚úÖ Modern software engineering practices
‚úÖ Performance optimization techniques
‚úÖ Automated testing methodologies

FUTURE ENHANCEMENTS:
üöÄ Deep Q-Learning (DQN) integration
üöÄ Multi-agent reinforcement learning
üöÄ Neural network-based evaluation functions
üöÄ Online learning v√† adaptation
üöÄ Transfer learning between game variants

D·ª± √°n n√†y demonstrating successful integration c·ªßa classical AI methods v·ªõi
modern machine learning, t·∫°o ra m·ªôt intelligent, adaptive, v√† highly
interactive gaming experience.

================================================================================
                            APPENDIX
================================================================================

FILES LI√äN QUAN:
- Assets/scripts/QLearningAgent.cs - Q-Learning implementation
- Assets/scripts/QuoridorAI.cs - Main AI controller
- Assets/scripts/DecisionTreeClassifier.cs - Decision tree logic
- Assets/scripts/QuoridorSelfPlayTrainer.cs - Training system
- Assets/scripts/AITestScript.cs - Testing framework
- Assets/scripts/Editor/ForceRetrainAI.cs - Development tools

Q-TABLE FILES:
- StreamingAssets/qtable.json - Balanced style
- StreamingAssets/qtable_aggressive.json - Aggressive style
- StreamingAssets/qtable_defensive.json - Defensive style
- StreamingAssets/qtable_wallmaster.json - Wall Master style
- StreamingAssets/qtable_speedrunner.json - Speed Runner style
- StreamingAssets/qtable_tactical.json - Tactical style

DOCUMENTATION:
- AI_ALGORITHMS_DETAILED_DESCRIPTION.txt - This file
- README.md - Project overview
- API_INTEGRATION_GUIDE.md - Voice/Chat integration
- TRAINING_GUIDE.md - AI training instructions

================================================================================
================================================================================
                    AI QUORIDOR GAME PROJECT - THU·∫¨T TO√ÅN AI CHI TI·∫æT
================================================================================

D·ª± √°n: AI Quoridor Game v·ªõi Machine Learning v√† Multi-Platform Integration
Ng√†y t·∫°o: 25/07/2025
T√°c gi·∫£: [Student Name]
Platform: Unity 3D
Language: C#

================================================================================
                            M·ª§C L·ª§C
================================================================================

1. T·ªîNG QUAN KI·∫æN TR√öC AI
2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
4. SPECIALIZED AI STYLES SYSTEM
5. DECISION TREE CLASSIFIER
6. ENTROPY ANALYSIS & SOFTMAX
7. TRAINING SYSTEM & SELF-PLAY
8. AI TESTING FRAMEWORK
9. HYBRID AI ARCHITECTURE
10. PERFORMANCE OPTIMIZATION

================================================================================
                        1. T·ªîNG QUAN KI·∫æN TR√öC AI
================================================================================

D·ª± √°n AI Quoridor s·ª≠ d·ª•ng m·ªôt ki·∫øn tr√∫c AI hybrid ti√™n ti·∫øn, k·∫øt h·ª£p:

1.1 CORE ALGORITHMS:
- Q-Learning (Reinforcement Learning) - Thu·∫≠t to√°n ch√≠nh
- Minimax + Alpha-Beta Pruning - Thu·∫≠t to√°n fallback
- Decision Tree Classifier - Ph√¢n t√≠ch ng·ªØ c·∫£nh game
- Entropy Analysis - ƒê√°nh gi√° ƒëa d·∫°ng move

1.2 SPECIALIZED SYSTEMS:
- 6 AI Personality Styles v·ªõi training ri√™ng bi·ªát
- Self-Play Training Pipeline t·ª± ƒë·ªông
- Real-time Performance Testing Framework
- Dynamic Algorithm Switching

1.3 TECHNICAL FOUNDATION:
- State Space: 13,118+ unique game states
- Action Space: Movement + Wall placement combinations
- Training Method: Self-play v·ªõi 1000 episodes
- Learning Rate: Œ± = 0.1, Discount Factor: Œ≥ = 0.99

================================================================================
                    2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
================================================================================

2.1 GI·ªöI THI·ªÜU Q-LEARNING:
Q-Learning l√† thu·∫≠t to√°n Reinforcement Learning kh√¥ng c·∫ßn model, h·ªçc th√¥ng qua
t∆∞∆°ng t√°c v·ªõi m√¥i tr∆∞·ªùng ƒë·ªÉ t·ªëi ∆∞u h√≥a policy. Trong Quoridor, AI h·ªçc c√°ch
ch·ªçn moves t·ªëi ∆∞u d·ª±a tr√™n kinh nghi·ªám t·ª´ h√†ng ng√†n game.

2.2 C√îNG TH·ª®C TO√ÅN H·ªåC:
Q(s,a) = Q(s,a) + Œ±[r + Œ≥*max(Q(s',a')) - Q(s,a)]

Trong ƒë√≥:
- Q(s,a): Q-value c·ªßa state s v·ªõi action a
- Œ±: Learning rate (0.1) - t·ªëc ƒë·ªô h·ªçc
- r: Immediate reward t·ª´ action
- Œ≥: Discount factor (0.99) - t·∫ßm quan tr·ªçng future rewards
- s': Next state sau khi th·ª±c hi·ªán action
- max(Q(s',a')): Maximum Q-value c·ªßa next state

2.3 IMPLEMENTATION CHI TI·∫æT:

```csharp
public class QLearningAgent
{
    // Core Q-Table: Dictionary l∆∞u tr·ªØ Q-values
    private Dictionary<string, Dictionary<string, float>> qTable;
    
    // Hyperparameters
    private float learningRate = 0.1f;      // T·ªëc ƒë·ªô h·ªçc
    private float discountFactor = 0.99f;   // Discount future rewards
    private float epsilon = 0.995f;         // Exploration rate
    
    // Q-Learning Update Implementation
    public void UpdateQValue(string state, string action, float reward, string nextState)
    {
        // L·∫•y Q-value hi·ªán t·∫°i
        float currentQ = GetQValue(state, action);
        
        // T√¨m max Q-value c·ªßa next state
        float maxNextQ = GetMaxQValue(nextState);
        
        // √Åp d·ª•ng c√¥ng th·ª©c Q-Learning
        float newQ = currentQ + learningRate * (reward + discountFactor * maxNextQ - currentQ);
        
        // C·∫≠p nh·∫≠t Q-table
        SetQValue(state, action, newQ);
    }
}
```

2.4 STATE ENCODING:
Game state ƒë∆∞·ª£c encode th√†nh string duy nh·∫•t:
"aiPos_x,aiPos_y-humanPos_x,humanPos_y-aiWalls-humanWalls-wallsList"

V√≠ d·ª•: "4,8-4,0-9-10-7,4,False|5,6,True"
- AI ·ªü (4,8), Human ·ªü (4,0)
- AI c√≥ 9 walls, Human c√≥ 10 walls
- Walls ƒë√£ ƒë·∫∑t: (7,4,False), (5,6,True)

2.5 ACTION ENCODING:
Actions ƒë∆∞·ª£c encode th√†nh string:
- Movement: "M:x,y" (v√≠ d·ª•: "M:4,7")
- Wall: "W:x,y,horizontal" (v√≠ d·ª•: "W:3,5,True")

2.6 EPSILON-GREEDY STRATEGY:
```csharp
public AIMove ChooseAction(GameState state, List<AIMove> possibleMoves)
{
    // Exploration vs Exploitation trade-off
    if (Random.Range(0f, 1f) < epsilon)
    {
        // EXPLORATION: Ch·ªçn random ƒë·ªÉ kh√°m ph√°
        return possibleMoves[Random.Range(0, possibleMoves.Count)];
    }
    else
    {
        // EXPLOITATION: Ch·ªçn action c√≥ Q-value cao nh·∫•t
        return GetBestAction(state, possibleMoves);
    }
}
```

2.7 EPSILON DECAY:
Epsilon gi·∫£m d·∫ßn t·ª´ 0.995 ‚Üí 0.1 qua training:
- Ban ƒë·∫ßu: High exploration (99.5%)
- Cu·ªëi training: High exploitation (90%)

2.8 REWARD SYSTEM:
- Win game: +1000
- Lose game: -1000
- Move toward goal: +10
- Block opponent: +50
- Waste move: -5
- Strategic position: +15

================================================================================
                3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
================================================================================

3.1 GI·ªöI THI·ªÜU MINIMAX:
Minimax l√† thu·∫≠t to√°n game theory c·ªï ƒëi·ªÉn, t√¨m ki·∫øm move t·ªëi ∆∞u b·∫±ng c√°ch
d·ª± ƒëo√°n tr∆∞·ªõc nhi·ªÅu moves. Alpha-Beta pruning t·ªëi ∆∞u hi·ªáu su·∫•t b·∫±ng c√°ch
lo·∫°i b·ªè c√°c nh√°nh kh√¥ng c·∫ßn thi·∫øt.

3.2 NGUY√äN L√ù HO·∫†T ƒê·ªòNG:
- Maximizing Player (AI): T·ªëi ƒëa h√≥a score
- Minimizing Player (Human): T·ªëi thi·ªÉu h√≥a score AI
- Search depth: 4 levels (configurable)
- Pruning: Lo·∫°i b·ªè nh√°nh kh√¥ng promising

3.3 IMPLEMENTATION:

```csharp
float Minimax(GameState state, int depth, bool isMaximizing, float alpha, float beta)
{
    // Base case: Depth = 0 ho·∫∑c game over
    if (depth == 0 || IsGameOver(state))
        return EvaluatePosition(state);
    
    if (isMaximizing) // AI turn
    {
        float maxEval = float.NegativeInfinity;
        foreach (var move in GeneratePossibleMoves(state, true))
        {
            GameState newState = ApplyMove(state, move, true);
            float eval = Minimax(newState, depth - 1, false, alpha, beta);
            maxEval = Mathf.Max(maxEval, eval);
            alpha = Mathf.Max(alpha, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return maxEval;
    }
    else // Human turn
    {
        float minEval = float.PositiveInfinity;
        foreach (var move in GeneratePossibleMoves(state, false))
        {
            GameState newState = ApplyMove(state, move, false);
            float eval = Minimax(newState, depth - 1, true, alpha, beta);
            minEval = Mathf.Min(minEval, eval);
            beta = Mathf.Min(beta, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return minEval;
    }
}
```

3.4 EVALUATION FUNCTION:
Position evaluation d·ª±a tr√™n multiple factors:

```csharp
float EvaluatePosition(GameState state)
{
    float score = 0f;
    
    // Distance to goal (c√†ng g·∫ßn c√†ng t·ªët)
    int aiDistance = GetShortestPath(state.aiPosition, goalRow);
    int humanDistance = GetShortestPath(state.humanPosition, goalRow);
    score += (humanDistance - aiDistance) * 100;
    
    // Wall advantage
    score += (state.aiWallsLeft - state.humanWallsLeft) * 50;
    
    // Mobility (s·ªë moves c√≥ th·ªÉ)
    int aiMobility = GetValidMoves(state.aiPosition).Count;
    int humanMobility = GetValidMoves(state.humanPosition).Count;
    score += (aiMobility - humanMobility) * 20;
    
    // Center control
    score += GetCenterControlScore(state) * 30;
    
    return score;
}
```

3.5 ALPHA-BETA PRUNING HI·ªÜU QU·∫¢:
- Reduces search space t·ª´ O(b^d) xu·ªëng O(b^(d/2))
- Best case: 50% reduction in nodes evaluated
- Ordering moves by promising gi√∫p pruning hi·ªáu qu·∫£ h∆°n

================================================================================
                        4. SPECIALIZED AI STYLES SYSTEM
================================================================================

4.1 CONCEPT:
6 AI personalities v·ªõi training v√† behavior ri√™ng bi·ªát, m·ªói style c√≥:
- Ri√™ng Q-table file
- Custom reward structure
- Specific move prioritization
- Unique decision making logic

4.2 AI STYLES CHI TI·∫æT:

üî• AGGRESSIVE STYLE:
- M·ª•c ti√™u: T·∫•n c√¥ng v√† ch·∫∑n ƒë·ªëi th·ªß
- Wall Usage: 70% chance
- Reward Structure:
  * blockingOpponentReward = 50 * intensity
  * wallPlacementReward = 30 * intensity
  * defensiveWallPenalty = -20 * intensity
- Q-table: qtable_aggressive.json

üõ°Ô∏è DEFENSIVE STYLE:
- M·ª•c ti√™u: Ph√≤ng th·ªß v√† di chuy·ªÉn an to√†n
- Wall Usage: 50% chance
- Reward Structure:
  * defensiveWallReward = 35 * intensity
  * movementReward = 20 * intensity
  * aggressiveMovePenalty = -15 * intensity
- Q-table: qtable_defensive.json

üß± WALL MASTER STYLE:
- M·ª•c ti√™u: Chuy√™n gia ƒë·∫∑t t∆∞·ªùng v√† t·∫°o m√™ cung
- Wall Usage: 90% chance
- Reward Structure:
  * wallPlacementReward = 60 * intensity
  * strategicWallReward = 40 * intensity
  * wallComboReward = 50 * intensity
  * movementWithoutWallPenalty = -25 * intensity
- Q-table: qtable_wallmaster.json

‚ö° SPEED RUNNER STYLE:
- M·ª•c ti√™u: ƒê·∫°t ƒë√≠ch nhanh nh·∫•t
- Wall Usage: 10% chance
- Reward Structure:
  * movementReward = 40 * intensity
  * progressToGoalReward = 50 * intensity
  * quickWinBonus = 100 * intensity
  * wallPlacementPenalty = -20 * intensity
- Q-table: qtable_speedrunner.json

üß† TACTICAL STYLE:
- M·ª•c ti√™u: L·ª£i th·∫ø v·ªã tr√≠ v√† k·∫ø ho·∫°ch d√†i h·∫°n
- Wall Usage: Variable (strategic)
- Reward Structure:
  * tacticalMoveReward = 45 * intensity
  * positionAdvantageReward = 35 * intensity
  * mobilityReward = 25 * intensity
  * patternRecognitionBonus = 40 * intensity
- Q-table: qtable_tactical.json

‚öñÔ∏è BALANCED STYLE:
- M·ª•c ti√™u: C√¢n b·∫±ng t·∫•t c·∫£ aspects
- Wall Usage: 30% chance
- Reward Structure: Default balanced values
- Q-table: qtable.json

4.3 STYLE SWITCHING MECHANISM:

```csharp
public void SwitchToStyle(PlayStyle newStyle)
{
    // Save current Q-table
    if (allowQTableSaving) qAgent.SaveQTable(GetStyleSpecificQTablePath());
    
    // Load new style Q-table
    specializedStyle = newStyle;
    qAgent.LoadQTable(GetStyleSpecificQTablePath());
    
    // Reconfigure parameters
    ConfigureStyleSpecificTraining();
}
```

4.4 STYLE INTENSITY:
Parameter t·ª´ 0.0 ‚Üí 1.0 control m·ª©c ƒë·ªô chuy√™n h√≥a:
- 0.0: Balanced behavior
- 0.5: Moderate specialization
- 1.0: Extreme specialization

================================================================================
                        5. DECISION TREE CLASSIFIER
================================================================================

5.1 PURPOSE:
Decision Tree ph√¢n t√≠ch game context ƒë·ªÉ ch·ªçn strategy ph√π h·ª£p, d·ª±a tr√™n:
- Game phase (Early/Mid/End)
- Position advantage
- Resource advantage
- Mobility situation

5.2 FEATURE EXTRACTION:

```csharp
public struct DecisionFeatures
{
    public int aiDistanceToGoal;      // AI distance to goal line
    public int humanDistanceToGoal;   // Human distance to goal line
    public int aiWallsLeft;           // AI walls remaining
    public int humanWallsLeft;        // Human walls remaining
    public int aiMobility;            // AI possible moves
    public int humanMobility;         // Human possible moves
    public float distanceAdvantage;   // Human - AI distance
    public float wallAdvantage;       // AI - Human walls
    public bool isEarlyGame;          // > 15 moves estimated remaining
    public bool isMidGame;            // 5-15 moves remaining
    public bool isEndGame;            // < 5 moves remaining
}
```

5.3 DECISION LOGIC:

```csharp
public AIStrategy Predict(DecisionFeatures features)
{
    // Early Game: Play conservatively
    if (features.isEarlyGame)
        return AIStrategy.Balanced;
    
    // AI has significant lead: Focus on blocking
    if (features.distanceAdvantage > 2)
        return AIStrategy.Blocking;
    
    // AI is behind: Be aggressive
    if (features.distanceAdvantage < -1)
        return AIStrategy.Aggressive;
    
    // End game with wall advantage: Play defensively
    if (features.isEndGame && features.wallAdvantage > 2)
        return AIStrategy.Defensive;
    
    // Close game: Balanced approach
    return AIStrategy.Balanced;
}
```

5.4 STRATEGY OUTCOMES:
- Balanced: Equal weight to movement v√† walls
- Blocking: Prioritize moves that increase opponent path
- Aggressive: Focus on attacking positions
- Defensive: Protect own position and path

================================================================================
                        6. ENTROPY ANALYSIS & SOFTMAX
================================================================================

6.1 ENTROPY CONCEPT:
Entropy ƒëo ƒë·ªô ƒëa d·∫°ng/uncertainty c·ªßa move scores. High entropy = nhi·ªÅu moves
c√≥ scores t∆∞∆°ng ƒë∆∞∆°ng, low entropy = m·ªôt move r√µ r√†ng t·ªët nh·∫•t.

6.2 ENTROPY CALCULATION:

```csharp
float CalculateEntropy(List<float> moveScores)
{
    // Normalize th√†nh probabilities
    float total = moveScores.Sum();
    var probabilities = moveScores.Select(s => s / total);
    
    // Calculate Shannon entropy
    float entropy = 0f;
    foreach (float p in probabilities)
    {
        if (p > 0f)
            entropy -= p * Mathf.Log(p, 2f);
    }
    
    return entropy; // Range: 0 (deterministic) to log2(n) (uniform)
}
```

6.3 SOFTMAX DISTRIBUTION:

```csharp
float[] CalculateSoftmax(List<float> scores, float temperature)
{
    float[] probabilities = new float[scores.Count];
    float sum = scores.Sum(s => Mathf.Exp(s / temperature));
    
    for (int i = 0; i < scores.Count; i++)
    {
        probabilities[i] = Mathf.Exp(scores[i] / temperature) / sum;
    }
    
    return probabilities;
}
```

6.4 TEMPERATURE PARAMETER:
- Low temperature (0.1): Sharp distribution, focus on best moves
- High temperature (2.0): Flat distribution, more exploration
- Default temperature (1.0): Balanced exploitation/exploration

6.5 ADAPTIVE MOVE SELECTION:

```csharp
public AIMove ChooseActionWithEntropy(GameState state, List<AIMove> possibleMoves)
{
    var moveScores = EvaluateAllMoves(state, possibleMoves);
    float entropy = CalculateEntropy(moveScores);
    
    if (entropy > entropyThreshold) // High diversity
    {
        // Use softmax for probabilistic selection
        var probabilities = CalculateSoftmax(moveScores, softmaxTemperature);
        return SelectMoveByProbability(possibleMoves, probabilities);
    }
    else // Clear best move
    {
        // Use deterministic best move
        return GetBestScoredMove(possibleMoves, moveScores);
    }
}
```

================================================================================
                        7. TRAINING SYSTEM & SELF-PLAY
================================================================================

7.1 SELF-PLAY CONCEPT:
AI h·ªçc b·∫±ng c√°ch ch∆°i v·ªõi ch√≠nh n√≥ h√†ng ng√†n l·∫ßn, gradually improve performance
th√¥ng qua trial v√† error.

7.2 TRAINING PIPELINE:

```csharp
IEnumerator RunSelfPlayTraining()
{
    for (int episode = 1; episode <= maxEpisodes; episode++)
    {
        // Reset game state
        ResetGameForTraining();
        
        while (!IsGameOver())
        {
            // Get current state
            GameState currentState = GetCurrentGameState();
            List<AIMove> possibleMoves = GetPossibleMoves();
            
            // Choose action using Œµ-greedy
            AIMove chosenMove = qLearningAgent.ChooseAction(currentState, possibleMoves);
            
            // Execute move
            ExecuteMove(chosenMove);
            
            // Calculate reward
            float reward = CalculateReward(currentState, chosenMove);
            
            // Update Q-table
            GameState newState = GetCurrentGameState();
            qLearningAgent.UpdateQValue(
                EncodeState(currentState),
                EncodeAction(chosenMove),
                reward,
                EncodeState(newState)
            );
            
            yield return new WaitForEndOfFrame();
        }
        
        // Epsilon decay
        qLearningAgent.DecayEpsilon();
        
        // Save progress periodically
        if (episode % 100 == 0)
        {
            qLearningAgent.SaveQTable();
        }
    }
}
```

7.3 REWARD ENGINEERING:
Critical cho Q-Learning success:

```csharp
float CalculateReward(GameState state, AIMove move)
{
    float reward = 0f;
    
    // Game outcome rewards
    if (IsWin(state)) reward += 1000f;
    if (IsLoss(state)) reward -= 1000f;
    
    // Progress rewards
    if (move.moveType == MoveType.Movement)
    {
        if (IsMoveTowardGoal(move)) reward += 20f;
        else reward -= 5f;
    }
    
    // Strategic rewards
    if (move.moveType == MoveType.WallPlacement)
    {
        if (IsWallBlockingOpponent(move)) reward += 50f;
        if (IsWallStrategic(move)) reward += 30f;
        if (IsWallWasted(move)) reward -= 25f;
    }
    
    // Style-specific rewards
    reward += GetStyleSpecificReward(state, move);
    
    return reward;
}
```

7.4 TRAINING METRICS:
- Episodes: 1000 standard
- Duration: ~15 minutes
- Convergence: Usually around episode 800
- Q-table size: 13,118+ states at completion
- Final epsilon: 0.1 (90% exploitation)

================================================================================
                        8. AI TESTING FRAMEWORK
================================================================================

8.1 TESTING PHILOSOPHY:
Comprehensive testing ensures AI quality v√† reliability across different
game scenarios v√† edge cases.

8.2 TEST CATEGORIES:

A) DECISION TREE TESTS:
```csharp
void TestDecisionTreeStrategies()
{
    // Test Early Game
    var earlyGameFeatures = new DecisionFeatures
    {
        aiDistanceToGoal = 8, humanDistanceToGoal = 8,
        isEarlyGame = true
    };
    var strategy = decisionTree.Predict(earlyGameFeatures);
    Assert.AreEqual(AIStrategy.Balanced, strategy);
    
    // Test AI Leading
    var leadingFeatures = new DecisionFeatures
    {
        distanceAdvantage = 3, // AI leading
        isMidGame = true
    };
    strategy = decisionTree.Predict(leadingFeatures);
    Assert.AreEqual(AIStrategy.Blocking, strategy);
}
```

B) ENTROPY ANALYSIS TESTS:
```csharp
void TestEntropyAnalysis()
{
    // High entropy case (diverse scores)
    var highEntropyScores = new List<float> { 100f, 98f, 97f, 95f, 94f };
    float entropy = CalculateTestEntropy(highEntropyScores);
    Assert.IsTrue(entropy > 0.5f);
    
    // Low entropy case (clear winner)
    var lowEntropyScores = new List<float> { 100f, 50f, 50f, 50f, 50f };
    entropy = CalculateTestEntropy(lowEntropyScores);
    Assert.IsTrue(entropy < 0.5f);
}
```

C) SOFTMAX DISTRIBUTION TESTS:
```csharp
void TestSoftmaxDistribution()
{
    var testScores = new List<float> { 100f, 98f, 95f, 90f };
    float temperature = 1.0f;
    
    var probabilities = CalculateSoftmax(testScores, temperature);
    
    // Verify probabilities sum to 1
    float sum = probabilities.Sum();
    Assert.AreEqual(1.0f, sum, 0.001f);
    
    // Verify highest score gets highest probability
    int maxIndex = Array.IndexOf(probabilities, probabilities.Max());
    Assert.AreEqual(0, maxIndex);
}
```

D) PERFORMANCE BENCHMARKS:
- Move generation time: <100ms
- Q-Learning decision: <50ms
- Minimax decision (depth 4): <200ms
- Memory usage: <256MB

8.3 AUTOMATED TESTING:
Tests run automatically ƒë·ªÉ ensure code quality:
- Unit tests cho individual algorithms
- Integration tests cho AI system interactions
- Performance tests cho response times
- Regression tests ƒë·ªÉ prevent quality degradation

================================================================================
                        9. HYBRID AI ARCHITECTURE
================================================================================

9.1 CONCEPT:
Hybrid system k·∫øt h·ª£p strengths c·ªßa multiple algorithms:
- Q-Learning: Data-driven, learns from experience
- Minimax: Logical, guaranteed optimal given evaluation
- Decision Tree: Fast, context-aware switching
- Entropy: Adaptive, handles uncertainty

9.2 ALGORITHM SELECTION LOGIC:

```csharp
public AIMove GetBestMove()
{
    UpdateGameState();
    List<AIMove> possibleMoves = GeneratePossibleMoves();
    
    // Primary: Use Q-Learning if trained model available
    if (useQLearning && qAgent.IsTrainedModel())
    {
        Debug.Log($"üß† Using {specializedStyle} Q-Learning");
        return qAgent.ChooseAction(currentGameState, possibleMoves);
    }
    
    // Fallback: Use Minimax for guaranteed reasonable play
    else
    {
        Debug.Log($"üîç Using {specializedStyle} Minimax");
        return FindBestMoveWithMinimax();
    }
}
```

9.3 SEAMLESS SWITCHING:
- No interruption in gameplay
- Maintains consistent API
- Preserves game state integrity
- Logs algorithm used for debugging

9.4 BENEFITS:
- Robustness: Fallback ensures AI never fails
- Performance: Q-Learning for trained scenarios, Minimax for edge cases
- Flexibility: Can switch algorithms based on situation
- Development: Easy to test individual components

================================================================================
                        10. PERFORMANCE OPTIMIZATION
================================================================================

10.1 Q-TABLE OPTIMIZATION:
- Sparse representation: Only store visited states
- Compression: String interning for state keys
- Lazy loading: Load Q-table on demand
- Periodic saving: Every 100 training episodes

10.2 MINIMAX OPTIMIZATION:
- Alpha-Beta pruning: 50% search reduction
- Move ordering: Promising moves first
- Iterative deepening: Gradual depth increase
- Transposition tables: Cache evaluated positions

10.3 MEMORY MANAGEMENT:
- Object pooling cho GameState objects
- String caching cho repeated state/action encodings
- Garbage collection optimization
- Memory profiling v√† monitoring

10.4 MULTITHREADING CONSIDERATIONS:
- Q-Learning training: Background thread
- Move calculation: Main thread (Unity requirement)
- File I/O: Async operations where possible
- Thread-safe collections cho shared data

10.5 CACHING STRATEGIES:
- Path finding results
- Move generation results
- Evaluation function results
- Feature extraction results

================================================================================
                            K·∫æT LU·∫¨N
================================================================================

D·ª± √°n AI Quoridor Project th·ªÉ hi·ªán m·ªôt implementation comprehensive c·ªßa modern
game AI techniques:

TECHNICAL INNOVATIONS:
‚úÖ Hybrid Q-Learning + Minimax architecture
‚úÖ 6 specialized AI personalities v·ªõi separate training
‚úÖ Advanced decision making v·ªõi entropy analysis
‚úÖ Comprehensive testing v√† validation framework
‚úÖ Professional development tools v√† debugging

EDUCATIONAL VALUE:
‚úÖ Practical machine learning implementation
‚úÖ Traditional game AI algorithms
‚úÖ Modern software engineering practices
‚úÖ Performance optimization techniques
‚úÖ Automated testing methodologies

FUTURE ENHANCEMENTS:
üöÄ Deep Q-Learning (DQN) integration
üöÄ Multi-agent reinforcement learning
üöÄ Neural network-based evaluation functions
üöÄ Online learning v√† adaptation
üöÄ Transfer learning between game variants

D·ª± √°n n√†y demonstrating successful integration c·ªßa classical AI methods v·ªõi
modern machine learning, t·∫°o ra m·ªôt intelligent, adaptive, v√† highly
interactive gaming experience.

================================================================================
                            APPENDIX
================================================================================

FILES LI√äN QUAN:
- Assets/scripts/QLearningAgent.cs - Q-Learning implementation
- Assets/scripts/QuoridorAI.cs - Main AI controller
- Assets/scripts/DecisionTreeClassifier.cs - Decision tree logic
- Assets/scripts/QuoridorSelfPlayTrainer.cs - Training system
- Assets/scripts/AITestScript.cs - Testing framework
- Assets/scripts/Editor/ForceRetrainAI.cs - Development tools

Q-TABLE FILES:
- StreamingAssets/qtable.json - Balanced style
- StreamingAssets/qtable_aggressive.json - Aggressive style
- StreamingAssets/qtable_defensive.json - Defensive style
- StreamingAssets/qtable_wallmaster.json - Wall Master style
- StreamingAssets/qtable_speedrunner.json - Speed Runner style
- StreamingAssets/qtable_tactical.json - Tactical style

DOCUMENTATION:
- AI_ALGORITHMS_DETAILED_DESCRIPTION.txt - This file
- README.md - Project overview
- API_INTEGRATION_GUIDE.md - Voice/Chat integration
- TRAINING_GUIDE.md - AI training instructions

================================================================================
================================================================================
                    AI QUORIDOR GAME PROJECT - THU·∫¨T TO√ÅN AI CHI TI·∫æT
================================================================================

D·ª± √°n: AI Quoridor Game v·ªõi Machine Learning v√† Multi-Platform Integration
Ng√†y t·∫°o: 25/07/2025
T√°c gi·∫£: [Student Name]
Platform: Unity 3D
Language: C#

================================================================================
                            M·ª§C L·ª§C
================================================================================

1. T·ªîNG QUAN KI·∫æN TR√öC AI
2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
4. SPECIALIZED AI STYLES SYSTEM
5. DECISION TREE CLASSIFIER
6. ENTROPY ANALYSIS & SOFTMAX
7. TRAINING SYSTEM & SELF-PLAY
8. AI TESTING FRAMEWORK
9. HYBRID AI ARCHITECTURE
10. PERFORMANCE OPTIMIZATION

================================================================================
                        1. T·ªîNG QUAN KI·∫æN TR√öC AI
================================================================================

D·ª± √°n AI Quoridor s·ª≠ d·ª•ng m·ªôt ki·∫øn tr√∫c AI hybrid ti√™n ti·∫øn, k·∫øt h·ª£p:

1.1 CORE ALGORITHMS:
- Q-Learning (Reinforcement Learning) - Thu·∫≠t to√°n ch√≠nh
- Minimax + Alpha-Beta Pruning - Thu·∫≠t to√°n fallback
- Decision Tree Classifier - Ph√¢n t√≠ch ng·ªØ c·∫£nh game
- Entropy Analysis - ƒê√°nh gi√° ƒëa d·∫°ng move

1.2 SPECIALIZED SYSTEMS:
- 6 AI Personality Styles v·ªõi training ri√™ng bi·ªát
- Self-Play Training Pipeline t·ª± ƒë·ªông
- Real-time Performance Testing Framework
- Dynamic Algorithm Switching

1.3 TECHNICAL FOUNDATION:
- State Space: 13,118+ unique game states
- Action Space: Movement + Wall placement combinations
- Training Method: Self-play v·ªõi 1000 episodes
- Learning Rate: Œ± = 0.1, Discount Factor: Œ≥ = 0.99

================================================================================
                    2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
================================================================================

2.1 GI·ªöI THI·ªÜU Q-LEARNING:
Q-Learning l√† thu·∫≠t to√°n Reinforcement Learning kh√¥ng c·∫ßn model, h·ªçc th√¥ng qua
t∆∞∆°ng t√°c v·ªõi m√¥i tr∆∞·ªùng ƒë·ªÉ t·ªëi ∆∞u h√≥a policy. Trong Quoridor, AI h·ªçc c√°ch
ch·ªçn moves t·ªëi ∆∞u d·ª±a tr√™n kinh nghi·ªám t·ª´ h√†ng ng√†n game.

2.2 C√îNG TH·ª®C TO√ÅN H·ªåC:
Q(s,a) = Q(s,a) + Œ±[r + Œ≥*max(Q(s',a')) - Q(s,a)]

Trong ƒë√≥:
- Q(s,a): Q-value c·ªßa state s v·ªõi action a
- Œ±: Learning rate (0.1) - t·ªëc ƒë·ªô h·ªçc
- r: Immediate reward t·ª´ action
- Œ≥: Discount factor (0.99) - t·∫ßm quan tr·ªçng future rewards
- s': Next state sau khi th·ª±c hi·ªán action
- max(Q(s',a')): Maximum Q-value c·ªßa next state

2.3 IMPLEMENTATION CHI TI·∫æT:

```csharp
public class QLearningAgent
{
    // Core Q-Table: Dictionary l∆∞u tr·ªØ Q-values
    private Dictionary<string, Dictionary<string, float>> qTable;
    
    // Hyperparameters
    private float learningRate = 0.1f;      // T·ªëc ƒë·ªô h·ªçc
    private float discountFactor = 0.99f;   // Discount future rewards
    private float epsilon = 0.995f;         // Exploration rate
    
    // Q-Learning Update Implementation
    public void UpdateQValue(string state, string action, float reward, string nextState)
    {
        // L·∫•y Q-value hi·ªán t·∫°i
        float currentQ = GetQValue(state, action);
        
        // T√¨m max Q-value c·ªßa next state
        float maxNextQ = GetMaxQValue(nextState);
        
        // √Åp d·ª•ng c√¥ng th·ª©c Q-Learning
        float newQ = currentQ + learningRate * (reward + discountFactor * maxNextQ - currentQ);
        
        // C·∫≠p nh·∫≠t Q-table
        SetQValue(state, action, newQ);
    }
}
```

2.4 STATE ENCODING:
Game state ƒë∆∞·ª£c encode th√†nh string duy nh·∫•t:
"aiPos_x,aiPos_y-humanPos_x,humanPos_y-aiWalls-humanWalls-wallsList"

V√≠ d·ª•: "4,8-4,0-9-10-7,4,False|5,6,True"
- AI ·ªü (4,8), Human ·ªü (4,0)
- AI c√≥ 9 walls, Human c√≥ 10 walls
- Walls ƒë√£ ƒë·∫∑t: (7,4,False), (5,6,True)

2.5 ACTION ENCODING:
Actions ƒë∆∞·ª£c encode th√†nh string:
- Movement: "M:x,y" (v√≠ d·ª•: "M:4,7")
- Wall: "W:x,y,horizontal" (v√≠ d·ª•: "W:3,5,True")

2.6 EPSILON-GREEDY STRATEGY:
```csharp
public AIMove ChooseAction(GameState state, List<AIMove> possibleMoves)
{
    // Exploration vs Exploitation trade-off
    if (Random.Range(0f, 1f) < epsilon)
    {
        // EXPLORATION: Ch·ªçn random ƒë·ªÉ kh√°m ph√°
        return possibleMoves[Random.Range(0, possibleMoves.Count)];
    }
    else
    {
        // EXPLOITATION: Ch·ªçn action c√≥ Q-value cao nh·∫•t
        return GetBestAction(state, possibleMoves);
    }
}
```

2.7 EPSILON DECAY:
Epsilon gi·∫£m d·∫ßn t·ª´ 0.995 ‚Üí 0.1 qua training:
- Ban ƒë·∫ßu: High exploration (99.5%)
- Cu·ªëi training: High exploitation (90%)

2.8 REWARD SYSTEM:
- Win game: +1000
- Lose game: -1000
- Move toward goal: +10
- Block opponent: +50
- Waste move: -5
- Strategic position: +15

================================================================================
                3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
================================================================================

3.1 GI·ªöI THI·ªÜU MINIMAX:
Minimax l√† thu·∫≠t to√°n game theory c·ªï ƒëi·ªÉn, t√¨m ki·∫øm move t·ªëi ∆∞u b·∫±ng c√°ch
d·ª± ƒëo√°n tr∆∞·ªõc nhi·ªÅu moves. Alpha-Beta pruning t·ªëi ∆∞u hi·ªáu su·∫•t b·∫±ng c√°ch
lo·∫°i b·ªè c√°c nh√°nh kh√¥ng c·∫ßn thi·∫øt.

3.2 NGUY√äN L√ù HO·∫†T ƒê·ªòNG:
- Maximizing Player (AI): T·ªëi ƒëa h√≥a score
- Minimizing Player (Human): T·ªëi thi·ªÉu h√≥a score AI
- Search depth: 4 levels (configurable)
- Pruning: Lo·∫°i b·ªè nh√°nh kh√¥ng promising

3.3 IMPLEMENTATION:

```csharp
float Minimax(GameState state, int depth, bool isMaximizing, float alpha, float beta)
{
    // Base case: Depth = 0 ho·∫∑c game over
    if (depth == 0 || IsGameOver(state))
        return EvaluatePosition(state);
    
    if (isMaximizing) // AI turn
    {
        float maxEval = float.NegativeInfinity;
        foreach (var move in GeneratePossibleMoves(state, true))
        {
            GameState newState = ApplyMove(state, move, true);
            float eval = Minimax(newState, depth - 1, false, alpha, beta);
            maxEval = Mathf.Max(maxEval, eval);
            alpha = Mathf.Max(alpha, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return maxEval;
    }
    else // Human turn
    {
        float minEval = float.PositiveInfinity;
        foreach (var move in GeneratePossibleMoves(state, false))
        {
            GameState newState = ApplyMove(state, move, false);
            float eval = Minimax(newState, depth - 1, true, alpha, beta);
            minEval = Mathf.Min(minEval, eval);
            beta = Mathf.Min(beta, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return minEval;
    }
}
```

3.4 EVALUATION FUNCTION:
Position evaluation d·ª±a tr√™n multiple factors:

```csharp
float EvaluatePosition(GameState state)
{
    float score = 0f;
    
    // Distance to goal (c√†ng g·∫ßn c√†ng t·ªët)
    int aiDistance = GetShortestPath(state.aiPosition, goalRow);
    int humanDistance = GetShortestPath(state.humanPosition, goalRow);
    score += (humanDistance - aiDistance) * 100;
    
    // Wall advantage
    score += (state.aiWallsLeft - state.humanWallsLeft) * 50;
    
    // Mobility (s·ªë moves c√≥ th·ªÉ)
    int aiMobility = GetValidMoves(state.aiPosition).Count;
    int humanMobility = GetValidMoves(state.humanPosition).Count;
    score += (aiMobility - humanMobility) * 20;
    
    // Center control
    score += GetCenterControlScore(state) * 30;
    
    return score;
}
```

3.5 ALPHA-BETA PRUNING HI·ªÜU QU·∫¢:
- Reduces search space t·ª´ O(b^d) xu·ªëng O(b^(d/2))
- Best case: 50% reduction in nodes evaluated
- Ordering moves by promising gi√∫p pruning hi·ªáu qu·∫£ h∆°n

================================================================================
                        4. SPECIALIZED AI STYLES SYSTEM
================================================================================

4.1 CONCEPT:
6 AI personalities v·ªõi training v√† behavior ri√™ng bi·ªát, m·ªói style c√≥:
- Ri√™ng Q-table file
- Custom reward structure
- Specific move prioritization
- Unique decision making logic

4.2 AI STYLES CHI TI·∫æT:

üî• AGGRESSIVE STYLE:
- M·ª•c ti√™u: T·∫•n c√¥ng v√† ch·∫∑n ƒë·ªëi th·ªß
- Wall Usage: 70% chance
- Reward Structure:
  * blockingOpponentReward = 50 * intensity
  * wallPlacementReward = 30 * intensity
  * defensiveWallPenalty = -20 * intensity
- Q-table: qtable_aggressive.json

üõ°Ô∏è DEFENSIVE STYLE:
- M·ª•c ti√™u: Ph√≤ng th·ªß v√† di chuy·ªÉn an to√†n
- Wall Usage: 50% chance
- Reward Structure:
  * defensiveWallReward = 35 * intensity
  * movementReward = 20 * intensity
  * aggressiveMovePenalty = -15 * intensity
- Q-table: qtable_defensive.json

üß± WALL MASTER STYLE:
- M·ª•c ti√™u: Chuy√™n gia ƒë·∫∑t t∆∞·ªùng v√† t·∫°o m√™ cung
- Wall Usage: 90% chance
- Reward Structure:
  * wallPlacementReward = 60 * intensity
  * strategicWallReward = 40 * intensity
  * wallComboReward = 50 * intensity
  * movementWithoutWallPenalty = -25 * intensity
- Q-table: qtable_wallmaster.json

‚ö° SPEED RUNNER STYLE:
- M·ª•c ti√™u: ƒê·∫°t ƒë√≠ch nhanh nh·∫•t
- Wall Usage: 10% chance
- Reward Structure:
  * movementReward = 40 * intensity
  * progressToGoalReward = 50 * intensity
  * quickWinBonus = 100 * intensity
  * wallPlacementPenalty = -20 * intensity
- Q-table: qtable_speedrunner.json

üß† TACTICAL STYLE:
- M·ª•c ti√™u: L·ª£i th·∫ø v·ªã tr√≠ v√† k·∫ø ho·∫°ch d√†i h·∫°n
- Wall Usage: Variable (strategic)
- Reward Structure:
  * tacticalMoveReward = 45 * intensity
  * positionAdvantageReward = 35 * intensity
  * mobilityReward = 25 * intensity
  * patternRecognitionBonus = 40 * intensity
- Q-table: qtable_tactical.json

‚öñÔ∏è BALANCED STYLE:
- M·ª•c ti√™u: C√¢n b·∫±ng t·∫•t c·∫£ aspects
- Wall Usage: 30% chance
- Reward Structure: Default balanced values
- Q-table: qtable.json

4.3 STYLE SWITCHING MECHANISM:

```csharp
public void SwitchToStyle(PlayStyle newStyle)
{
    // Save current Q-table
    if (allowQTableSaving) qAgent.SaveQTable(GetStyleSpecificQTablePath());
    
    // Load new style Q-table
    specializedStyle = newStyle;
    qAgent.LoadQTable(GetStyleSpecificQTablePath());
    
    // Reconfigure parameters
    ConfigureStyleSpecificTraining();
}
```

4.4 STYLE INTENSITY:
Parameter t·ª´ 0.0 ‚Üí 1.0 control m·ª©c ƒë·ªô chuy√™n h√≥a:
- 0.0: Balanced behavior
- 0.5: Moderate specialization
- 1.0: Extreme specialization

================================================================================
                        5. DECISION TREE CLASSIFIER
================================================================================

5.1 PURPOSE:
Decision Tree ph√¢n t√≠ch game context ƒë·ªÉ ch·ªçn strategy ph√π h·ª£p, d·ª±a tr√™n:
- Game phase (Early/Mid/End)
- Position advantage
- Resource advantage
- Mobility situation

5.2 FEATURE EXTRACTION:

```csharp
public struct DecisionFeatures
{
    public int aiDistanceToGoal;      // AI distance to goal line
    public int humanDistanceToGoal;   // Human distance to goal line
    public int aiWallsLeft;           // AI walls remaining
    public int humanWallsLeft;        // Human walls remaining
    public int aiMobility;            // AI possible moves
    public int humanMobility;         // Human possible moves
    public float distanceAdvantage;   // Human - AI distance
    public float wallAdvantage;       // AI - Human walls
    public bool isEarlyGame;          // > 15 moves estimated remaining
    public bool isMidGame;            // 5-15 moves remaining
    public bool isEndGame;            // < 5 moves remaining
}
```

5.3 DECISION LOGIC:

```csharp
public AIStrategy Predict(DecisionFeatures features)
{
    // Early Game: Play conservatively
    if (features.isEarlyGame)
        return AIStrategy.Balanced;
    
    // AI has significant lead: Focus on blocking
    if (features.distanceAdvantage > 2)
        return AIStrategy.Blocking;
    
    // AI is behind: Be aggressive
    if (features.distanceAdvantage < -1)
        return AIStrategy.Aggressive;
    
    // End game with wall advantage: Play defensively
    if (features.isEndGame && features.wallAdvantage > 2)
        return AIStrategy.Defensive;
    
    // Close game: Balanced approach
    return AIStrategy.Balanced;
}
```

5.4 STRATEGY OUTCOMES:
- Balanced: Equal weight to movement v√† walls
- Blocking: Prioritize moves that increase opponent path
- Aggressive: Focus on attacking positions
- Defensive: Protect own position and path

================================================================================
                        6. ENTROPY ANALYSIS & SOFTMAX
================================================================================

6.1 ENTROPY CONCEPT:
Entropy ƒëo ƒë·ªô ƒëa d·∫°ng/uncertainty c·ªßa move scores. High entropy = nhi·ªÅu moves
c√≥ scores t∆∞∆°ng ƒë∆∞∆°ng, low entropy = m·ªôt move r√µ r√†ng t·ªët nh·∫•t.

6.2 ENTROPY CALCULATION:

```csharp
float CalculateEntropy(List<float> moveScores)
{
    // Normalize th√†nh probabilities
    float total = moveScores.Sum();
    var probabilities = moveScores.Select(s => s / total);
    
    // Calculate Shannon entropy
    float entropy = 0f;
    foreach (float p in probabilities)
    {
        if (p > 0f)
            entropy -= p * Mathf.Log(p, 2f);
    }
    
    return entropy; // Range: 0 (deterministic) to log2(n) (uniform)
}
```

6.3 SOFTMAX DISTRIBUTION:

```csharp
float[] CalculateSoftmax(List<float> scores, float temperature)
{
    float[] probabilities = new float[scores.Count];
    float sum = scores.Sum(s => Mathf.Exp(s / temperature));
    
    for (int i = 0; i < scores.Count; i++)
    {
        probabilities[i] = Mathf.Exp(scores[i] / temperature) / sum;
    }
    
    return probabilities;
}
```

6.4 TEMPERATURE PARAMETER:
- Low temperature (0.1): Sharp distribution, focus on best moves
- High temperature (2.0): Flat distribution, more exploration
- Default temperature (1.0): Balanced exploitation/exploration

6.5 ADAPTIVE MOVE SELECTION:

```csharp
public AIMove ChooseActionWithEntropy(GameState state, List<AIMove> possibleMoves)
{
    var moveScores = EvaluateAllMoves(state, possibleMoves);
    float entropy = CalculateEntropy(moveScores);
    
    if (entropy > entropyThreshold) // High diversity
    {
        // Use softmax for probabilistic selection
        var probabilities = CalculateSoftmax(moveScores, softmaxTemperature);
        return SelectMoveByProbability(possibleMoves, probabilities);
    }
    else // Clear best move
    {
        // Use deterministic best move
        return GetBestScoredMove(possibleMoves, moveScores);
    }
}
```

================================================================================
                        7. TRAINING SYSTEM & SELF-PLAY
================================================================================

7.1 SELF-PLAY CONCEPT:
AI h·ªçc b·∫±ng c√°ch ch∆°i v·ªõi ch√≠nh n√≥ h√†ng ng√†n l·∫ßn, gradually improve performance
th√¥ng qua trial v√† error.

7.2 TRAINING PIPELINE:

```csharp
IEnumerator RunSelfPlayTraining()
{
    for (int episode = 1; episode <= maxEpisodes; episode++)
    {
        // Reset game state
        ResetGameForTraining();
        
        while (!IsGameOver())
        {
            // Get current state
            GameState currentState = GetCurrentGameState();
            List<AIMove> possibleMoves = GetPossibleMoves();
            
            // Choose action using Œµ-greedy
            AIMove chosenMove = qLearningAgent.ChooseAction(currentState, possibleMoves);
            
            // Execute move
            ExecuteMove(chosenMove);
            
            // Calculate reward
            float reward = CalculateReward(currentState, chosenMove);
            
            // Update Q-table
            GameState newState = GetCurrentGameState();
            qLearningAgent.UpdateQValue(
                EncodeState(currentState),
                EncodeAction(chosenMove),
                reward,
                EncodeState(newState)
            );
            
            yield return new WaitForEndOfFrame();
        }
        
        // Epsilon decay
        qLearningAgent.DecayEpsilon();
        
        // Save progress periodically
        if (episode % 100 == 0)
        {
            qLearningAgent.SaveQTable();
        }
    }
}
```

7.3 REWARD ENGINEERING:
Critical cho Q-Learning success:

```csharp
float CalculateReward(GameState state, AIMove move)
{
    float reward = 0f;
    
    // Game outcome rewards
    if (IsWin(state)) reward += 1000f;
    if (IsLoss(state)) reward -= 1000f;
    
    // Progress rewards
    if (move.moveType == MoveType.Movement)
    {
        if (IsMoveTowardGoal(move)) reward += 20f;
        else reward -= 5f;
    }
    
    // Strategic rewards
    if (move.moveType == MoveType.WallPlacement)
    {
        if (IsWallBlockingOpponent(move)) reward += 50f;
        if (IsWallStrategic(move)) reward += 30f;
        if (IsWallWasted(move)) reward -= 25f;
    }
    
    // Style-specific rewards
    reward += GetStyleSpecificReward(state, move);
    
    return reward;
}
```

7.4 TRAINING METRICS:
- Episodes: 1000 standard
- Duration: ~15 minutes
- Convergence: Usually around episode 800
- Q-table size: 13,118+ states at completion
- Final epsilon: 0.1 (90% exploitation)

================================================================================
                        8. AI TESTING FRAMEWORK
================================================================================

8.1 TESTING PHILOSOPHY:
Comprehensive testing ensures AI quality v√† reliability across different
game scenarios v√† edge cases.

8.2 TEST CATEGORIES:

A) DECISION TREE TESTS:
```csharp
void TestDecisionTreeStrategies()
{
    // Test Early Game
    var earlyGameFeatures = new DecisionFeatures
    {
        aiDistanceToGoal = 8, humanDistanceToGoal = 8,
        isEarlyGame = true
    };
    var strategy = decisionTree.Predict(earlyGameFeatures);
    Assert.AreEqual(AIStrategy.Balanced, strategy);
    
    // Test AI Leading
    var leadingFeatures = new DecisionFeatures
    {
        distanceAdvantage = 3, // AI leading
        isMidGame = true
    };
    strategy = decisionTree.Predict(leadingFeatures);
    Assert.AreEqual(AIStrategy.Blocking, strategy);
}
```

B) ENTROPY ANALYSIS TESTS:
```csharp
void TestEntropyAnalysis()
{
    // High entropy case (diverse scores)
    var highEntropyScores = new List<float> { 100f, 98f, 97f, 95f, 94f };
    float entropy = CalculateTestEntropy(highEntropyScores);
    Assert.IsTrue(entropy > 0.5f);
    
    // Low entropy case (clear winner)
    var lowEntropyScores = new List<float> { 100f, 50f, 50f, 50f, 50f };
    entropy = CalculateTestEntropy(lowEntropyScores);
    Assert.IsTrue(entropy < 0.5f);
}
```

C) SOFTMAX DISTRIBUTION TESTS:
```csharp
void TestSoftmaxDistribution()
{
    var testScores = new List<float> { 100f, 98f, 95f, 90f };
    float temperature = 1.0f;
    
    var probabilities = CalculateSoftmax(testScores, temperature);
    
    // Verify probabilities sum to 1
    float sum = probabilities.Sum();
    Assert.AreEqual(1.0f, sum, 0.001f);
    
    // Verify highest score gets highest probability
    int maxIndex = Array.IndexOf(probabilities, probabilities.Max());
    Assert.AreEqual(0, maxIndex);
}
```

D) PERFORMANCE BENCHMARKS:
- Move generation time: <100ms
- Q-Learning decision: <50ms
- Minimax decision (depth 4): <200ms
- Memory usage: <256MB

8.3 AUTOMATED TESTING:
Tests run automatically ƒë·ªÉ ensure code quality:
- Unit tests cho individual algorithms
- Integration tests cho AI system interactions
- Performance tests cho response times
- Regression tests ƒë·ªÉ prevent quality degradation

================================================================================
                        9. HYBRID AI ARCHITECTURE
================================================================================

9.1 CONCEPT:
Hybrid system k·∫øt h·ª£p strengths c·ªßa multiple algorithms:
- Q-Learning: Data-driven, learns from experience
- Minimax: Logical, guaranteed optimal given evaluation
- Decision Tree: Fast, context-aware switching
- Entropy: Adaptive, handles uncertainty

9.2 ALGORITHM SELECTION LOGIC:

```csharp
public AIMove GetBestMove()
{
    UpdateGameState();
    List<AIMove> possibleMoves = GeneratePossibleMoves();
    
    // Primary: Use Q-Learning if trained model available
    if (useQLearning && qAgent.IsTrainedModel())
    {
        Debug.Log($"üß† Using {specializedStyle} Q-Learning");
        return qAgent.ChooseAction(currentGameState, possibleMoves);
    }
    
    // Fallback: Use Minimax for guaranteed reasonable play
    else
    {
        Debug.Log($"üîç Using {specializedStyle} Minimax");
        return FindBestMoveWithMinimax();
    }
}
```

9.3 SEAMLESS SWITCHING:
- No interruption in gameplay
- Maintains consistent API
- Preserves game state integrity
- Logs algorithm used for debugging

9.4 BENEFITS:
- Robustness: Fallback ensures AI never fails
- Performance: Q-Learning for trained scenarios, Minimax for edge cases
- Flexibility: Can switch algorithms based on situation
- Development: Easy to test individual components

================================================================================
                        10. PERFORMANCE OPTIMIZATION
================================================================================

10.1 Q-TABLE OPTIMIZATION:
- Sparse representation: Only store visited states
- Compression: String interning for state keys
- Lazy loading: Load Q-table on demand
- Periodic saving: Every 100 training episodes

10.2 MINIMAX OPTIMIZATION:
- Alpha-Beta pruning: 50% search reduction
- Move ordering: Promising moves first
- Iterative deepening: Gradual depth increase
- Transposition tables: Cache evaluated positions

10.3 MEMORY MANAGEMENT:
- Object pooling cho GameState objects
- String caching cho repeated state/action encodings
- Garbage collection optimization
- Memory profiling v√† monitoring

10.4 MULTITHREADING CONSIDERATIONS:
- Q-Learning training: Background thread
- Move calculation: Main thread (Unity requirement)
- File I/O: Async operations where possible
- Thread-safe collections cho shared data

10.5 CACHING STRATEGIES:
- Path finding results
- Move generation results
- Evaluation function results
- Feature extraction results

================================================================================
                            K·∫æT LU·∫¨N
================================================================================

D·ª± √°n AI Quoridor Project th·ªÉ hi·ªán m·ªôt implementation comprehensive c·ªßa modern
game AI techniques:

TECHNICAL INNOVATIONS:
‚úÖ Hybrid Q-Learning + Minimax architecture
‚úÖ 6 specialized AI personalities v·ªõi separate training
‚úÖ Advanced decision making v·ªõi entropy analysis
‚úÖ Comprehensive testing v√† validation framework
‚úÖ Professional development tools v√† debugging

EDUCATIONAL VALUE:
‚úÖ Practical machine learning implementation
‚úÖ Traditional game AI algorithms
‚úÖ Modern software engineering practices
‚úÖ Performance optimization techniques
‚úÖ Automated testing methodologies

FUTURE ENHANCEMENTS:
üöÄ Deep Q-Learning (DQN) integration
üöÄ Multi-agent reinforcement learning
üöÄ Neural network-based evaluation functions
üöÄ Online learning v√† adaptation
üöÄ Transfer learning between game variants

D·ª± √°n n√†y demonstrating successful integration c·ªßa classical AI methods v·ªõi
modern machine learning, t·∫°o ra m·ªôt intelligent, adaptive, v√† highly
interactive gaming experience.

================================================================================
                            APPENDIX
================================================================================

FILES LI√äN QUAN:
- Assets/scripts/QLearningAgent.cs - Q-Learning implementation
- Assets/scripts/QuoridorAI.cs - Main AI controller
- Assets/scripts/DecisionTreeClassifier.cs - Decision tree logic
- Assets/scripts/QuoridorSelfPlayTrainer.cs - Training system
- Assets/scripts/AITestScript.cs - Testing framework
- Assets/scripts/Editor/ForceRetrainAI.cs - Development tools

Q-TABLE FILES:
- StreamingAssets/qtable.json - Balanced style
- StreamingAssets/qtable_aggressive.json - Aggressive style
- StreamingAssets/qtable_defensive.json - Defensive style
- StreamingAssets/qtable_wallmaster.json - Wall Master style
- StreamingAssets/qtable_speedrunner.json - Speed Runner style
- StreamingAssets/qtable_tactical.json - Tactical style

DOCUMENTATION:
- AI_ALGORITHMS_DETAILED_DESCRIPTION.txt - This file
- README.md - Project overview
- API_INTEGRATION_GUIDE.md - Voice/Chat integration
- TRAINING_GUIDE.md - AI training instructions

================================================================================
================================================================================
                    AI QUORIDOR GAME PROJECT - THU·∫¨T TO√ÅN AI CHI TI·∫æT
================================================================================

D·ª± √°n: AI Quoridor Game v·ªõi Machine Learning v√† Multi-Platform Integration
Ng√†y t·∫°o: 25/07/2025
T√°c gi·∫£: [Student Name]
Platform: Unity 3D
Language: C#

================================================================================
                            M·ª§C L·ª§C
================================================================================

1. T·ªîNG QUAN KI·∫æN TR√öC AI
2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
4. SPECIALIZED AI STYLES SYSTEM
5. DECISION TREE CLASSIFIER
6. ENTROPY ANALYSIS & SOFTMAX
7. TRAINING SYSTEM & SELF-PLAY
8. AI TESTING FRAMEWORK
9. HYBRID AI ARCHITECTURE
10. PERFORMANCE OPTIMIZATION

================================================================================
                        1. T·ªîNG QUAN KI·∫æN TR√öC AI
================================================================================

D·ª± √°n AI Quoridor s·ª≠ d·ª•ng m·ªôt ki·∫øn tr√∫c AI hybrid ti√™n ti·∫øn, k·∫øt h·ª£p:

1.1 CORE ALGORITHMS:
- Q-Learning (Reinforcement Learning) - Thu·∫≠t to√°n ch√≠nh
- Minimax + Alpha-Beta Pruning - Thu·∫≠t to√°n fallback
- Decision Tree Classifier - Ph√¢n t√≠ch ng·ªØ c·∫£nh game
- Entropy Analysis - ƒê√°nh gi√° ƒëa d·∫°ng move

1.2 SPECIALIZED SYSTEMS:
- 6 AI Personality Styles v·ªõi training ri√™ng bi·ªát
- Self-Play Training Pipeline t·ª± ƒë·ªông
- Real-time Performance Testing Framework
- Dynamic Algorithm Switching

1.3 TECHNICAL FOUNDATION:
- State Space: 13,118+ unique game states
- Action Space: Movement + Wall placement combinations
- Training Method: Self-play v·ªõi 1000 episodes
- Learning Rate: Œ± = 0.1, Discount Factor: Œ≥ = 0.99

================================================================================
                    2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
================================================================================

2.1 GI·ªöI THI·ªÜU Q-LEARNING:
Q-Learning l√† thu·∫≠t to√°n Reinforcement Learning kh√¥ng c·∫ßn model, h·ªçc th√¥ng qua
t∆∞∆°ng t√°c v·ªõi m√¥i tr∆∞·ªùng ƒë·ªÉ t·ªëi ∆∞u h√≥a policy. Trong Quoridor, AI h·ªçc c√°ch
ch·ªçn moves t·ªëi ∆∞u d·ª±a tr√™n kinh nghi·ªám t·ª´ h√†ng ng√†n game.

2.2 C√îNG TH·ª®C TO√ÅN H·ªåC:
Q(s,a) = Q(s,a) + Œ±[r + Œ≥*max(Q(s',a')) - Q(s,a)]

Trong ƒë√≥:
- Q(s,a): Q-value c·ªßa state s v·ªõi action a
- Œ±: Learning rate (0.1) - t·ªëc ƒë·ªô h·ªçc
- r: Immediate reward t·ª´ action
- Œ≥: Discount factor (0.99) - t·∫ßm quan tr·ªçng future rewards
- s': Next state sau khi th·ª±c hi·ªán action
- max(Q(s',a')): Maximum Q-value c·ªßa next state

2.3 IMPLEMENTATION CHI TI·∫æT:

```csharp
public class QLearningAgent
{
    // Core Q-Table: Dictionary l∆∞u tr·ªØ Q-values
    private Dictionary<string, Dictionary<string, float>> qTable;
    
    // Hyperparameters
    private float learningRate = 0.1f;      // T·ªëc ƒë·ªô h·ªçc
    private float discountFactor = 0.99f;   // Discount future rewards
    private float epsilon = 0.995f;         // Exploration rate
    
    // Q-Learning Update Implementation
    public void UpdateQValue(string state, string action, float reward, string nextState)
    {
        // L·∫•y Q-value hi·ªán t·∫°i
        float currentQ = GetQValue(state, action);
        
        // T√¨m max Q-value c·ªßa next state
        float maxNextQ = GetMaxQValue(nextState);
        
        // √Åp d·ª•ng c√¥ng th·ª©c Q-Learning
        float newQ = currentQ + learningRate * (reward + discountFactor * maxNextQ - currentQ);
        
        // C·∫≠p nh·∫≠t Q-table
        SetQValue(state, action, newQ);
    }
}
```

2.4 STATE ENCODING:
Game state ƒë∆∞·ª£c encode th√†nh string duy nh·∫•t:
"aiPos_x,aiPos_y-humanPos_x,humanPos_y-aiWalls-humanWalls-wallsList"

V√≠ d·ª•: "4,8-4,0-9-10-7,4,False|5,6,True"
- AI ·ªü (4,8), Human ·ªü (4,0)
- AI c√≥ 9 walls, Human c√≥ 10 walls
- Walls ƒë√£ ƒë·∫∑t: (7,4,False), (5,6,True)

2.5 ACTION ENCODING:
Actions ƒë∆∞·ª£c encode th√†nh string:
- Movement: "M:x,y" (v√≠ d·ª•: "M:4,7")
- Wall: "W:x,y,horizontal" (v√≠ d·ª•: "W:3,5,True")

2.6 EPSILON-GREEDY STRATEGY:
```csharp
public AIMove ChooseAction(GameState state, List<AIMove> possibleMoves)
{
    // Exploration vs Exploitation trade-off
    if (Random.Range(0f, 1f) < epsilon)
    {
        // EXPLORATION: Ch·ªçn random ƒë·ªÉ kh√°m ph√°
        return possibleMoves[Random.Range(0, possibleMoves.Count)];
    }
    else
    {
        // EXPLOITATION: Ch·ªçn action c√≥ Q-value cao nh·∫•t
        return GetBestAction(state, possibleMoves);
    }
}
```

2.7 EPSILON DECAY:
Epsilon gi·∫£m d·∫ßn t·ª´ 0.995 ‚Üí 0.1 qua training:
- Ban ƒë·∫ßu: High exploration (99.5%)
- Cu·ªëi training: High exploitation (90%)

2.8 REWARD SYSTEM:
- Win game: +1000
- Lose game: -1000
- Move toward goal: +10
- Block opponent: +50
- Waste move: -5
- Strategic position: +15

================================================================================
                3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
================================================================================

3.1 GI·ªöI THI·ªÜU MINIMAX:
Minimax l√† thu·∫≠t to√°n game theory c·ªï ƒëi·ªÉn, t√¨m ki·∫øm move t·ªëi ∆∞u b·∫±ng c√°ch
d·ª± ƒëo√°n tr∆∞·ªõc nhi·ªÅu moves. Alpha-Beta pruning t·ªëi ∆∞u hi·ªáu su·∫•t b·∫±ng c√°ch
lo·∫°i b·ªè c√°c nh√°nh kh√¥ng c·∫ßn thi·∫øt.

3.2 NGUY√äN L√ù HO·∫†T ƒê·ªòNG:
- Maximizing Player (AI): T·ªëi ƒëa h√≥a score
- Minimizing Player (Human): T·ªëi thi·ªÉu h√≥a score AI
- Search depth: 4 levels (configurable)
- Pruning: Lo·∫°i b·ªè nh√°nh kh√¥ng promising

3.3 IMPLEMENTATION:

```csharp
float Minimax(GameState state, int depth, bool isMaximizing, float alpha, float beta)
{
    // Base case: Depth = 0 ho·∫∑c game over
    if (depth == 0 || IsGameOver(state))
        return EvaluatePosition(state);
    
    if (isMaximizing) // AI turn
    {
        float maxEval = float.NegativeInfinity;
        foreach (var move in GeneratePossibleMoves(state, true))
        {
            GameState newState = ApplyMove(state, move, true);
            float eval = Minimax(newState, depth - 1, false, alpha, beta);
            maxEval = Mathf.Max(maxEval, eval);
            alpha = Mathf.Max(alpha, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return maxEval;
    }
    else // Human turn
    {
        float minEval = float.PositiveInfinity;
        foreach (var move in GeneratePossibleMoves(state, false))
        {
            GameState newState = ApplyMove(state, move, false);
            float eval = Minimax(newState, depth - 1, true, alpha, beta);
            minEval = Mathf.Min(minEval, eval);
            beta = Mathf.Min(beta, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return minEval;
    }
}
```

3.4 EVALUATION FUNCTION:
Position evaluation d·ª±a tr√™n multiple factors:

```csharp
float EvaluatePosition(GameState state)
{
    float score = 0f;
    
    // Distance to goal (c√†ng g·∫ßn c√†ng t·ªët)
    int aiDistance = GetShortestPath(state.aiPosition, goalRow);
    int humanDistance = GetShortestPath(state.humanPosition, goalRow);
    score += (humanDistance - aiDistance) * 100;
    
    // Wall advantage
    score += (state.aiWallsLeft - state.humanWallsLeft) * 50;
    
    // Mobility (s·ªë moves c√≥ th·ªÉ)
    int aiMobility = GetValidMoves(state.aiPosition).Count;
    int humanMobility = GetValidMoves(state.humanPosition).Count;
    score += (aiMobility - humanMobility) * 20;
    
    // Center control
    score += GetCenterControlScore(state) * 30;
    
    return score;
}
```

3.5 ALPHA-BETA PRUNING HI·ªÜU QU·∫¢:
- Reduces search space t·ª´ O(b^d) xu·ªëng O(b^(d/2))
- Best case: 50% reduction in nodes evaluated
- Ordering moves by promising gi√∫p pruning hi·ªáu qu·∫£ h∆°n

================================================================================
                        4. SPECIALIZED AI STYLES SYSTEM
================================================================================

4.1 CONCEPT:
6 AI personalities v·ªõi training v√† behavior ri√™ng bi·ªát, m·ªói style c√≥:
- Ri√™ng Q-table file
- Custom reward structure
- Specific move prioritization
- Unique decision making logic

4.2 AI STYLES CHI TI·∫æT:

üî• AGGRESSIVE STYLE:
- M·ª•c ti√™u: T·∫•n c√¥ng v√† ch·∫∑n ƒë·ªëi th·ªß
- Wall Usage: 70% chance
- Reward Structure:
  * blockingOpponentReward = 50 * intensity
  * wallPlacementReward = 30 * intensity
  * defensiveWallPenalty = -20 * intensity
- Q-table: qtable_aggressive.json

üõ°Ô∏è DEFENSIVE STYLE:
- M·ª•c ti√™u: Ph√≤ng th·ªß v√† di chuy·ªÉn an to√†n
- Wall Usage: 50% chance
- Reward Structure:
  * defensiveWallReward = 35 * intensity
  * movementReward = 20 * intensity
  * aggressiveMovePenalty = -15 * intensity
- Q-table: qtable_defensive.json

üß± WALL MASTER STYLE:
- M·ª•c ti√™u: Chuy√™n gia ƒë·∫∑t t∆∞·ªùng v√† t·∫°o m√™ cung
- Wall Usage: 90% chance
- Reward Structure:
  * wallPlacementReward = 60 * intensity
  * strategicWallReward = 40 * intensity
  * wallComboReward = 50 * intensity
  * movementWithoutWallPenalty = -25 * intensity
- Q-table: qtable_wallmaster.json

‚ö° SPEED RUNNER STYLE:
- M·ª•c ti√™u: ƒê·∫°t ƒë√≠ch nhanh nh·∫•t
- Wall Usage: 10% chance
- Reward Structure:
  * movementReward = 40 * intensity
  * progressToGoalReward = 50 * intensity
  * quickWinBonus = 100 * intensity
  * wallPlacementPenalty = -20 * intensity
- Q-table: qtable_speedrunner.json

üß† TACTICAL STYLE:
- M·ª•c ti√™u: L·ª£i th·∫ø v·ªã tr√≠ v√† k·∫ø ho·∫°ch d√†i h·∫°n
- Wall Usage: Variable (strategic)
- Reward Structure:
  * tacticalMoveReward = 45 * intensity
  * positionAdvantageReward = 35 * intensity
  * mobilityReward = 25 * intensity
  * patternRecognitionBonus = 40 * intensity
- Q-table: qtable_tactical.json

‚öñÔ∏è BALANCED STYLE:
- M·ª•c ti√™u: C√¢n b·∫±ng t·∫•t c·∫£ aspects
- Wall Usage: 30% chance
- Reward Structure: Default balanced values
- Q-table: qtable.json

4.3 STYLE SWITCHING MECHANISM:

```csharp
public void SwitchToStyle(PlayStyle newStyle)
{
    // Save current Q-table
    if (allowQTableSaving) qAgent.SaveQTable(GetStyleSpecificQTablePath());
    
    // Load new style Q-table
    specializedStyle = newStyle;
    qAgent.LoadQTable(GetStyleSpecificQTablePath());
    
    // Reconfigure parameters
    ConfigureStyleSpecificTraining();
}
```

4.4 STYLE INTENSITY:
Parameter t·ª´ 0.0 ‚Üí 1.0 control m·ª©c ƒë·ªô chuy√™n h√≥a:
- 0.0: Balanced behavior
- 0.5: Moderate specialization
- 1.0: Extreme specialization

================================================================================
                        5. DECISION TREE CLASSIFIER
================================================================================

5.1 PURPOSE:
Decision Tree ph√¢n t√≠ch game context ƒë·ªÉ ch·ªçn strategy ph√π h·ª£p, d·ª±a tr√™n:
- Game phase (Early/Mid/End)
- Position advantage
- Resource advantage
- Mobility situation

5.2 FEATURE EXTRACTION:

```csharp
public struct DecisionFeatures
{
    public int aiDistanceToGoal;      // AI distance to goal line
    public int humanDistanceToGoal;   // Human distance to goal line
    public int aiWallsLeft;           // AI walls remaining
    public int humanWallsLeft;        // Human walls remaining
    public int aiMobility;            // AI possible moves
    public int humanMobility;         // Human possible moves
    public float distanceAdvantage;   // Human - AI distance
    public float wallAdvantage;       // AI - Human walls
    public bool isEarlyGame;          // > 15 moves estimated remaining
    public bool isMidGame;            // 5-15 moves remaining
    public bool isEndGame;            // < 5 moves remaining
}
```

5.3 DECISION LOGIC:

```csharp
public AIStrategy Predict(DecisionFeatures features)
{
    // Early Game: Play conservatively
    if (features.isEarlyGame)
        return AIStrategy.Balanced;
    
    // AI has significant lead: Focus on blocking
    if (features.distanceAdvantage > 2)
        return AIStrategy.Blocking;
    
    // AI is behind: Be aggressive
    if (features.distanceAdvantage < -1)
        return AIStrategy.Aggressive;
    
    // End game with wall advantage: Play defensively
    if (features.isEndGame && features.wallAdvantage > 2)
        return AIStrategy.Defensive;
    
    // Close game: Balanced approach
    return AIStrategy.Balanced;
}
```

5.4 STRATEGY OUTCOMES:
- Balanced: Equal weight to movement v√† walls
- Blocking: Prioritize moves that increase opponent path
- Aggressive: Focus on attacking positions
- Defensive: Protect own position and path

================================================================================
                        6. ENTROPY ANALYSIS & SOFTMAX
================================================================================

6.1 ENTROPY CONCEPT:
Entropy ƒëo ƒë·ªô ƒëa d·∫°ng/uncertainty c·ªßa move scores. High entropy = nhi·ªÅu moves
c√≥ scores t∆∞∆°ng ƒë∆∞∆°ng, low entropy = m·ªôt move r√µ r√†ng t·ªët nh·∫•t.

6.2 ENTROPY CALCULATION:

```csharp
float CalculateEntropy(List<float> moveScores)
{
    // Normalize th√†nh probabilities
    float total = moveScores.Sum();
    var probabilities = moveScores.Select(s => s / total);
    
    // Calculate Shannon entropy
    float entropy = 0f;
    foreach (float p in probabilities)
    {
        if (p > 0f)
            entropy -= p * Mathf.Log(p, 2f);
    }
    
    return entropy; // Range: 0 (deterministic) to log2(n) (uniform)
}
```

6.3 SOFTMAX DISTRIBUTION:

```csharp
float[] CalculateSoftmax(List<float> scores, float temperature)
{
    float[] probabilities = new float[scores.Count];
    float sum = scores.Sum(s => Mathf.Exp(s / temperature));
    
    for (int i = 0; i < scores.Count; i++)
    {
        probabilities[i] = Mathf.Exp(scores[i] / temperature) / sum;
    }
    
    return probabilities;
}
```

6.4 TEMPERATURE PARAMETER:
- Low temperature (0.1): Sharp distribution, focus on best moves
- High temperature (2.0): Flat distribution, more exploration
- Default temperature (1.0): Balanced exploitation/exploration

6.5 ADAPTIVE MOVE SELECTION:

```csharp
public AIMove ChooseActionWithEntropy(GameState state, List<AIMove> possibleMoves)
{
    var moveScores = EvaluateAllMoves(state, possibleMoves);
    float entropy = CalculateEntropy(moveScores);
    
    if (entropy > entropyThreshold) // High diversity
    {
        // Use softmax for probabilistic selection
        var probabilities = CalculateSoftmax(moveScores, softmaxTemperature);
        return SelectMoveByProbability(possibleMoves, probabilities);
    }
    else // Clear best move
    {
        // Use deterministic best move
        return GetBestScoredMove(possibleMoves, moveScores);
    }
}
```

================================================================================
                        7. TRAINING SYSTEM & SELF-PLAY
================================================================================

7.1 SELF-PLAY CONCEPT:
AI h·ªçc b·∫±ng c√°ch ch∆°i v·ªõi ch√≠nh n√≥ h√†ng ng√†n l·∫ßn, gradually improve performance
th√¥ng qua trial v√† error.

7.2 TRAINING PIPELINE:

```csharp
IEnumerator RunSelfPlayTraining()
{
    for (int episode = 1; episode <= maxEpisodes; episode++)
    {
        // Reset game state
        ResetGameForTraining();
        
        while (!IsGameOver())
        {
            // Get current state
            GameState currentState = GetCurrentGameState();
            List<AIMove> possibleMoves = GetPossibleMoves();
            
            // Choose action using Œµ-greedy
            AIMove chosenMove = qLearningAgent.ChooseAction(currentState, possibleMoves);
            
            // Execute move
            ExecuteMove(chosenMove);
            
            // Calculate reward
            float reward = CalculateReward(currentState, chosenMove);
            
            // Update Q-table
            GameState newState = GetCurrentGameState();
            qLearningAgent.UpdateQValue(
                EncodeState(currentState),
                EncodeAction(chosenMove),
                reward,
                EncodeState(newState)
            );
            
            yield return new WaitForEndOfFrame();
        }
        
        // Epsilon decay
        qLearningAgent.DecayEpsilon();
        
        // Save progress periodically
        if (episode % 100 == 0)
        {
            qLearningAgent.SaveQTable();
        }
    }
}
```

7.3 REWARD ENGINEERING:
Critical cho Q-Learning success:

```csharp
float CalculateReward(GameState state, AIMove move)
{
    float reward = 0f;
    
    // Game outcome rewards
    if (IsWin(state)) reward += 1000f;
    if (IsLoss(state)) reward -= 1000f;
    
    // Progress rewards
    if (move.moveType == MoveType.Movement)
    {
        if (IsMoveTowardGoal(move)) reward += 20f;
        else reward -= 5f;
    }
    
    // Strategic rewards
    if (move.moveType == MoveType.WallPlacement)
    {
        if (IsWallBlockingOpponent(move)) reward += 50f;
        if (IsWallStrategic(move)) reward += 30f;
        if (IsWallWasted(move)) reward -= 25f;
    }
    
    // Style-specific rewards
    reward += GetStyleSpecificReward(state, move);
    
    return reward;
}
```

7.4 TRAINING METRICS:
- Episodes: 1000 standard
- Duration: ~15 minutes
- Convergence: Usually around episode 800
- Q-table size: 13,118+ states at completion
- Final epsilon: 0.1 (90% exploitation)

================================================================================
                        8. AI TESTING FRAMEWORK
================================================================================

8.1 TESTING PHILOSOPHY:
Comprehensive testing ensures AI quality v√† reliability across different
game scenarios v√† edge cases.

8.2 TEST CATEGORIES:

A) DECISION TREE TESTS:
```csharp
void TestDecisionTreeStrategies()
{
    // Test Early Game
    var earlyGameFeatures = new DecisionFeatures
    {
        aiDistanceToGoal = 8, humanDistanceToGoal = 8,
        isEarlyGame = true
    };
    var strategy = decisionTree.Predict(earlyGameFeatures);
    Assert.AreEqual(AIStrategy.Balanced, strategy);
    
    // Test AI Leading
    var leadingFeatures = new DecisionFeatures
    {
        distanceAdvantage = 3, // AI leading
        isMidGame = true
    };
    strategy = decisionTree.Predict(leadingFeatures);
    Assert.AreEqual(AIStrategy.Blocking, strategy);
}
```

B) ENTROPY ANALYSIS TESTS:
```csharp
void TestEntropyAnalysis()
{
    // High entropy case (diverse scores)
    var highEntropyScores = new List<float> { 100f, 98f, 97f, 95f, 94f };
    float entropy = CalculateTestEntropy(highEntropyScores);
    Assert.IsTrue(entropy > 0.5f);
    
    // Low entropy case (clear winner)
    var lowEntropyScores = new List<float> { 100f, 50f, 50f, 50f, 50f };
    entropy = CalculateTestEntropy(lowEntropyScores);
    Assert.IsTrue(entropy < 0.5f);
}
```

C) SOFTMAX DISTRIBUTION TESTS:
```csharp
void TestSoftmaxDistribution()
{
    var testScores = new List<float> { 100f, 98f, 95f, 90f };
    float temperature = 1.0f;
    
    var probabilities = CalculateSoftmax(testScores, temperature);
    
    // Verify probabilities sum to 1
    float sum = probabilities.Sum();
    Assert.AreEqual(1.0f, sum, 0.001f);
    
    // Verify highest score gets highest probability
    int maxIndex = Array.IndexOf(probabilities, probabilities.Max());
    Assert.AreEqual(0, maxIndex);
}
```

D) PERFORMANCE BENCHMARKS:
- Move generation time: <100ms
- Q-Learning decision: <50ms
- Minimax decision (depth 4): <200ms
- Memory usage: <256MB

8.3 AUTOMATED TESTING:
Tests run automatically ƒë·ªÉ ensure code quality:
- Unit tests cho individual algorithms
- Integration tests cho AI system interactions
- Performance tests cho response times
- Regression tests ƒë·ªÉ prevent quality degradation

================================================================================
                        9. HYBRID AI ARCHITECTURE
================================================================================

9.1 CONCEPT:
Hybrid system k·∫øt h·ª£p strengths c·ªßa multiple algorithms:
- Q-Learning: Data-driven, learns from experience
- Minimax: Logical, guaranteed optimal given evaluation
- Decision Tree: Fast, context-aware switching
- Entropy: Adaptive, handles uncertainty

9.2 ALGORITHM SELECTION LOGIC:

```csharp
public AIMove GetBestMove()
{
    UpdateGameState();
    List<AIMove> possibleMoves = GeneratePossibleMoves();
    
    // Primary: Use Q-Learning if trained model available
    if (useQLearning && qAgent.IsTrainedModel())
    {
        Debug.Log($"üß† Using {specializedStyle} Q-Learning");
        return qAgent.ChooseAction(currentGameState, possibleMoves);
    }
    
    // Fallback: Use Minimax for guaranteed reasonable play
    else
    {
        Debug.Log($"üîç Using {specializedStyle} Minimax");
        return FindBestMoveWithMinimax();
    }
}
```

9.3 SEAMLESS SWITCHING:
- No interruption in gameplay
- Maintains consistent API
- Preserves game state integrity
- Logs algorithm used for debugging

9.4 BENEFITS:
- Robustness: Fallback ensures AI never fails
- Performance: Q-Learning for trained scenarios, Minimax for edge cases
- Flexibility: Can switch algorithms based on situation
- Development: Easy to test individual components

================================================================================
                        10. PERFORMANCE OPTIMIZATION
================================================================================

10.1 Q-TABLE OPTIMIZATION:
- Sparse representation: Only store visited states
- Compression: String interning for state keys
- Lazy loading: Load Q-table on demand
- Periodic saving: Every 100 training episodes

10.2 MINIMAX OPTIMIZATION:
- Alpha-Beta pruning: 50% search reduction
- Move ordering: Promising moves first
- Iterative deepening: Gradual depth increase
- Transposition tables: Cache evaluated positions

10.3 MEMORY MANAGEMENT:
- Object pooling cho GameState objects
- String caching cho repeated state/action encodings
- Garbage collection optimization
- Memory profiling v√† monitoring

10.4 MULTITHREADING CONSIDERATIONS:
- Q-Learning training: Background thread
- Move calculation: Main thread (Unity requirement)
- File I/O: Async operations where possible
- Thread-safe collections cho shared data

10.5 CACHING STRATEGIES:
- Path finding results
- Move generation results
- Evaluation function results
- Feature extraction results

================================================================================
                            K·∫æT LU·∫¨N
================================================================================

D·ª± √°n AI Quoridor Project th·ªÉ hi·ªán m·ªôt implementation comprehensive c·ªßa modern
game AI techniques:

TECHNICAL INNOVATIONS:
‚úÖ Hybrid Q-Learning + Minimax architecture
‚úÖ 6 specialized AI personalities v·ªõi separate training
‚úÖ Advanced decision making v·ªõi entropy analysis
‚úÖ Comprehensive testing v√† validation framework
‚úÖ Professional development tools v√† debugging

EDUCATIONAL VALUE:
‚úÖ Practical machine learning implementation
‚úÖ Traditional game AI algorithms
‚úÖ Modern software engineering practices
‚úÖ Performance optimization techniques
‚úÖ Automated testing methodologies

FUTURE ENHANCEMENTS:
üöÄ Deep Q-Learning (DQN) integration
üöÄ Multi-agent reinforcement learning
üöÄ Neural network-based evaluation functions
üöÄ Online learning v√† adaptation
üöÄ Transfer learning between game variants

D·ª± √°n n√†y demonstrating successful integration c·ªßa classical AI methods v·ªõi
modern machine learning, t·∫°o ra m·ªôt intelligent, adaptive, v√† highly
interactive gaming experience.

================================================================================
                            APPENDIX
================================================================================

FILES LI√äN QUAN:
- Assets/scripts/QLearningAgent.cs - Q-Learning implementation
- Assets/scripts/QuoridorAI.cs - Main AI controller
- Assets/scripts/DecisionTreeClassifier.cs - Decision tree logic
- Assets/scripts/QuoridorSelfPlayTrainer.cs - Training system
- Assets/scripts/AITestScript.cs - Testing framework
- Assets/scripts/Editor/ForceRetrainAI.cs - Development tools

Q-TABLE FILES:
- StreamingAssets/qtable.json - Balanced style
- StreamingAssets/qtable_aggressive.json - Aggressive style
- StreamingAssets/qtable_defensive.json - Defensive style
- StreamingAssets/qtable_wallmaster.json - Wall Master style
- StreamingAssets/qtable_speedrunner.json - Speed Runner style
- StreamingAssets/qtable_tactical.json - Tactical style

DOCUMENTATION:
- AI_ALGORITHMS_DETAILED_DESCRIPTION.txt - This file
- README.md - Project overview
- API_INTEGRATION_GUIDE.md - Voice/Chat integration
- TRAINING_GUIDE.md - AI training instructions

================================================================================
================================================================================
                    AI QUORIDOR GAME PROJECT - THU·∫¨T TO√ÅN AI CHI TI·∫æT
================================================================================

D·ª± √°n: AI Quoridor Game v·ªõi Machine Learning v√† Multi-Platform Integration
Ng√†y t·∫°o: 25/07/2025
T√°c gi·∫£: [Student Name]
Platform: Unity 3D
Language: C#

================================================================================
                            M·ª§C L·ª§C
================================================================================

1. T·ªîNG QUAN KI·∫æN TR√öC AI
2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
4. SPECIALIZED AI STYLES SYSTEM
5. DECISION TREE CLASSIFIER
6. ENTROPY ANALYSIS & SOFTMAX
7. TRAINING SYSTEM & SELF-PLAY
8. AI TESTING FRAMEWORK
9. HYBRID AI ARCHITECTURE
10. PERFORMANCE OPTIMIZATION

================================================================================
                        1. T·ªîNG QUAN KI·∫æN TR√öC AI
================================================================================

D·ª± √°n AI Quoridor s·ª≠ d·ª•ng m·ªôt ki·∫øn tr√∫c AI hybrid ti√™n ti·∫øn, k·∫øt h·ª£p:

1.1 CORE ALGORITHMS:
- Q-Learning (Reinforcement Learning) - Thu·∫≠t to√°n ch√≠nh
- Minimax + Alpha-Beta Pruning - Thu·∫≠t to√°n fallback
- Decision Tree Classifier - Ph√¢n t√≠ch ng·ªØ c·∫£nh game
- Entropy Analysis - ƒê√°nh gi√° ƒëa d·∫°ng move

1.2 SPECIALIZED SYSTEMS:
- 6 AI Personality Styles v·ªõi training ri√™ng bi·ªát
- Self-Play Training Pipeline t·ª± ƒë·ªông
- Real-time Performance Testing Framework
- Dynamic Algorithm Switching

1.3 TECHNICAL FOUNDATION:
- State Space: 13,118+ unique game states
- Action Space: Movement + Wall placement combinations
- Training Method: Self-play v·ªõi 1000 episodes
- Learning Rate: Œ± = 0.1, Discount Factor: Œ≥ = 0.99

================================================================================
                    2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
================================================================================

2.1 GI·ªöI THI·ªÜU Q-LEARNING:
Q-Learning l√† thu·∫≠t to√°n Reinforcement Learning kh√¥ng c·∫ßn model, h·ªçc th√¥ng qua
t∆∞∆°ng t√°c v·ªõi m√¥i tr∆∞·ªùng ƒë·ªÉ t·ªëi ∆∞u h√≥a policy. Trong Quoridor, AI h·ªçc c√°ch
ch·ªçn moves t·ªëi ∆∞u d·ª±a tr√™n kinh nghi·ªám t·ª´ h√†ng ng√†n game.

2.2 C√îNG TH·ª®C TO√ÅN H·ªåC:
Q(s,a) = Q(s,a) + Œ±[r + Œ≥*max(Q(s',a')) - Q(s,a)]

Trong ƒë√≥:
- Q(s,a): Q-value c·ªßa state s v·ªõi action a
- Œ±: Learning rate (0.1) - t·ªëc ƒë·ªô h·ªçc
- r: Immediate reward t·ª´ action
- Œ≥: Discount factor (0.99) - t·∫ßm quan tr·ªçng future rewards
- s': Next state sau khi th·ª±c hi·ªán action
- max(Q(s',a')): Maximum Q-value c·ªßa next state

2.3 IMPLEMENTATION CHI TI·∫æT:

```csharp
public class QLearningAgent
{
    // Core Q-Table: Dictionary l∆∞u tr·ªØ Q-values
    private Dictionary<string, Dictionary<string, float>> qTable;
    
    // Hyperparameters
    private float learningRate = 0.1f;      // T·ªëc ƒë·ªô h·ªçc
    private float discountFactor = 0.99f;   // Discount future rewards
    private float epsilon = 0.995f;         // Exploration rate
    
    // Q-Learning Update Implementation
    public void UpdateQValue(string state, string action, float reward, string nextState)
    {
        // L·∫•y Q-value hi·ªán t·∫°i
        float currentQ = GetQValue(state, action);
        
        // T√¨m max Q-value c·ªßa next state
        float maxNextQ = GetMaxQValue(nextState);
        
        // √Åp d·ª•ng c√¥ng th·ª©c Q-Learning
        float newQ = currentQ + learningRate * (reward + discountFactor * maxNextQ - currentQ);
        
        // C·∫≠p nh·∫≠t Q-table
        SetQValue(state, action, newQ);
    }
}
```

2.4 STATE ENCODING:
Game state ƒë∆∞·ª£c encode th√†nh string duy nh·∫•t:
"aiPos_x,aiPos_y-humanPos_x,humanPos_y-aiWalls-humanWalls-wallsList"

V√≠ d·ª•: "4,8-4,0-9-10-7,4,False|5,6,True"
- AI ·ªü (4,8), Human ·ªü (4,0)
- AI c√≥ 9 walls, Human c√≥ 10 walls
- Walls ƒë√£ ƒë·∫∑t: (7,4,False), (5,6,True)

2.5 ACTION ENCODING:
Actions ƒë∆∞·ª£c encode th√†nh string:
- Movement: "M:x,y" (v√≠ d·ª•: "M:4,7")
- Wall: "W:x,y,horizontal" (v√≠ d·ª•: "W:3,5,True")

2.6 EPSILON-GREEDY STRATEGY:
```csharp
public AIMove ChooseAction(GameState state, List<AIMove> possibleMoves)
{
    // Exploration vs Exploitation trade-off
    if (Random.Range(0f, 1f) < epsilon)
    {
        // EXPLORATION: Ch·ªçn random ƒë·ªÉ kh√°m ph√°
        return possibleMoves[Random.Range(0, possibleMoves.Count)];
    }
    else
    {
        // EXPLOITATION: Ch·ªçn action c√≥ Q-value cao nh·∫•t
        return GetBestAction(state, possibleMoves);
    }
}
```

2.7 EPSILON DECAY:
Epsilon gi·∫£m d·∫ßn t·ª´ 0.995 ‚Üí 0.1 qua training:
- Ban ƒë·∫ßu: High exploration (99.5%)
- Cu·ªëi training: High exploitation (90%)

2.8 REWARD SYSTEM:
- Win game: +1000
- Lose game: -1000
- Move toward goal: +10
- Block opponent: +50
- Waste move: -5
- Strategic position: +15

================================================================================
                3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
================================================================================

3.1 GI·ªöI THI·ªÜU MINIMAX:
Minimax l√† thu·∫≠t to√°n game theory c·ªï ƒëi·ªÉn, t√¨m ki·∫øm move t·ªëi ∆∞u b·∫±ng c√°ch
d·ª± ƒëo√°n tr∆∞·ªõc nhi·ªÅu moves. Alpha-Beta pruning t·ªëi ∆∞u hi·ªáu su·∫•t b·∫±ng c√°ch
lo·∫°i b·ªè c√°c nh√°nh kh√¥ng c·∫ßn thi·∫øt.

3.2 NGUY√äN L√ù HO·∫†T ƒê·ªòNG:
- Maximizing Player (AI): T·ªëi ƒëa h√≥a score
- Minimizing Player (Human): T·ªëi thi·ªÉu h√≥a score AI
- Search depth: 4 levels (configurable)
- Pruning: Lo·∫°i b·ªè nh√°nh kh√¥ng promising

3.3 IMPLEMENTATION:

```csharp
float Minimax(GameState state, int depth, bool isMaximizing, float alpha, float beta)
{
    // Base case: Depth = 0 ho·∫∑c game over
    if (depth == 0 || IsGameOver(state))
        return EvaluatePosition(state);
    
    if (isMaximizing) // AI turn
    {
        float maxEval = float.NegativeInfinity;
        foreach (var move in GeneratePossibleMoves(state, true))
        {
            GameState newState = ApplyMove(state, move, true);
            float eval = Minimax(newState, depth - 1, false, alpha, beta);
            maxEval = Mathf.Max(maxEval, eval);
            alpha = Mathf.Max(alpha, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return maxEval;
    }
    else // Human turn
    {
        float minEval = float.PositiveInfinity;
        foreach (var move in GeneratePossibleMoves(state, false))
        {
            GameState newState = ApplyMove(state, move, false);
            float eval = Minimax(newState, depth - 1, true, alpha, beta);
            minEval = Mathf.Min(minEval, eval);
            beta = Mathf.Min(beta, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return minEval;
    }
}
```

3.4 EVALUATION FUNCTION:
Position evaluation d·ª±a tr√™n multiple factors:

```csharp
float EvaluatePosition(GameState state)
{
    float score = 0f;
    
    // Distance to goal (c√†ng g·∫ßn c√†ng t·ªët)
    int aiDistance = GetShortestPath(state.aiPosition, goalRow);
    int humanDistance = GetShortestPath(state.humanPosition, goalRow);
    score += (humanDistance - aiDistance) * 100;
    
    // Wall advantage
    score += (state.aiWallsLeft - state.humanWallsLeft) * 50;
    
    // Mobility (s·ªë moves c√≥ th·ªÉ)
    int aiMobility = GetValidMoves(state.aiPosition).Count;
    int humanMobility = GetValidMoves(state.humanPosition).Count;
    score += (aiMobility - humanMobility) * 20;
    
    // Center control
    score += GetCenterControlScore(state) * 30;
    
    return score;
}
```

3.5 ALPHA-BETA PRUNING HI·ªÜU QU·∫¢:
- Reduces search space t·ª´ O(b^d) xu·ªëng O(b^(d/2))
- Best case: 50% reduction in nodes evaluated
- Ordering moves by promising gi√∫p pruning hi·ªáu qu·∫£ h∆°n

================================================================================
                        4. SPECIALIZED AI STYLES SYSTEM
================================================================================

4.1 CONCEPT:
6 AI personalities v·ªõi training v√† behavior ri√™ng bi·ªát, m·ªói style c√≥:
- Ri√™ng Q-table file
- Custom reward structure
- Specific move prioritization
- Unique decision making logic

4.2 AI STYLES CHI TI·∫æT:

üî• AGGRESSIVE STYLE:
- M·ª•c ti√™u: T·∫•n c√¥ng v√† ch·∫∑n ƒë·ªëi th·ªß
- Wall Usage: 70% chance
- Reward Structure:
  * blockingOpponentReward = 50 * intensity
  * wallPlacementReward = 30 * intensity
  * defensiveWallPenalty = -20 * intensity
- Q-table: qtable_aggressive.json

üõ°Ô∏è DEFENSIVE STYLE:
- M·ª•c ti√™u: Ph√≤ng th·ªß v√† di chuy·ªÉn an to√†n
- Wall Usage: 50% chance
- Reward Structure:
  * defensiveWallReward = 35 * intensity
  * movementReward = 20 * intensity
  * aggressiveMovePenalty = -15 * intensity
- Q-table: qtable_defensive.json

üß± WALL MASTER STYLE:
- M·ª•c ti√™u: Chuy√™n gia ƒë·∫∑t t∆∞·ªùng v√† t·∫°o m√™ cung
- Wall Usage: 90% chance
- Reward Structure:
  * wallPlacementReward = 60 * intensity
  * strategicWallReward = 40 * intensity
  * wallComboReward = 50 * intensity
  * movementWithoutWallPenalty = -25 * intensity
- Q-table: qtable_wallmaster.json

‚ö° SPEED RUNNER STYLE:
- M·ª•c ti√™u: ƒê·∫°t ƒë√≠ch nhanh nh·∫•t
- Wall Usage: 10% chance
- Reward Structure:
  * movementReward = 40 * intensity
  * progressToGoalReward = 50 * intensity
  * quickWinBonus = 100 * intensity
  * wallPlacementPenalty = -20 * intensity
- Q-table: qtable_speedrunner.json

üß† TACTICAL STYLE:
- M·ª•c ti√™u: L·ª£i th·∫ø v·ªã tr√≠ v√† k·∫ø ho·∫°ch d√†i h·∫°n
- Wall Usage: Variable (strategic)
- Reward Structure:
  * tacticalMoveReward = 45 * intensity
  * positionAdvantageReward = 35 * intensity
  * mobilityReward = 25 * intensity
  * patternRecognitionBonus = 40 * intensity
- Q-table: qtable_tactical.json

‚öñÔ∏è BALANCED STYLE:
- M·ª•c ti√™u: C√¢n b·∫±ng t·∫•t c·∫£ aspects
- Wall Usage: 30% chance
- Reward Structure: Default balanced values
- Q-table: qtable.json

4.3 STYLE SWITCHING MECHANISM:

```csharp
public void SwitchToStyle(PlayStyle newStyle)
{
    // Save current Q-table
    if (allowQTableSaving) qAgent.SaveQTable(GetStyleSpecificQTablePath());
    
    // Load new style Q-table
    specializedStyle = newStyle;
    qAgent.LoadQTable(GetStyleSpecificQTablePath());
    
    // Reconfigure parameters
    ConfigureStyleSpecificTraining();
}
```

4.4 STYLE INTENSITY:
Parameter t·ª´ 0.0 ‚Üí 1.0 control m·ª©c ƒë·ªô chuy√™n h√≥a:
- 0.0: Balanced behavior
- 0.5: Moderate specialization
- 1.0: Extreme specialization

================================================================================
                        5. DECISION TREE CLASSIFIER
================================================================================

5.1 PURPOSE:
Decision Tree ph√¢n t√≠ch game context ƒë·ªÉ ch·ªçn strategy ph√π h·ª£p, d·ª±a tr√™n:
- Game phase (Early/Mid/End)
- Position advantage
- Resource advantage
- Mobility situation

5.2 FEATURE EXTRACTION:

```csharp
public struct DecisionFeatures
{
    public int aiDistanceToGoal;      // AI distance to goal line
    public int humanDistanceToGoal;   // Human distance to goal line
    public int aiWallsLeft;           // AI walls remaining
    public int humanWallsLeft;        // Human walls remaining
    public int aiMobility;            // AI possible moves
    public int humanMobility;         // Human possible moves
    public float distanceAdvantage;   // Human - AI distance
    public float wallAdvantage;       // AI - Human walls
    public bool isEarlyGame;          // > 15 moves estimated remaining
    public bool isMidGame;            // 5-15 moves remaining
    public bool isEndGame;            // < 5 moves remaining
}
```

5.3 DECISION LOGIC:

```csharp
public AIStrategy Predict(DecisionFeatures features)
{
    // Early Game: Play conservatively
    if (features.isEarlyGame)
        return AIStrategy.Balanced;
    
    // AI has significant lead: Focus on blocking
    if (features.distanceAdvantage > 2)
        return AIStrategy.Blocking;
    
    // AI is behind: Be aggressive
    if (features.distanceAdvantage < -1)
        return AIStrategy.Aggressive;
    
    // End game with wall advantage: Play defensively
    if (features.isEndGame && features.wallAdvantage > 2)
        return AIStrategy.Defensive;
    
    // Close game: Balanced approach
    return AIStrategy.Balanced;
}
```

5.4 STRATEGY OUTCOMES:
- Balanced: Equal weight to movement v√† walls
- Blocking: Prioritize moves that increase opponent path
- Aggressive: Focus on attacking positions
- Defensive: Protect own position and path

================================================================================
                        6. ENTROPY ANALYSIS & SOFTMAX
================================================================================

6.1 ENTROPY CONCEPT:
Entropy ƒëo ƒë·ªô ƒëa d·∫°ng/uncertainty c·ªßa move scores. High entropy = nhi·ªÅu moves
c√≥ scores t∆∞∆°ng ƒë∆∞∆°ng, low entropy = m·ªôt move r√µ r√†ng t·ªët nh·∫•t.

6.2 ENTROPY CALCULATION:

```csharp
float CalculateEntropy(List<float> moveScores)
{
    // Normalize th√†nh probabilities
    float total = moveScores.Sum();
    var probabilities = moveScores.Select(s => s / total);
    
    // Calculate Shannon entropy
    float entropy = 0f;
    foreach (float p in probabilities)
    {
        if (p > 0f)
            entropy -= p * Mathf.Log(p, 2f);
    }
    
    return entropy; // Range: 0 (deterministic) to log2(n) (uniform)
}
```

6.3 SOFTMAX DISTRIBUTION:

```csharp
float[] CalculateSoftmax(List<float> scores, float temperature)
{
    float[] probabilities = new float[scores.Count];
    float sum = scores.Sum(s => Mathf.Exp(s / temperature));
    
    for (int i = 0; i < scores.Count; i++)
    {
        probabilities[i] = Mathf.Exp(scores[i] / temperature) / sum;
    }
    
    return probabilities;
}
```

6.4 TEMPERATURE PARAMETER:
- Low temperature (0.1): Sharp distribution, focus on best moves
- High temperature (2.0): Flat distribution, more exploration
- Default temperature (1.0): Balanced exploitation/exploration

6.5 ADAPTIVE MOVE SELECTION:

```csharp
public AIMove ChooseActionWithEntropy(GameState state, List<AIMove> possibleMoves)
{
    var moveScores = EvaluateAllMoves(state, possibleMoves);
    float entropy = CalculateEntropy(moveScores);
    
    if (entropy > entropyThreshold) // High diversity
    {
        // Use softmax for probabilistic selection
        var probabilities = CalculateSoftmax(moveScores, softmaxTemperature);
        return SelectMoveByProbability(possibleMoves, probabilities);
    }
    else // Clear best move
    {
        // Use deterministic best move
        return GetBestScoredMove(possibleMoves, moveScores);
    }
}
```

================================================================================
                        7. TRAINING SYSTEM & SELF-PLAY
================================================================================

7.1 SELF-PLAY CONCEPT:
AI h·ªçc b·∫±ng c√°ch ch∆°i v·ªõi ch√≠nh n√≥ h√†ng ng√†n l·∫ßn, gradually improve performance
th√¥ng qua trial v√† error.

7.2 TRAINING PIPELINE:

```csharp
IEnumerator RunSelfPlayTraining()
{
    for (int episode = 1; episode <= maxEpisodes; episode++)
    {
        // Reset game state
        ResetGameForTraining();
        
        while (!IsGameOver())
        {
            // Get current state
            GameState currentState = GetCurrentGameState();
            List<AIMove> possibleMoves = GetPossibleMoves();
            
            // Choose action using Œµ-greedy
            AIMove chosenMove = qLearningAgent.ChooseAction(currentState, possibleMoves);
            
            // Execute move
            ExecuteMove(chosenMove);
            
            // Calculate reward
            float reward = CalculateReward(currentState, chosenMove);
            
            // Update Q-table
            GameState newState = GetCurrentGameState();
            qLearningAgent.UpdateQValue(
                EncodeState(currentState),
                EncodeAction(chosenMove),
                reward,
                EncodeState(newState)
            );
            
            yield return new WaitForEndOfFrame();
        }
        
        // Epsilon decay
        qLearningAgent.DecayEpsilon();
        
        // Save progress periodically
        if (episode % 100 == 0)
        {
            qLearningAgent.SaveQTable();
        }
    }
}
```

7.3 REWARD ENGINEERING:
Critical cho Q-Learning success:

```csharp
float CalculateReward(GameState state, AIMove move)
{
    float reward = 0f;
    
    // Game outcome rewards
    if (IsWin(state)) reward += 1000f;
    if (IsLoss(state)) reward -= 1000f;
    
    // Progress rewards
    if (move.moveType == MoveType.Movement)
    {
        if (IsMoveTowardGoal(move)) reward += 20f;
        else reward -= 5f;
    }
    
    // Strategic rewards
    if (move.moveType == MoveType.WallPlacement)
    {
        if (IsWallBlockingOpponent(move)) reward += 50f;
        if (IsWallStrategic(move)) reward += 30f;
        if (IsWallWasted(move)) reward -= 25f;
    }
    
    // Style-specific rewards
    reward += GetStyleSpecificReward(state, move);
    
    return reward;
}
```

7.4 TRAINING METRICS:
- Episodes: 1000 standard
- Duration: ~15 minutes
- Convergence: Usually around episode 800
- Q-table size: 13,118+ states at completion
- Final epsilon: 0.1 (90% exploitation)

================================================================================
                        8. AI TESTING FRAMEWORK
================================================================================

8.1 TESTING PHILOSOPHY:
Comprehensive testing ensures AI quality v√† reliability across different
game scenarios v√† edge cases.

8.2 TEST CATEGORIES:

A) DECISION TREE TESTS:
```csharp
void TestDecisionTreeStrategies()
{
    // Test Early Game
    var earlyGameFeatures = new DecisionFeatures
    {
        aiDistanceToGoal = 8, humanDistanceToGoal = 8,
        isEarlyGame = true
    };
    var strategy = decisionTree.Predict(earlyGameFeatures);
    Assert.AreEqual(AIStrategy.Balanced, strategy);
    
    // Test AI Leading
    var leadingFeatures = new DecisionFeatures
    {
        distanceAdvantage = 3, // AI leading
        isMidGame = true
    };
    strategy = decisionTree.Predict(leadingFeatures);
    Assert.AreEqual(AIStrategy.Blocking, strategy);
}
```

B) ENTROPY ANALYSIS TESTS:
```csharp
void TestEntropyAnalysis()
{
    // High entropy case (diverse scores)
    var highEntropyScores = new List<float> { 100f, 98f, 97f, 95f, 94f };
    float entropy = CalculateTestEntropy(highEntropyScores);
    Assert.IsTrue(entropy > 0.5f);
    
    // Low entropy case (clear winner)
    var lowEntropyScores = new List<float> { 100f, 50f, 50f, 50f, 50f };
    entropy = CalculateTestEntropy(lowEntropyScores);
    Assert.IsTrue(entropy < 0.5f);
}
```

C) SOFTMAX DISTRIBUTION TESTS:
```csharp
void TestSoftmaxDistribution()
{
    var testScores = new List<float> { 100f, 98f, 95f, 90f };
    float temperature = 1.0f;
    
    var probabilities = CalculateSoftmax(testScores, temperature);
    
    // Verify probabilities sum to 1
    float sum = probabilities.Sum();
    Assert.AreEqual(1.0f, sum, 0.001f);
    
    // Verify highest score gets highest probability
    int maxIndex = Array.IndexOf(probabilities, probabilities.Max());
    Assert.AreEqual(0, maxIndex);
}
```

D) PERFORMANCE BENCHMARKS:
- Move generation time: <100ms
- Q-Learning decision: <50ms
- Minimax decision (depth 4): <200ms
- Memory usage: <256MB

8.3 AUTOMATED TESTING:
Tests run automatically ƒë·ªÉ ensure code quality:
- Unit tests cho individual algorithms
- Integration tests cho AI system interactions
- Performance tests cho response times
- Regression tests ƒë·ªÉ prevent quality degradation

================================================================================
                        9. HYBRID AI ARCHITECTURE
================================================================================

9.1 CONCEPT:
Hybrid system k·∫øt h·ª£p strengths c·ªßa multiple algorithms:
- Q-Learning: Data-driven, learns from experience
- Minimax: Logical, guaranteed optimal given evaluation
- Decision Tree: Fast, context-aware switching
- Entropy: Adaptive, handles uncertainty

9.2 ALGORITHM SELECTION LOGIC:

```csharp
public AIMove GetBestMove()
{
    UpdateGameState();
    List<AIMove> possibleMoves = GeneratePossibleMoves();
    
    // Primary: Use Q-Learning if trained model available
    if (useQLearning && qAgent.IsTrainedModel())
    {
        Debug.Log($"üß† Using {specializedStyle} Q-Learning");
        return qAgent.ChooseAction(currentGameState, possibleMoves);
    }
    
    // Fallback: Use Minimax for guaranteed reasonable play
    else
    {
        Debug.Log($"üîç Using {specializedStyle} Minimax");
        return FindBestMoveWithMinimax();
    }
}
```

9.3 SEAMLESS SWITCHING:
- No interruption in gameplay
- Maintains consistent API
- Preserves game state integrity
- Logs algorithm used for debugging

9.4 BENEFITS:
- Robustness: Fallback ensures AI never fails
- Performance: Q-Learning for trained scenarios, Minimax for edge cases
- Flexibility: Can switch algorithms based on situation
- Development: Easy to test individual components

================================================================================
                        10. PERFORMANCE OPTIMIZATION
================================================================================

10.1 Q-TABLE OPTIMIZATION:
- Sparse representation: Only store visited states
- Compression: String interning for state keys
- Lazy loading: Load Q-table on demand
- Periodic saving: Every 100 training episodes

10.2 MINIMAX OPTIMIZATION:
- Alpha-Beta pruning: 50% search reduction
- Move ordering: Promising moves first
- Iterative deepening: Gradual depth increase
- Transposition tables: Cache evaluated positions

10.3 MEMORY MANAGEMENT:
- Object pooling cho GameState objects
- String caching cho repeated state/action encodings
- Garbage collection optimization
- Memory profiling v√† monitoring

10.4 MULTITHREADING CONSIDERATIONS:
- Q-Learning training: Background thread
- Move calculation: Main thread (Unity requirement)
- File I/O: Async operations where possible
- Thread-safe collections cho shared data

10.5 CACHING STRATEGIES:
- Path finding results
- Move generation results
- Evaluation function results
- Feature extraction results

================================================================================
                            K·∫æT LU·∫¨N
================================================================================

D·ª± √°n AI Quoridor Project th·ªÉ hi·ªán m·ªôt implementation comprehensive c·ªßa modern
game AI techniques:

TECHNICAL INNOVATIONS:
‚úÖ Hybrid Q-Learning + Minimax architecture
‚úÖ 6 specialized AI personalities v·ªõi separate training
‚úÖ Advanced decision making v·ªõi entropy analysis
‚úÖ Comprehensive testing v√† validation framework
‚úÖ Professional development tools v√† debugging

EDUCATIONAL VALUE:
‚úÖ Practical machine learning implementation
‚úÖ Traditional game AI algorithms
‚úÖ Modern software engineering practices
‚úÖ Performance optimization techniques
‚úÖ Automated testing methodologies

FUTURE ENHANCEMENTS:
üöÄ Deep Q-Learning (DQN) integration
üöÄ Multi-agent reinforcement learning
üöÄ Neural network-based evaluation functions
üöÄ Online learning v√† adaptation
üöÄ Transfer learning between game variants

D·ª± √°n n√†y demonstrating successful integration c·ªßa classical AI methods v·ªõi
modern machine learning, t·∫°o ra m·ªôt intelligent, adaptive, v√† highly
interactive gaming experience.

================================================================================
                            APPENDIX
================================================================================

FILES LI√äN QUAN:
- Assets/scripts/QLearningAgent.cs - Q-Learning implementation
- Assets/scripts/QuoridorAI.cs - Main AI controller
- Assets/scripts/DecisionTreeClassifier.cs - Decision tree logic
- Assets/scripts/QuoridorSelfPlayTrainer.cs - Training system
- Assets/scripts/AITestScript.cs - Testing framework
- Assets/scripts/Editor/ForceRetrainAI.cs - Development tools

Q-TABLE FILES:
- StreamingAssets/qtable.json - Balanced style
- StreamingAssets/qtable_aggressive.json - Aggressive style
- StreamingAssets/qtable_defensive.json - Defensive style
- StreamingAssets/qtable_wallmaster.json - Wall Master style
- StreamingAssets/qtable_speedrunner.json - Speed Runner style
- StreamingAssets/qtable_tactical.json - Tactical style

DOCUMENTATION:
- AI_ALGORITHMS_DETAILED_DESCRIPTION.txt - This file
- README.md - Project overview
- API_INTEGRATION_GUIDE.md - Voice/Chat integration
- TRAINING_GUIDE.md - AI training instructions

================================================================================
================================================================================
                    AI QUORIDOR GAME PROJECT - THU·∫¨T TO√ÅN AI CHI TI·∫æT
================================================================================

D·ª± √°n: AI Quoridor Game v·ªõi Machine Learning v√† Multi-Platform Integration
Ng√†y t·∫°o: 25/07/2025
T√°c gi·∫£: [Student Name]
Platform: Unity 3D
Language: C#

================================================================================
                            M·ª§C L·ª§C
================================================================================

1. T·ªîNG QUAN KI·∫æN TR√öC AI
2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
4. SPECIALIZED AI STYLES SYSTEM
5. DECISION TREE CLASSIFIER
6. ENTROPY ANALYSIS & SOFTMAX
7. TRAINING SYSTEM & SELF-PLAY
8. AI TESTING FRAMEWORK
9. HYBRID AI ARCHITECTURE
10. PERFORMANCE OPTIMIZATION

================================================================================
                        1. T·ªîNG QUAN KI·∫æN TR√öC AI
================================================================================

D·ª± √°n AI Quoridor s·ª≠ d·ª•ng m·ªôt ki·∫øn tr√∫c AI hybrid ti√™n ti·∫øn, k·∫øt h·ª£p:

1.1 CORE ALGORITHMS:
- Q-Learning (Reinforcement Learning) - Thu·∫≠t to√°n ch√≠nh
- Minimax + Alpha-Beta Pruning - Thu·∫≠t to√°n fallback
- Decision Tree Classifier - Ph√¢n t√≠ch ng·ªØ c·∫£nh game
- Entropy Analysis - ƒê√°nh gi√° ƒëa d·∫°ng move

1.2 SPECIALIZED SYSTEMS:
- 6 AI Personality Styles v·ªõi training ri√™ng bi·ªát
- Self-Play Training Pipeline t·ª± ƒë·ªông
- Real-time Performance Testing Framework
- Dynamic Algorithm Switching

1.3 TECHNICAL FOUNDATION:
- State Space: 13,118+ unique game states
- Action Space: Movement + Wall placement combinations
- Training Method: Self-play v·ªõi 1000 episodes
- Learning Rate: Œ± = 0.1, Discount Factor: Œ≥ = 0.99

================================================================================
                    2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
================================================================================

2.1 GI·ªöI THI·ªÜU Q-LEARNING:
Q-Learning l√† thu·∫≠t to√°n Reinforcement Learning kh√¥ng c·∫ßn model, h·ªçc th√¥ng qua
t∆∞∆°ng t√°c v·ªõi m√¥i tr∆∞·ªùng ƒë·ªÉ t·ªëi ∆∞u h√≥a policy. Trong Quoridor, AI h·ªçc c√°ch
ch·ªçn moves t·ªëi ∆∞u d·ª±a tr√™n kinh nghi·ªám t·ª´ h√†ng ng√†n game.

2.2 C√îNG TH·ª®C TO√ÅN H·ªåC:
Q(s,a) = Q(s,a) + Œ±[r + Œ≥*max(Q(s',a')) - Q(s,a)]

Trong ƒë√≥:
- Q(s,a): Q-value c·ªßa state s v·ªõi action a
- Œ±: Learning rate (0.1) - t·ªëc ƒë·ªô h·ªçc
- r: Immediate reward t·ª´ action
- Œ≥: Discount factor (0.99) - t·∫ßm quan tr·ªçng future rewards
- s': Next state sau khi th·ª±c hi·ªán action
- max(Q(s',a')): Maximum Q-value c·ªßa next state

2.3 IMPLEMENTATION CHI TI·∫æT:

```csharp
public class QLearningAgent
{
    // Core Q-Table: Dictionary l∆∞u tr·ªØ Q-values
    private Dictionary<string, Dictionary<string, float>> qTable;
    
    // Hyperparameters
    private float learningRate = 0.1f;      // T·ªëc ƒë·ªô h·ªçc
    private float discountFactor = 0.99f;   // Discount future rewards
    private float epsilon = 0.995f;         // Exploration rate
    
    // Q-Learning Update Implementation
    public void UpdateQValue(string state, string action, float reward, string nextState)
    {
        // L·∫•y Q-value hi·ªán t·∫°i
        float currentQ = GetQValue(state, action);
        
        // T√¨m max Q-value c·ªßa next state
        float maxNextQ = GetMaxQValue(nextState);
        
        // √Åp d·ª•ng c√¥ng th·ª©c Q-Learning
        float newQ = currentQ + learningRate * (reward + discountFactor * maxNextQ - currentQ);
        
        // C·∫≠p nh·∫≠t Q-table
        SetQValue(state, action, newQ);
    }
}
```

2.4 STATE ENCODING:
Game state ƒë∆∞·ª£c encode th√†nh string duy nh·∫•t:
"aiPos_x,aiPos_y-humanPos_x,humanPos_y-aiWalls-humanWalls-wallsList"

V√≠ d·ª•: "4,8-4,0-9-10-7,4,False|5,6,True"
- AI ·ªü (4,8), Human ·ªü (4,0)
- AI c√≥ 9 walls, Human c√≥ 10 walls
- Walls ƒë√£ ƒë·∫∑t: (7,4,False), (5,6,True)

2.5 ACTION ENCODING:
Actions ƒë∆∞·ª£c encode th√†nh string:
- Movement: "M:x,y" (v√≠ d·ª•: "M:4,7")
- Wall: "W:x,y,horizontal" (v√≠ d·ª•: "W:3,5,True")

2.6 EPSILON-GREEDY STRATEGY:
```csharp
public AIMove ChooseAction(GameState state, List<AIMove> possibleMoves)
{
    // Exploration vs Exploitation trade-off
    if (Random.Range(0f, 1f) < epsilon)
    {
        // EXPLORATION: Ch·ªçn random ƒë·ªÉ kh√°m ph√°
        return possibleMoves[Random.Range(0, possibleMoves.Count)];
    }
    else
    {
        // EXPLOITATION: Ch·ªçn action c√≥ Q-value cao nh·∫•t
        return GetBestAction(state, possibleMoves);
    }
}
```

2.7 EPSILON DECAY:
Epsilon gi·∫£m d·∫ßn t·ª´ 0.995 ‚Üí 0.1 qua training:
- Ban ƒë·∫ßu: High exploration (99.5%)
- Cu·ªëi training: High exploitation (90%)

2.8 REWARD SYSTEM:
- Win game: +1000
- Lose game: -1000
- Move toward goal: +10
- Block opponent: +50
- Waste move: -5
- Strategic position: +15

================================================================================
                3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
================================================================================

3.1 GI·ªöI THI·ªÜU MINIMAX:
Minimax l√† thu·∫≠t to√°n game theory c·ªï ƒëi·ªÉn, t√¨m ki·∫øm move t·ªëi ∆∞u b·∫±ng c√°ch
d·ª± ƒëo√°n tr∆∞·ªõc nhi·ªÅu moves. Alpha-Beta pruning t·ªëi ∆∞u hi·ªáu su·∫•t b·∫±ng c√°ch
lo·∫°i b·ªè c√°c nh√°nh kh√¥ng c·∫ßn thi·∫øt.

3.2 NGUY√äN L√ù HO·∫†T ƒê·ªòNG:
- Maximizing Player (AI): T·ªëi ƒëa h√≥a score
- Minimizing Player (Human): T·ªëi thi·ªÉu h√≥a score AI
- Search depth: 4 levels (configurable)
- Pruning: Lo·∫°i b·ªè nh√°nh kh√¥ng promising

3.3 IMPLEMENTATION:

```csharp
float Minimax(GameState state, int depth, bool isMaximizing, float alpha, float beta)
{
    // Base case: Depth = 0 ho·∫∑c game over
    if (depth == 0 || IsGameOver(state))
        return EvaluatePosition(state);
    
    if (isMaximizing) // AI turn
    {
        float maxEval = float.NegativeInfinity;
        foreach (var move in GeneratePossibleMoves(state, true))
        {
            GameState newState = ApplyMove(state, move, true);
            float eval = Minimax(newState, depth - 1, false, alpha, beta);
            maxEval = Mathf.Max(maxEval, eval);
            alpha = Mathf.Max(alpha, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return maxEval;
    }
    else // Human turn
    {
        float minEval = float.PositiveInfinity;
        foreach (var move in GeneratePossibleMoves(state, false))
        {
            GameState newState = ApplyMove(state, move, false);
            float eval = Minimax(newState, depth - 1, true, alpha, beta);
            minEval = Mathf.Min(minEval, eval);
            beta = Mathf.Min(beta, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return minEval;
    }
}
```

3.4 EVALUATION FUNCTION:
Position evaluation d·ª±a tr√™n multiple factors:

```csharp
float EvaluatePosition(GameState state)
{
    float score = 0f;
    
    // Distance to goal (c√†ng g·∫ßn c√†ng t·ªët)
    int aiDistance = GetShortestPath(state.aiPosition, goalRow);
    int humanDistance = GetShortestPath(state.humanPosition, goalRow);
    score += (humanDistance - aiDistance) * 100;
    
    // Wall advantage
    score += (state.aiWallsLeft - state.humanWallsLeft) * 50;
    
    // Mobility (s·ªë moves c√≥ th·ªÉ)
    int aiMobility = GetValidMoves(state.aiPosition).Count;
    int humanMobility = GetValidMoves(state.humanPosition).Count;
    score += (aiMobility - humanMobility) * 20;
    
    // Center control
    score += GetCenterControlScore(state) * 30;
    
    return score;
}
```

3.5 ALPHA-BETA PRUNING HI·ªÜU QU·∫¢:
- Reduces search space t·ª´ O(b^d) xu·ªëng O(b^(d/2))
- Best case: 50% reduction in nodes evaluated
- Ordering moves by promising gi√∫p pruning hi·ªáu qu·∫£ h∆°n

================================================================================
                        4. SPECIALIZED AI STYLES SYSTEM
================================================================================

4.1 CONCEPT:
6 AI personalities v·ªõi training v√† behavior ri√™ng bi·ªát, m·ªói style c√≥:
- Ri√™ng Q-table file
- Custom reward structure
- Specific move prioritization
- Unique decision making logic

4.2 AI STYLES CHI TI·∫æT:

üî• AGGRESSIVE STYLE:
- M·ª•c ti√™u: T·∫•n c√¥ng v√† ch·∫∑n ƒë·ªëi th·ªß
- Wall Usage: 70% chance
- Reward Structure:
  * blockingOpponentReward = 50 * intensity
  * wallPlacementReward = 30 * intensity
  * defensiveWallPenalty = -20 * intensity
- Q-table: qtable_aggressive.json

üõ°Ô∏è DEFENSIVE STYLE:
- M·ª•c ti√™u: Ph√≤ng th·ªß v√† di chuy·ªÉn an to√†n
- Wall Usage: 50% chance
- Reward Structure:
  * defensiveWallReward = 35 * intensity
  * movementReward = 20 * intensity
  * aggressiveMovePenalty = -15 * intensity
- Q-table: qtable_defensive.json

üß± WALL MASTER STYLE:
- M·ª•c ti√™u: Chuy√™n gia ƒë·∫∑t t∆∞·ªùng v√† t·∫°o m√™ cung
- Wall Usage: 90% chance
- Reward Structure:
  * wallPlacementReward = 60 * intensity
  * strategicWallReward = 40 * intensity
  * wallComboReward = 50 * intensity
  * movementWithoutWallPenalty = -25 * intensity
- Q-table: qtable_wallmaster.json

‚ö° SPEED RUNNER STYLE:
- M·ª•c ti√™u: ƒê·∫°t ƒë√≠ch nhanh nh·∫•t
- Wall Usage: 10% chance
- Reward Structure:
  * movementReward = 40 * intensity
  * progressToGoalReward = 50 * intensity
  * quickWinBonus = 100 * intensity
  * wallPlacementPenalty = -20 * intensity
- Q-table: qtable_speedrunner.json

üß† TACTICAL STYLE:
- M·ª•c ti√™u: L·ª£i th·∫ø v·ªã tr√≠ v√† k·∫ø ho·∫°ch d√†i h·∫°n
- Wall Usage: Variable (strategic)
- Reward Structure:
  * tacticalMoveReward = 45 * intensity
  * positionAdvantageReward = 35 * intensity
  * mobilityReward = 25 * intensity
  * patternRecognitionBonus = 40 * intensity
- Q-table: qtable_tactical.json

‚öñÔ∏è BALANCED STYLE:
- M·ª•c ti√™u: C√¢n b·∫±ng t·∫•t c·∫£ aspects
- Wall Usage: 30% chance
- Reward Structure: Default balanced values
- Q-table: qtable.json

4.3 STYLE SWITCHING MECHANISM:

```csharp
public void SwitchToStyle(PlayStyle newStyle)
{
    // Save current Q-table
    if (allowQTableSaving) qAgent.SaveQTable(GetStyleSpecificQTablePath());
    
    // Load new style Q-table
    specializedStyle = newStyle;
    qAgent.LoadQTable(GetStyleSpecificQTablePath());
    
    // Reconfigure parameters
    ConfigureStyleSpecificTraining();
}
```

4.4 STYLE INTENSITY:
Parameter t·ª´ 0.0 ‚Üí 1.0 control m·ª©c ƒë·ªô chuy√™n h√≥a:
- 0.0: Balanced behavior
- 0.5: Moderate specialization
- 1.0: Extreme specialization

================================================================================
                        5. DECISION TREE CLASSIFIER
================================================================================

5.1 PURPOSE:
Decision Tree ph√¢n t√≠ch game context ƒë·ªÉ ch·ªçn strategy ph√π h·ª£p, d·ª±a tr√™n:
- Game phase (Early/Mid/End)
- Position advantage
- Resource advantage
- Mobility situation

5.2 FEATURE EXTRACTION:

```csharp
public struct DecisionFeatures
{
    public int aiDistanceToGoal;      // AI distance to goal line
    public int humanDistanceToGoal;   // Human distance to goal line
    public int aiWallsLeft;           // AI walls remaining
    public int humanWallsLeft;        // Human walls remaining
    public int aiMobility;            // AI possible moves
    public int humanMobility;         // Human possible moves
    public float distanceAdvantage;   // Human - AI distance
    public float wallAdvantage;       // AI - Human walls
    public bool isEarlyGame;          // > 15 moves estimated remaining
    public bool isMidGame;            // 5-15 moves remaining
    public bool isEndGame;            // < 5 moves remaining
}
```

5.3 DECISION LOGIC:

```csharp
public AIStrategy Predict(DecisionFeatures features)
{
    // Early Game: Play conservatively
    if (features.isEarlyGame)
        return AIStrategy.Balanced;
    
    // AI has significant lead: Focus on blocking
    if (features.distanceAdvantage > 2)
        return AIStrategy.Blocking;
    
    // AI is behind: Be aggressive
    if (features.distanceAdvantage < -1)
        return AIStrategy.Aggressive;
    
    // End game with wall advantage: Play defensively
    if (features.isEndGame && features.wallAdvantage > 2)
        return AIStrategy.Defensive;
    
    // Close game: Balanced approach
    return AIStrategy.Balanced;
}
```

5.4 STRATEGY OUTCOMES:
- Balanced: Equal weight to movement v√† walls
- Blocking: Prioritize moves that increase opponent path
- Aggressive: Focus on attacking positions
- Defensive: Protect own position and path

================================================================================
                        6. ENTROPY ANALYSIS & SOFTMAX
================================================================================

6.1 ENTROPY CONCEPT:
Entropy ƒëo ƒë·ªô ƒëa d·∫°ng/uncertainty c·ªßa move scores. High entropy = nhi·ªÅu moves
c√≥ scores t∆∞∆°ng ƒë∆∞∆°ng, low entropy = m·ªôt move r√µ r√†ng t·ªët nh·∫•t.

6.2 ENTROPY CALCULATION:

```csharp
float CalculateEntropy(List<float> moveScores)
{
    // Normalize th√†nh probabilities
    float total = moveScores.Sum();
    var probabilities = moveScores.Select(s => s / total);
    
    // Calculate Shannon entropy
    float entropy = 0f;
    foreach (float p in probabilities)
    {
        if (p > 0f)
            entropy -= p * Mathf.Log(p, 2f);
    }
    
    return entropy; // Range: 0 (deterministic) to log2(n) (uniform)
}
```

6.3 SOFTMAX DISTRIBUTION:

```csharp
float[] CalculateSoftmax(List<float> scores, float temperature)
{
    float[] probabilities = new float[scores.Count];
    float sum = scores.Sum(s => Mathf.Exp(s / temperature));
    
    for (int i = 0; i < scores.Count; i++)
    {
        probabilities[i] = Mathf.Exp(scores[i] / temperature) / sum;
    }
    
    return probabilities;
}
```

6.4 TEMPERATURE PARAMETER:
- Low temperature (0.1): Sharp distribution, focus on best moves
- High temperature (2.0): Flat distribution, more exploration
- Default temperature (1.0): Balanced exploitation/exploration

6.5 ADAPTIVE MOVE SELECTION:

```csharp
public AIMove ChooseActionWithEntropy(GameState state, List<AIMove> possibleMoves)
{
    var moveScores = EvaluateAllMoves(state, possibleMoves);
    float entropy = CalculateEntropy(moveScores);
    
    if (entropy > entropyThreshold) // High diversity
    {
        // Use softmax for probabilistic selection
        var probabilities = CalculateSoftmax(moveScores, softmaxTemperature);
        return SelectMoveByProbability(possibleMoves, probabilities);
    }
    else // Clear best move
    {
        // Use deterministic best move
        return GetBestScoredMove(possibleMoves, moveScores);
    }
}
```

================================================================================
                        7. TRAINING SYSTEM & SELF-PLAY
================================================================================

7.1 SELF-PLAY CONCEPT:
AI h·ªçc b·∫±ng c√°ch ch∆°i v·ªõi ch√≠nh n√≥ h√†ng ng√†n l·∫ßn, gradually improve performance
th√¥ng qua trial v√† error.

7.2 TRAINING PIPELINE:

```csharp
IEnumerator RunSelfPlayTraining()
{
    for (int episode = 1; episode <= maxEpisodes; episode++)
    {
        // Reset game state
        ResetGameForTraining();
        
        while (!IsGameOver())
        {
            // Get current state
            GameState currentState = GetCurrentGameState();
            List<AIMove> possibleMoves = GetPossibleMoves();
            
            // Choose action using Œµ-greedy
            AIMove chosenMove = qLearningAgent.ChooseAction(currentState, possibleMoves);
            
            // Execute move
            ExecuteMove(chosenMove);
            
            // Calculate reward
            float reward = CalculateReward(currentState, chosenMove);
            
            // Update Q-table
            GameState newState = GetCurrentGameState();
            qLearningAgent.UpdateQValue(
                EncodeState(currentState),
                EncodeAction(chosenMove),
                reward,
                EncodeState(newState)
            );
            
            yield return new WaitForEndOfFrame();
        }
        
        // Epsilon decay
        qLearningAgent.DecayEpsilon();
        
        // Save progress periodically
        if (episode % 100 == 0)
        {
            qLearningAgent.SaveQTable();
        }
    }
}
```

7.3 REWARD ENGINEERING:
Critical cho Q-Learning success:

```csharp
float CalculateReward(GameState state, AIMove move)
{
    float reward = 0f;
    
    // Game outcome rewards
    if (IsWin(state)) reward += 1000f;
    if (IsLoss(state)) reward -= 1000f;
    
    // Progress rewards
    if (move.moveType == MoveType.Movement)
    {
        if (IsMoveTowardGoal(move)) reward += 20f;
        else reward -= 5f;
    }
    
    // Strategic rewards
    if (move.moveType == MoveType.WallPlacement)
    {
        if (IsWallBlockingOpponent(move)) reward += 50f;
        if (IsWallStrategic(move)) reward += 30f;
        if (IsWallWasted(move)) reward -= 25f;
    }
    
    // Style-specific rewards
    reward += GetStyleSpecificReward(state, move);
    
    return reward;
}
```

7.4 TRAINING METRICS:
- Episodes: 1000 standard
- Duration: ~15 minutes
- Convergence: Usually around episode 800
- Q-table size: 13,118+ states at completion
- Final epsilon: 0.1 (90% exploitation)

================================================================================
                        8. AI TESTING FRAMEWORK
================================================================================

8.1 TESTING PHILOSOPHY:
Comprehensive testing ensures AI quality v√† reliability across different
game scenarios v√† edge cases.

8.2 TEST CATEGORIES:

A) DECISION TREE TESTS:
```csharp
void TestDecisionTreeStrategies()
{
    // Test Early Game
    var earlyGameFeatures = new DecisionFeatures
    {
        aiDistanceToGoal = 8, humanDistanceToGoal = 8,
        isEarlyGame = true
    };
    var strategy = decisionTree.Predict(earlyGameFeatures);
    Assert.AreEqual(AIStrategy.Balanced, strategy);
    
    // Test AI Leading
    var leadingFeatures = new DecisionFeatures
    {
        distanceAdvantage = 3, // AI leading
        isMidGame = true
    };
    strategy = decisionTree.Predict(leadingFeatures);
    Assert.AreEqual(AIStrategy.Blocking, strategy);
}
```

B) ENTROPY ANALYSIS TESTS:
```csharp
void TestEntropyAnalysis()
{
    // High entropy case (diverse scores)
    var highEntropyScores = new List<float> { 100f, 98f, 97f, 95f, 94f };
    float entropy = CalculateTestEntropy(highEntropyScores);
    Assert.IsTrue(entropy > 0.5f);
    
    // Low entropy case (clear winner)
    var lowEntropyScores = new List<float> { 100f, 50f, 50f, 50f, 50f };
    entropy = CalculateTestEntropy(lowEntropyScores);
    Assert.IsTrue(entropy < 0.5f);
}
```

C) SOFTMAX DISTRIBUTION TESTS:
```csharp
void TestSoftmaxDistribution()
{
    var testScores = new List<float> { 100f, 98f, 95f, 90f };
    float temperature = 1.0f;
    
    var probabilities = CalculateSoftmax(testScores, temperature);
    
    // Verify probabilities sum to 1
    float sum = probabilities.Sum();
    Assert.AreEqual(1.0f, sum, 0.001f);
    
    // Verify highest score gets highest probability
    int maxIndex = Array.IndexOf(probabilities, probabilities.Max());
    Assert.AreEqual(0, maxIndex);
}
```

D) PERFORMANCE BENCHMARKS:
- Move generation time: <100ms
- Q-Learning decision: <50ms
- Minimax decision (depth 4): <200ms
- Memory usage: <256MB

8.3 AUTOMATED TESTING:
Tests run automatically ƒë·ªÉ ensure code quality:
- Unit tests cho individual algorithms
- Integration tests cho AI system interactions
- Performance tests cho response times
- Regression tests ƒë·ªÉ prevent quality degradation

================================================================================
                        9. HYBRID AI ARCHITECTURE
================================================================================

9.1 CONCEPT:
Hybrid system k·∫øt h·ª£p strengths c·ªßa multiple algorithms:
- Q-Learning: Data-driven, learns from experience
- Minimax: Logical, guaranteed optimal given evaluation
- Decision Tree: Fast, context-aware switching
- Entropy: Adaptive, handles uncertainty

9.2 ALGORITHM SELECTION LOGIC:

```csharp
public AIMove GetBestMove()
{
    UpdateGameState();
    List<AIMove> possibleMoves = GeneratePossibleMoves();
    
    // Primary: Use Q-Learning if trained model available
    if (useQLearning && qAgent.IsTrainedModel())
    {
        Debug.Log($"üß† Using {specializedStyle} Q-Learning");
        return qAgent.ChooseAction(currentGameState, possibleMoves);
    }
    
    // Fallback: Use Minimax for guaranteed reasonable play
    else
    {
        Debug.Log($"üîç Using {specializedStyle} Minimax");
        return FindBestMoveWithMinimax();
    }
}
```

9.3 SEAMLESS SWITCHING:
- No interruption in gameplay
- Maintains consistent API
- Preserves game state integrity
- Logs algorithm used for debugging

9.4 BENEFITS:
- Robustness: Fallback ensures AI never fails
- Performance: Q-Learning for trained scenarios, Minimax for edge cases
- Flexibility: Can switch algorithms based on situation
- Development: Easy to test individual components

================================================================================
                        10. PERFORMANCE OPTIMIZATION
================================================================================

10.1 Q-TABLE OPTIMIZATION:
- Sparse representation: Only store visited states
- Compression: String interning for state keys
- Lazy loading: Load Q-table on demand
- Periodic saving: Every 100 training episodes

10.2 MINIMAX OPTIMIZATION:
- Alpha-Beta pruning: 50% search reduction
- Move ordering: Promising moves first
- Iterative deepening: Gradual depth increase
- Transposition tables: Cache evaluated positions

10.3 MEMORY MANAGEMENT:
- Object pooling cho GameState objects
- String caching cho repeated state/action encodings
- Garbage collection optimization
- Memory profiling v√† monitoring

10.4 MULTITHREADING CONSIDERATIONS:
- Q-Learning training: Background thread
- Move calculation: Main thread (Unity requirement)
- File I/O: Async operations where possible
- Thread-safe collections cho shared data

10.5 CACHING STRATEGIES:
- Path finding results
- Move generation results
- Evaluation function results
- Feature extraction results

================================================================================
                            K·∫æT LU·∫¨N
================================================================================

D·ª± √°n AI Quoridor Project th·ªÉ hi·ªán m·ªôt implementation comprehensive c·ªßa modern
game AI techniques:

TECHNICAL INNOVATIONS:
‚úÖ Hybrid Q-Learning + Minimax architecture
‚úÖ 6 specialized AI personalities v·ªõi separate training
‚úÖ Advanced decision making v·ªõi entropy analysis
‚úÖ Comprehensive testing v√† validation framework
‚úÖ Professional development tools v√† debugging

EDUCATIONAL VALUE:
‚úÖ Practical machine learning implementation
‚úÖ Traditional game AI algorithms
‚úÖ Modern software engineering practices
‚úÖ Performance optimization techniques
‚úÖ Automated testing methodologies

FUTURE ENHANCEMENTS:
üöÄ Deep Q-Learning (DQN) integration
üöÄ Multi-agent reinforcement learning
üöÄ Neural network-based evaluation functions
üöÄ Online learning v√† adaptation
üöÄ Transfer learning between game variants

D·ª± √°n n√†y demonstrating successful integration c·ªßa classical AI methods v·ªõi
modern machine learning, t·∫°o ra m·ªôt intelligent, adaptive, v√† highly
interactive gaming experience.

================================================================================
                            APPENDIX
================================================================================

FILES LI√äN QUAN:
- Assets/scripts/QLearningAgent.cs - Q-Learning implementation
- Assets/scripts/QuoridorAI.cs - Main AI controller
- Assets/scripts/DecisionTreeClassifier.cs - Decision tree logic
- Assets/scripts/QuoridorSelfPlayTrainer.cs - Training system
- Assets/scripts/AITestScript.cs - Testing framework
- Assets/scripts/Editor/ForceRetrainAI.cs - Development tools

Q-TABLE FILES:
- StreamingAssets/qtable.json - Balanced style
- StreamingAssets/qtable_aggressive.json - Aggressive style
- StreamingAssets/qtable_defensive.json - Defensive style
- StreamingAssets/qtable_wallmaster.json - Wall Master style
- StreamingAssets/qtable_speedrunner.json - Speed Runner style
- StreamingAssets/qtable_tactical.json - Tactical style

DOCUMENTATION:
- AI_ALGORITHMS_DETAILED_DESCRIPTION.txt - This file
- README.md - Project overview
- API_INTEGRATION_GUIDE.md - Voice/Chat integration
- TRAINING_GUIDE.md - AI training instructions

================================================================================
================================================================================
                    AI QUORIDOR GAME PROJECT - THU·∫¨T TO√ÅN AI CHI TI·∫æT
================================================================================

D·ª± √°n: AI Quoridor Game v·ªõi Machine Learning v√† Multi-Platform Integration
Ng√†y t·∫°o: 25/07/2025
T√°c gi·∫£: [Student Name]
Platform: Unity 3D
Language: C#

================================================================================
                            M·ª§C L·ª§C
================================================================================

1. T·ªîNG QUAN KI·∫æN TR√öC AI
2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
4. SPECIALIZED AI STYLES SYSTEM
5. DECISION TREE CLASSIFIER
6. ENTROPY ANALYSIS & SOFTMAX
7. TRAINING SYSTEM & SELF-PLAY
8. AI TESTING FRAMEWORK
9. HYBRID AI ARCHITECTURE
10. PERFORMANCE OPTIMIZATION

================================================================================
                        1. T·ªîNG QUAN KI·∫æN TR√öC AI
================================================================================

D·ª± √°n AI Quoridor s·ª≠ d·ª•ng m·ªôt ki·∫øn tr√∫c AI hybrid ti√™n ti·∫øn, k·∫øt h·ª£p:

1.1 CORE ALGORITHMS:
- Q-Learning (Reinforcement Learning) - Thu·∫≠t to√°n ch√≠nh
- Minimax + Alpha-Beta Pruning - Thu·∫≠t to√°n fallback
- Decision Tree Classifier - Ph√¢n t√≠ch ng·ªØ c·∫£nh game
- Entropy Analysis - ƒê√°nh gi√° ƒëa d·∫°ng move

1.2 SPECIALIZED SYSTEMS:
- 6 AI Personality Styles v·ªõi training ri√™ng bi·ªát
- Self-Play Training Pipeline t·ª± ƒë·ªông
- Real-time Performance Testing Framework
- Dynamic Algorithm Switching

1.3 TECHNICAL FOUNDATION:
- State Space: 13,118+ unique game states
- Action Space: Movement + Wall placement combinations
- Training Method: Self-play v·ªõi 1000 episodes
- Learning Rate: Œ± = 0.1, Discount Factor: Œ≥ = 0.99

================================================================================
                    2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
================================================================================

2.1 GI·ªöI THI·ªÜU Q-LEARNING:
Q-Learning l√† thu·∫≠t to√°n Reinforcement Learning kh√¥ng c·∫ßn model, h·ªçc th√¥ng qua
t∆∞∆°ng t√°c v·ªõi m√¥i tr∆∞·ªùng ƒë·ªÉ t·ªëi ∆∞u h√≥a policy. Trong Quoridor, AI h·ªçc c√°ch
ch·ªçn moves t·ªëi ∆∞u d·ª±a tr√™n kinh nghi·ªám t·ª´ h√†ng ng√†n game.

2.2 C√îNG TH·ª®C TO√ÅN H·ªåC:
Q(s,a) = Q(s,a) + Œ±[r + Œ≥*max(Q(s',a')) - Q(s,a)]

Trong ƒë√≥:
- Q(s,a): Q-value c·ªßa state s v·ªõi action a
- Œ±: Learning rate (0.1) - t·ªëc ƒë·ªô h·ªçc
- r: Immediate reward t·ª´ action
- Œ≥: Discount factor (0.99) - t·∫ßm quan tr·ªçng future rewards
- s': Next state sau khi th·ª±c hi·ªán action
- max(Q(s',a')): Maximum Q-value c·ªßa next state

2.3 IMPLEMENTATION CHI TI·∫æT:

```csharp
public class QLearningAgent
{
    // Core Q-Table: Dictionary l∆∞u tr·ªØ Q-values
    private Dictionary<string, Dictionary<string, float>> qTable;
    
    // Hyperparameters
    private float learningRate = 0.1f;      // T·ªëc ƒë·ªô h·ªçc
    private float discountFactor = 0.99f;   // Discount future rewards
    private float epsilon = 0.995f;         // Exploration rate
    
    // Q-Learning Update Implementation
    public void UpdateQValue(string state, string action, float reward, string nextState)
    {
        // L·∫•y Q-value hi·ªán t·∫°i
        float currentQ = GetQValue(state, action);
        
        // T√¨m max Q-value c·ªßa next state
        float maxNextQ = GetMaxQValue(nextState);
        
        // √Åp d·ª•ng c√¥ng th·ª©c Q-Learning
        float newQ = currentQ + learningRate * (reward + discountFactor * maxNextQ - currentQ);
        
        // C·∫≠p nh·∫≠t Q-table
        SetQValue(state, action, newQ);
    }
}
```

2.4 STATE ENCODING:
Game state ƒë∆∞·ª£c encode th√†nh string duy nh·∫•t:
"aiPos_x,aiPos_y-humanPos_x,humanPos_y-aiWalls-humanWalls-wallsList"

V√≠ d·ª•: "4,8-4,0-9-10-7,4,False|5,6,True"
- AI ·ªü (4,8), Human ·ªü (4,0)
- AI c√≥ 9 walls, Human c√≥ 10 walls
- Walls ƒë√£ ƒë·∫∑t: (7,4,False), (5,6,True)

2.5 ACTION ENCODING:
Actions ƒë∆∞·ª£c encode th√†nh string:
- Movement: "M:x,y" (v√≠ d·ª•: "M:4,7")
- Wall: "W:x,y,horizontal" (v√≠ d·ª•: "W:3,5,True")

2.6 EPSILON-GREEDY STRATEGY:
```csharp
public AIMove ChooseAction(GameState state, List<AIMove> possibleMoves)
{
    // Exploration vs Exploitation trade-off
    if (Random.Range(0f, 1f) < epsilon)
    {
        // EXPLORATION: Ch·ªçn random ƒë·ªÉ kh√°m ph√°
        return possibleMoves[Random.Range(0, possibleMoves.Count)];
    }
    else
    {
        // EXPLOITATION: Ch·ªçn action c√≥ Q-value cao nh·∫•t
        return GetBestAction(state, possibleMoves);
    }
}
```

2.7 EPSILON DECAY:
Epsilon gi·∫£m d·∫ßn t·ª´ 0.995 ‚Üí 0.1 qua training:
- Ban ƒë·∫ßu: High exploration (99.5%)
- Cu·ªëi training: High exploitation (90%)

2.8 REWARD SYSTEM:
- Win game: +1000
- Lose game: -1000
- Move toward goal: +10
- Block opponent: +50
- Waste move: -5
- Strategic position: +15

================================================================================
                3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
================================================================================

3.1 GI·ªöI THI·ªÜU MINIMAX:
Minimax l√† thu·∫≠t to√°n game theory c·ªï ƒëi·ªÉn, t√¨m ki·∫øm move t·ªëi ∆∞u b·∫±ng c√°ch
d·ª± ƒëo√°n tr∆∞·ªõc nhi·ªÅu moves. Alpha-Beta pruning t·ªëi ∆∞u hi·ªáu su·∫•t b·∫±ng c√°ch
lo·∫°i b·ªè c√°c nh√°nh kh√¥ng c·∫ßn thi·∫øt.

3.2 NGUY√äN L√ù HO·∫†T ƒê·ªòNG:
- Maximizing Player (AI): T·ªëi ƒëa h√≥a score
- Minimizing Player (Human): T·ªëi thi·ªÉu h√≥a score AI
- Search depth: 4 levels (configurable)
- Pruning: Lo·∫°i b·ªè nh√°nh kh√¥ng promising

3.3 IMPLEMENTATION:

```csharp
float Minimax(GameState state, int depth, bool isMaximizing, float alpha, float beta)
{
    // Base case: Depth = 0 ho·∫∑c game over
    if (depth == 0 || IsGameOver(state))
        return EvaluatePosition(state);
    
    if (isMaximizing) // AI turn
    {
        float maxEval = float.NegativeInfinity;
        foreach (var move in GeneratePossibleMoves(state, true))
        {
            GameState newState = ApplyMove(state, move, true);
            float eval = Minimax(newState, depth - 1, false, alpha, beta);
            maxEval = Mathf.Max(maxEval, eval);
            alpha = Mathf.Max(alpha, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return maxEval;
    }
    else // Human turn
    {
        float minEval = float.PositiveInfinity;
        foreach (var move in GeneratePossibleMoves(state, false))
        {
            GameState newState = ApplyMove(state, move, false);
            float eval = Minimax(newState, depth - 1, true, alpha, beta);
            minEval = Mathf.Min(minEval, eval);
            beta = Mathf.Min(beta, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return minEval;
    }
}
```

3.4 EVALUATION FUNCTION:
Position evaluation d·ª±a tr√™n multiple factors:

```csharp
float EvaluatePosition(GameState state)
{
    float score = 0f;
    
    // Distance to goal (c√†ng g·∫ßn c√†ng t·ªët)
    int aiDistance = GetShortestPath(state.aiPosition, goalRow);
    int humanDistance = GetShortestPath(state.humanPosition, goalRow);
    score += (humanDistance - aiDistance) * 100;
    
    // Wall advantage
    score += (state.aiWallsLeft - state.humanWallsLeft) * 50;
    
    // Mobility (s·ªë moves c√≥ th·ªÉ)
    int aiMobility = GetValidMoves(state.aiPosition).Count;
    int humanMobility = GetValidMoves(state.humanPosition).Count;
    score += (aiMobility - humanMobility) * 20;
    
    // Center control
    score += GetCenterControlScore(state) * 30;
    
    return score;
}
```

3.5 ALPHA-BETA PRUNING HI·ªÜU QU·∫¢:
- Reduces search space t·ª´ O(b^d) xu·ªëng O(b^(d/2))
- Best case: 50% reduction in nodes evaluated
- Ordering moves by promising gi√∫p pruning hi·ªáu qu·∫£ h∆°n

================================================================================
                        4. SPECIALIZED AI STYLES SYSTEM
================================================================================

4.1 CONCEPT:
6 AI personalities v·ªõi training v√† behavior ri√™ng bi·ªát, m·ªói style c√≥:
- Ri√™ng Q-table file
- Custom reward structure
- Specific move prioritization
- Unique decision making logic

4.2 AI STYLES CHI TI·∫æT:

üî• AGGRESSIVE STYLE:
- M·ª•c ti√™u: T·∫•n c√¥ng v√† ch·∫∑n ƒë·ªëi th·ªß
- Wall Usage: 70% chance
- Reward Structure:
  * blockingOpponentReward = 50 * intensity
  * wallPlacementReward = 30 * intensity
  * defensiveWallPenalty = -20 * intensity
- Q-table: qtable_aggressive.json

üõ°Ô∏è DEFENSIVE STYLE:
- M·ª•c ti√™u: Ph√≤ng th·ªß v√† di chuy·ªÉn an to√†n
- Wall Usage: 50% chance
- Reward Structure:
  * defensiveWallReward = 35 * intensity
  * movementReward = 20 * intensity
  * aggressiveMovePenalty = -15 * intensity
- Q-table: qtable_defensive.json

üß± WALL MASTER STYLE:
- M·ª•c ti√™u: Chuy√™n gia ƒë·∫∑t t∆∞·ªùng v√† t·∫°o m√™ cung
- Wall Usage: 90% chance
- Reward Structure:
  * wallPlacementReward = 60 * intensity
  * strategicWallReward = 40 * intensity
  * wallComboReward = 50 * intensity
  * movementWithoutWallPenalty = -25 * intensity
- Q-table: qtable_wallmaster.json

‚ö° SPEED RUNNER STYLE:
- M·ª•c ti√™u: ƒê·∫°t ƒë√≠ch nhanh nh·∫•t
- Wall Usage: 10% chance
- Reward Structure:
  * movementReward = 40 * intensity
  * progressToGoalReward = 50 * intensity
  * quickWinBonus = 100 * intensity
  * wallPlacementPenalty = -20 * intensity
- Q-table: qtable_speedrunner.json

üß† TACTICAL STYLE:
- M·ª•c ti√™u: L·ª£i th·∫ø v·ªã tr√≠ v√† k·∫ø ho·∫°ch d√†i h·∫°n
- Wall Usage: Variable (strategic)
- Reward Structure:
  * tacticalMoveReward = 45 * intensity
  * positionAdvantageReward = 35 * intensity
  * mobilityReward = 25 * intensity
  * patternRecognitionBonus = 40 * intensity
- Q-table: qtable_tactical.json

‚öñÔ∏è BALANCED STYLE:
- M·ª•c ti√™u: C√¢n b·∫±ng t·∫•t c·∫£ aspects
- Wall Usage: 30% chance
- Reward Structure: Default balanced values
- Q-table: qtable.json

4.3 STYLE SWITCHING MECHANISM:

```csharp
public void SwitchToStyle(PlayStyle newStyle)
{
    // Save current Q-table
    if (allowQTableSaving) qAgent.SaveQTable(GetStyleSpecificQTablePath());
    
    // Load new style Q-table
    specializedStyle = newStyle;
    qAgent.LoadQTable(GetStyleSpecificQTablePath());
    
    // Reconfigure parameters
    ConfigureStyleSpecificTraining();
}
```

4.4 STYLE INTENSITY:
Parameter t·ª´ 0.0 ‚Üí 1.0 control m·ª©c ƒë·ªô chuy√™n h√≥a:
- 0.0: Balanced behavior
- 0.5: Moderate specialization
- 1.0: Extreme specialization

================================================================================
                        5. DECISION TREE CLASSIFIER
================================================================================

5.1 PURPOSE:
Decision Tree ph√¢n t√≠ch game context ƒë·ªÉ ch·ªçn strategy ph√π h·ª£p, d·ª±a tr√™n:
- Game phase (Early/Mid/End)
- Position advantage
- Resource advantage
- Mobility situation

5.2 FEATURE EXTRACTION:

```csharp
public struct DecisionFeatures
{
    public int aiDistanceToGoal;      // AI distance to goal line
    public int humanDistanceToGoal;   // Human distance to goal line
    public int aiWallsLeft;           // AI walls remaining
    public int humanWallsLeft;        // Human walls remaining
    public int aiMobility;            // AI possible moves
    public int humanMobility;         // Human possible moves
    public float distanceAdvantage;   // Human - AI distance
    public float wallAdvantage;       // AI - Human walls
    public bool isEarlyGame;          // > 15 moves estimated remaining
    public bool isMidGame;            // 5-15 moves remaining
    public bool isEndGame;            // < 5 moves remaining
}
```

5.3 DECISION LOGIC:

```csharp
public AIStrategy Predict(DecisionFeatures features)
{
    // Early Game: Play conservatively
    if (features.isEarlyGame)
        return AIStrategy.Balanced;
    
    // AI has significant lead: Focus on blocking
    if (features.distanceAdvantage > 2)
        return AIStrategy.Blocking;
    
    // AI is behind: Be aggressive
    if (features.distanceAdvantage < -1)
        return AIStrategy.Aggressive;
    
    // End game with wall advantage: Play defensively
    if (features.isEndGame && features.wallAdvantage > 2)
        return AIStrategy.Defensive;
    
    // Close game: Balanced approach
    return AIStrategy.Balanced;
}
```

5.4 STRATEGY OUTCOMES:
- Balanced: Equal weight to movement v√† walls
- Blocking: Prioritize moves that increase opponent path
- Aggressive: Focus on attacking positions
- Defensive: Protect own position and path

================================================================================
                        6. ENTROPY ANALYSIS & SOFTMAX
================================================================================

6.1 ENTROPY CONCEPT:
Entropy ƒëo ƒë·ªô ƒëa d·∫°ng/uncertainty c·ªßa move scores. High entropy = nhi·ªÅu moves
c√≥ scores t∆∞∆°ng ƒë∆∞∆°ng, low entropy = m·ªôt move r√µ r√†ng t·ªët nh·∫•t.

6.2 ENTROPY CALCULATION:

```csharp
float CalculateEntropy(List<float> moveScores)
{
    // Normalize th√†nh probabilities
    float total = moveScores.Sum();
    var probabilities = moveScores.Select(s => s / total);
    
    // Calculate Shannon entropy
    float entropy = 0f;
    foreach (float p in probabilities)
    {
        if (p > 0f)
            entropy -= p * Mathf.Log(p, 2f);
    }
    
    return entropy; // Range: 0 (deterministic) to log2(n) (uniform)
}
```

6.3 SOFTMAX DISTRIBUTION:

```csharp
float[] CalculateSoftmax(List<float> scores, float temperature)
{
    float[] probabilities = new float[scores.Count];
    float sum = scores.Sum(s => Mathf.Exp(s / temperature));
    
    for (int i = 0; i < scores.Count; i++)
    {
        probabilities[i] = Mathf.Exp(scores[i] / temperature) / sum;
    }
    
    return probabilities;
}
```

6.4 TEMPERATURE PARAMETER:
- Low temperature (0.1): Sharp distribution, focus on best moves
- High temperature (2.0): Flat distribution, more exploration
- Default temperature (1.0): Balanced exploitation/exploration

6.5 ADAPTIVE MOVE SELECTION:

```csharp
public AIMove ChooseActionWithEntropy(GameState state, List<AIMove> possibleMoves)
{
    var moveScores = EvaluateAllMoves(state, possibleMoves);
    float entropy = CalculateEntropy(moveScores);
    
    if (entropy > entropyThreshold) // High diversity
    {
        // Use softmax for probabilistic selection
        var probabilities = CalculateSoftmax(moveScores, softmaxTemperature);
        return SelectMoveByProbability(possibleMoves, probabilities);
    }
    else // Clear best move
    {
        // Use deterministic best move
        return GetBestScoredMove(possibleMoves, moveScores);
    }
}
```

================================================================================
                        7. TRAINING SYSTEM & SELF-PLAY
================================================================================

7.1 SELF-PLAY CONCEPT:
AI h·ªçc b·∫±ng c√°ch ch∆°i v·ªõi ch√≠nh n√≥ h√†ng ng√†n l·∫ßn, gradually improve performance
th√¥ng qua trial v√† error.

7.2 TRAINING PIPELINE:

```csharp
IEnumerator RunSelfPlayTraining()
{
    for (int episode = 1; episode <= maxEpisodes; episode++)
    {
        // Reset game state
        ResetGameForTraining();
        
        while (!IsGameOver())
        {
            // Get current state
            GameState currentState = GetCurrentGameState();
            List<AIMove> possibleMoves = GetPossibleMoves();
            
            // Choose action using Œµ-greedy
            AIMove chosenMove = qLearningAgent.ChooseAction(currentState, possibleMoves);
            
            // Execute move
            ExecuteMove(chosenMove);
            
            // Calculate reward
            float reward = CalculateReward(currentState, chosenMove);
            
            // Update Q-table
            GameState newState = GetCurrentGameState();
            qLearningAgent.UpdateQValue(
                EncodeState(currentState),
                EncodeAction(chosenMove),
                reward,
                EncodeState(newState)
            );
            
            yield return new WaitForEndOfFrame();
        }
        
        // Epsilon decay
        qLearningAgent.DecayEpsilon();
        
        // Save progress periodically
        if (episode % 100 == 0)
        {
            qLearningAgent.SaveQTable();
        }
    }
}
```

7.3 REWARD ENGINEERING:
Critical cho Q-Learning success:

```csharp
float CalculateReward(GameState state, AIMove move)
{
    float reward = 0f;
    
    // Game outcome rewards
    if (IsWin(state)) reward += 1000f;
    if (IsLoss(state)) reward -= 1000f;
    
    // Progress rewards
    if (move.moveType == MoveType.Movement)
    {
        if (IsMoveTowardGoal(move)) reward += 20f;
        else reward -= 5f;
    }
    
    // Strategic rewards
    if (move.moveType == MoveType.WallPlacement)
    {
        if (IsWallBlockingOpponent(move)) reward += 50f;
        if (IsWallStrategic(move)) reward += 30f;
        if (IsWallWasted(move)) reward -= 25f;
    }
    
    // Style-specific rewards
    reward += GetStyleSpecificReward(state, move);
    
    return reward;
}
```

7.4 TRAINING METRICS:
- Episodes: 1000 standard
- Duration: ~15 minutes
- Convergence: Usually around episode 800
- Q-table size: 13,118+ states at completion
- Final epsilon: 0.1 (90% exploitation)

================================================================================
                        8. AI TESTING FRAMEWORK
================================================================================

8.1 TESTING PHILOSOPHY:
Comprehensive testing ensures AI quality v√† reliability across different
game scenarios v√† edge cases.

8.2 TEST CATEGORIES:

A) DECISION TREE TESTS:
```csharp
void TestDecisionTreeStrategies()
{
    // Test Early Game
    var earlyGameFeatures = new DecisionFeatures
    {
        aiDistanceToGoal = 8, humanDistanceToGoal = 8,
        isEarlyGame = true
    };
    var strategy = decisionTree.Predict(earlyGameFeatures);
    Assert.AreEqual(AIStrategy.Balanced, strategy);
    
    // Test AI Leading
    var leadingFeatures = new DecisionFeatures
    {
        distanceAdvantage = 3, // AI leading
        isMidGame = true
    };
    strategy = decisionTree.Predict(leadingFeatures);
    Assert.AreEqual(AIStrategy.Blocking, strategy);
}
```

B) ENTROPY ANALYSIS TESTS:
```csharp
void TestEntropyAnalysis()
{
    // High entropy case (diverse scores)
    var highEntropyScores = new List<float> { 100f, 98f, 97f, 95f, 94f };
    float entropy = CalculateTestEntropy(highEntropyScores);
    Assert.IsTrue(entropy > 0.5f);
    
    // Low entropy case (clear winner)
    var lowEntropyScores = new List<float> { 100f, 50f, 50f, 50f, 50f };
    entropy = CalculateTestEntropy(lowEntropyScores);
    Assert.IsTrue(entropy < 0.5f);
}
```

C) SOFTMAX DISTRIBUTION TESTS:
```csharp
void TestSoftmaxDistribution()
{
    var testScores = new List<float> { 100f, 98f, 95f, 90f };
    float temperature = 1.0f;
    
    var probabilities = CalculateSoftmax(testScores, temperature);
    
    // Verify probabilities sum to 1
    float sum = probabilities.Sum();
    Assert.AreEqual(1.0f, sum, 0.001f);
    
    // Verify highest score gets highest probability
    int maxIndex = Array.IndexOf(probabilities, probabilities.Max());
    Assert.AreEqual(0, maxIndex);
}
```

D) PERFORMANCE BENCHMARKS:
- Move generation time: <100ms
- Q-Learning decision: <50ms
- Minimax decision (depth 4): <200ms
- Memory usage: <256MB

8.3 AUTOMATED TESTING:
Tests run automatically ƒë·ªÉ ensure code quality:
- Unit tests cho individual algorithms
- Integration tests cho AI system interactions
- Performance tests cho response times
- Regression tests ƒë·ªÉ prevent quality degradation

================================================================================
                        9. HYBRID AI ARCHITECTURE
================================================================================

9.1 CONCEPT:
Hybrid system k·∫øt h·ª£p strengths c·ªßa multiple algorithms:
- Q-Learning: Data-driven, learns from experience
- Minimax: Logical, guaranteed optimal given evaluation
- Decision Tree: Fast, context-aware switching
- Entropy: Adaptive, handles uncertainty

9.2 ALGORITHM SELECTION LOGIC:

```csharp
public AIMove GetBestMove()
{
    UpdateGameState();
    List<AIMove> possibleMoves = GeneratePossibleMoves();
    
    // Primary: Use Q-Learning if trained model available
    if (useQLearning && qAgent.IsTrainedModel())
    {
        Debug.Log($"üß† Using {specializedStyle} Q-Learning");
        return qAgent.ChooseAction(currentGameState, possibleMoves);
    }
    
    // Fallback: Use Minimax for guaranteed reasonable play
    else
    {
        Debug.Log($"üîç Using {specializedStyle} Minimax");
        return FindBestMoveWithMinimax();
    }
}
```

9.3 SEAMLESS SWITCHING:
- No interruption in gameplay
- Maintains consistent API
- Preserves game state integrity
- Logs algorithm used for debugging

9.4 BENEFITS:
- Robustness: Fallback ensures AI never fails
- Performance: Q-Learning for trained scenarios, Minimax for edge cases
- Flexibility: Can switch algorithms based on situation
- Development: Easy to test individual components

================================================================================
                        10. PERFORMANCE OPTIMIZATION
================================================================================

10.1 Q-TABLE OPTIMIZATION:
- Sparse representation: Only store visited states
- Compression: String interning for state keys
- Lazy loading: Load Q-table on demand
- Periodic saving: Every 100 training episodes

10.2 MINIMAX OPTIMIZATION:
- Alpha-Beta pruning: 50% search reduction
- Move ordering: Promising moves first
- Iterative deepening: Gradual depth increase
- Transposition tables: Cache evaluated positions

10.3 MEMORY MANAGEMENT:
- Object pooling cho GameState objects
- String caching cho repeated state/action encodings
- Garbage collection optimization
- Memory profiling v√† monitoring

10.4 MULTITHREADING CONSIDERATIONS:
- Q-Learning training: Background thread
- Move calculation: Main thread (Unity requirement)
- File I/O: Async operations where possible
- Thread-safe collections cho shared data

10.5 CACHING STRATEGIES:
- Path finding results
- Move generation results
- Evaluation function results
- Feature extraction results

================================================================================
                            K·∫æT LU·∫¨N
================================================================================

D·ª± √°n AI Quoridor Project th·ªÉ hi·ªán m·ªôt implementation comprehensive c·ªßa modern
game AI techniques:

TECHNICAL INNOVATIONS:
‚úÖ Hybrid Q-Learning + Minimax architecture
‚úÖ 6 specialized AI personalities v·ªõi separate training
‚úÖ Advanced decision making v·ªõi entropy analysis
‚úÖ Comprehensive testing v√† validation framework
‚úÖ Professional development tools v√† debugging

EDUCATIONAL VALUE:
‚úÖ Practical machine learning implementation
‚úÖ Traditional game AI algorithms
‚úÖ Modern software engineering practices
‚úÖ Performance optimization techniques
‚úÖ Automated testing methodologies

FUTURE ENHANCEMENTS:
üöÄ Deep Q-Learning (DQN) integration
üöÄ Multi-agent reinforcement learning
üöÄ Neural network-based evaluation functions
üöÄ Online learning v√† adaptation
üöÄ Transfer learning between game variants

D·ª± √°n n√†y demonstrating successful integration c·ªßa classical AI methods v·ªõi
modern machine learning, t·∫°o ra m·ªôt intelligent, adaptive, v√† highly
interactive gaming experience.

================================================================================
                            APPENDIX
================================================================================

FILES LI√äN QUAN:
- Assets/scripts/QLearningAgent.cs - Q-Learning implementation
- Assets/scripts/QuoridorAI.cs - Main AI controller
- Assets/scripts/DecisionTreeClassifier.cs - Decision tree logic
- Assets/scripts/QuoridorSelfPlayTrainer.cs - Training system
- Assets/scripts/AITestScript.cs - Testing framework
- Assets/scripts/Editor/ForceRetrainAI.cs - Development tools

Q-TABLE FILES:
- StreamingAssets/qtable.json - Balanced style
- StreamingAssets/qtable_aggressive.json - Aggressive style
- StreamingAssets/qtable_defensive.json - Defensive style
- StreamingAssets/qtable_wallmaster.json - Wall Master style
- StreamingAssets/qtable_speedrunner.json - Speed Runner style
- StreamingAssets/qtable_tactical.json - Tactical style

DOCUMENTATION:
- AI_ALGORITHMS_DETAILED_DESCRIPTION.txt - This file
- README.md - Project overview
- API_INTEGRATION_GUIDE.md - Voice/Chat integration
- TRAINING_GUIDE.md - AI training instructions

================================================================================
================================================================================
                    AI QUORIDOR GAME PROJECT - THU·∫¨T TO√ÅN AI CHI TI·∫æT
================================================================================

D·ª± √°n: AI Quoridor Game v·ªõi Machine Learning v√† Multi-Platform Integration
Ng√†y t·∫°o: 25/07/2025
T√°c gi·∫£: [Student Name]
Platform: Unity 3D
Language: C#

================================================================================
                            M·ª§C L·ª§C
================================================================================

1. T·ªîNG QUAN KI·∫æN TR√öC AI
2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
4. SPECIALIZED AI STYLES SYSTEM
5. DECISION TREE CLASSIFIER
6. ENTROPY ANALYSIS & SOFTMAX
7. TRAINING SYSTEM & SELF-PLAY
8. AI TESTING FRAMEWORK
9. HYBRID AI ARCHITECTURE
10. PERFORMANCE OPTIMIZATION

================================================================================
                        1. T·ªîNG QUAN KI·∫æN TR√öC AI
================================================================================

D·ª± √°n AI Quoridor s·ª≠ d·ª•ng m·ªôt ki·∫øn tr√∫c AI hybrid ti√™n ti·∫øn, k·∫øt h·ª£p:

1.1 CORE ALGORITHMS:
- Q-Learning (Reinforcement Learning) - Thu·∫≠t to√°n ch√≠nh
- Minimax + Alpha-Beta Pruning - Thu·∫≠t to√°n fallback
- Decision Tree Classifier - Ph√¢n t√≠ch ng·ªØ c·∫£nh game
- Entropy Analysis - ƒê√°nh gi√° ƒëa d·∫°ng move

1.2 SPECIALIZED SYSTEMS:
- 6 AI Personality Styles v·ªõi training ri√™ng bi·ªát
- Self-Play Training Pipeline t·ª± ƒë·ªông
- Real-time Performance Testing Framework
- Dynamic Algorithm Switching

1.3 TECHNICAL FOUNDATION:
- State Space: 13,118+ unique game states
- Action Space: Movement + Wall placement combinations
- Training Method: Self-play v·ªõi 1000 episodes
- Learning Rate: Œ± = 0.1, Discount Factor: Œ≥ = 0.99

================================================================================
                    2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
================================================================================

2.1 GI·ªöI THI·ªÜU Q-LEARNING:
Q-Learning l√† thu·∫≠t to√°n Reinforcement Learning kh√¥ng c·∫ßn model, h·ªçc th√¥ng qua
t∆∞∆°ng t√°c v·ªõi m√¥i tr∆∞·ªùng ƒë·ªÉ t·ªëi ∆∞u h√≥a policy. Trong Quoridor, AI h·ªçc c√°ch
ch·ªçn moves t·ªëi ∆∞u d·ª±a tr√™n kinh nghi·ªám t·ª´ h√†ng ng√†n game.

2.2 C√îNG TH·ª®C TO√ÅN H·ªåC:
Q(s,a) = Q(s,a) + Œ±[r + Œ≥*max(Q(s',a')) - Q(s,a)]

Trong ƒë√≥:
- Q(s,a): Q-value c·ªßa state s v·ªõi action a
- Œ±: Learning rate (0.1) - t·ªëc ƒë·ªô h·ªçc
- r: Immediate reward t·ª´ action
- Œ≥: Discount factor (0.99) - t·∫ßm quan tr·ªçng future rewards
- s': Next state sau khi th·ª±c hi·ªán action
- max(Q(s',a')): Maximum Q-value c·ªßa next state

2.3 IMPLEMENTATION CHI TI·∫æT:

```csharp
public class QLearningAgent
{
    // Core Q-Table: Dictionary l∆∞u tr·ªØ Q-values
    private Dictionary<string, Dictionary<string, float>> qTable;
    
    // Hyperparameters
    private float learningRate = 0.1f;      // T·ªëc ƒë·ªô h·ªçc
    private float discountFactor = 0.99f;   // Discount future rewards
    private float epsilon = 0.995f;         // Exploration rate
    
    // Q-Learning Update Implementation
    public void UpdateQValue(string state, string action, float reward, string nextState)
    {
        // L·∫•y Q-value hi·ªán t·∫°i
        float currentQ = GetQValue(state, action);
        
        // T√¨m max Q-value c·ªßa next state
        float maxNextQ = GetMaxQValue(nextState);
        
        // √Åp d·ª•ng c√¥ng th·ª©c Q-Learning
        float newQ = currentQ + learningRate * (reward + discountFactor * maxNextQ - currentQ);
        
        // C·∫≠p nh·∫≠t Q-table
        SetQValue(state, action, newQ);
    }
}
```

2.4 STATE ENCODING:
Game state ƒë∆∞·ª£c encode th√†nh string duy nh·∫•t:
"aiPos_x,aiPos_y-humanPos_x,humanPos_y-aiWalls-humanWalls-wallsList"

V√≠ d·ª•: "4,8-4,0-9-10-7,4,False|5,6,True"
- AI ·ªü (4,8), Human ·ªü (4,0)
- AI c√≥ 9 walls, Human c√≥ 10 walls
- Walls ƒë√£ ƒë·∫∑t: (7,4,False), (5,6,True)

2.5 ACTION ENCODING:
Actions ƒë∆∞·ª£c encode th√†nh string:
- Movement: "M:x,y" (v√≠ d·ª•: "M:4,7")
- Wall: "W:x,y,horizontal" (v√≠ d·ª•: "W:3,5,True")

2.6 EPSILON-GREEDY STRATEGY:
```csharp
public AIMove ChooseAction(GameState state, List<AIMove> possibleMoves)
{
    // Exploration vs Exploitation trade-off
    if (Random.Range(0f, 1f) < epsilon)
    {
        // EXPLORATION: Ch·ªçn random ƒë·ªÉ kh√°m ph√°
        return possibleMoves[Random.Range(0, possibleMoves.Count)];
    }
    else
    {
        // EXPLOITATION: Ch·ªçn action c√≥ Q-value cao nh·∫•t
        return GetBestAction(state, possibleMoves);
    }
}
```

2.7 EPSILON DECAY:
Epsilon gi·∫£m d·∫ßn t·ª´ 0.995 ‚Üí 0.1 qua training:
- Ban ƒë·∫ßu: High exploration (99.5%)
- Cu·ªëi training: High exploitation (90%)

2.8 REWARD SYSTEM:
- Win game: +1000
- Lose game: -1000
- Move toward goal: +10
- Block opponent: +50
- Waste move: -5
- Strategic position: +15

================================================================================
                3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
================================================================================

3.1 GI·ªöI THI·ªÜU MINIMAX:
Minimax l√† thu·∫≠t to√°n game theory c·ªï ƒëi·ªÉn, t√¨m ki·∫øm move t·ªëi ∆∞u b·∫±ng c√°ch
d·ª± ƒëo√°n tr∆∞·ªõc nhi·ªÅu moves. Alpha-Beta pruning t·ªëi ∆∞u hi·ªáu su·∫•t b·∫±ng c√°ch
lo·∫°i b·ªè c√°c nh√°nh kh√¥ng c·∫ßn thi·∫øt.

3.2 NGUY√äN L√ù HO·∫†T ƒê·ªòNG:
- Maximizing Player (AI): T·ªëi ƒëa h√≥a score
- Minimizing Player (Human): T·ªëi thi·ªÉu h√≥a score AI
- Search depth: 4 levels (configurable)
- Pruning: Lo·∫°i b·ªè nh√°nh kh√¥ng promising

3.3 IMPLEMENTATION:

```csharp
float Minimax(GameState state, int depth, bool isMaximizing, float alpha, float beta)
{
    // Base case: Depth = 0 ho·∫∑c game over
    if (depth == 0 || IsGameOver(state))
        return EvaluatePosition(state);
    
    if (isMaximizing) // AI turn
    {
        float maxEval = float.NegativeInfinity;
        foreach (var move in GeneratePossibleMoves(state, true))
        {
            GameState newState = ApplyMove(state, move, true);
            float eval = Minimax(newState, depth - 1, false, alpha, beta);
            maxEval = Mathf.Max(maxEval, eval);
            alpha = Mathf.Max(alpha, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return maxEval;
    }
    else // Human turn
    {
        float minEval = float.PositiveInfinity;
        foreach (var move in GeneratePossibleMoves(state, false))
        {
            GameState newState = ApplyMove(state, move, false);
            float eval = Minimax(newState, depth - 1, true, alpha, beta);
            minEval = Mathf.Min(minEval, eval);
            beta = Mathf.Min(beta, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return minEval;
    }
}
```

3.4 EVALUATION FUNCTION:
Position evaluation d·ª±a tr√™n multiple factors:

```csharp
float EvaluatePosition(GameState state)
{
    float score = 0f;
    
    // Distance to goal (c√†ng g·∫ßn c√†ng t·ªët)
    int aiDistance = GetShortestPath(state.aiPosition, goalRow);
    int humanDistance = GetShortestPath(state.humanPosition, goalRow);
    score += (humanDistance - aiDistance) * 100;
    
    // Wall advantage
    score += (state.aiWallsLeft - state.humanWallsLeft) * 50;
    
    // Mobility (s·ªë moves c√≥ th·ªÉ)
    int aiMobility = GetValidMoves(state.aiPosition).Count;
    int humanMobility = GetValidMoves(state.humanPosition).Count;
    score += (aiMobility - humanMobility) * 20;
    
    // Center control
    score += GetCenterControlScore(state) * 30;
    
    return score;
}
```

3.5 ALPHA-BETA PRUNING HI·ªÜU QU·∫¢:
- Reduces search space t·ª´ O(b^d) xu·ªëng O(b^(d/2))
- Best case: 50% reduction in nodes evaluated
- Ordering moves by promising gi√∫p pruning hi·ªáu qu·∫£ h∆°n

================================================================================
                        4. SPECIALIZED AI STYLES SYSTEM
================================================================================

4.1 CONCEPT:
6 AI personalities v·ªõi training v√† behavior ri√™ng bi·ªát, m·ªói style c√≥:
- Ri√™ng Q-table file
- Custom reward structure
- Specific move prioritization
- Unique decision making logic

4.2 AI STYLES CHI TI·∫æT:

üî• AGGRESSIVE STYLE:
- M·ª•c ti√™u: T·∫•n c√¥ng v√† ch·∫∑n ƒë·ªëi th·ªß
- Wall Usage: 70% chance
- Reward Structure:
  * blockingOpponentReward = 50 * intensity
  * wallPlacementReward = 30 * intensity
  * defensiveWallPenalty = -20 * intensity
- Q-table: qtable_aggressive.json

üõ°Ô∏è DEFENSIVE STYLE:
- M·ª•c ti√™u: Ph√≤ng th·ªß v√† di chuy·ªÉn an to√†n
- Wall Usage: 50% chance
- Reward Structure:
  * defensiveWallReward = 35 * intensity
  * movementReward = 20 * intensity
  * aggressiveMovePenalty = -15 * intensity
- Q-table: qtable_defensive.json

üß± WALL MASTER STYLE:
- M·ª•c ti√™u: Chuy√™n gia ƒë·∫∑t t∆∞·ªùng v√† t·∫°o m√™ cung
- Wall Usage: 90% chance
- Reward Structure:
  * wallPlacementReward = 60 * intensity
  * strategicWallReward = 40 * intensity
  * wallComboReward = 50 * intensity
  * movementWithoutWallPenalty = -25 * intensity
- Q-table: qtable_wallmaster.json

‚ö° SPEED RUNNER STYLE:
- M·ª•c ti√™u: ƒê·∫°t ƒë√≠ch nhanh nh·∫•t
- Wall Usage: 10% chance
- Reward Structure:
  * movementReward = 40 * intensity
  * progressToGoalReward = 50 * intensity
  * quickWinBonus = 100 * intensity
  * wallPlacementPenalty = -20 * intensity
- Q-table: qtable_speedrunner.json

üß† TACTICAL STYLE:
- M·ª•c ti√™u: L·ª£i th·∫ø v·ªã tr√≠ v√† k·∫ø ho·∫°ch d√†i h·∫°n
- Wall Usage: Variable (strategic)
- Reward Structure:
  * tacticalMoveReward = 45 * intensity
  * positionAdvantageReward = 35 * intensity
  * mobilityReward = 25 * intensity
  * patternRecognitionBonus = 40 * intensity
- Q-table: qtable_tactical.json

‚öñÔ∏è BALANCED STYLE:
- M·ª•c ti√™u: C√¢n b·∫±ng t·∫•t c·∫£ aspects
- Wall Usage: 30% chance
- Reward Structure: Default balanced values
- Q-table: qtable.json

4.3 STYLE SWITCHING MECHANISM:

```csharp
public void SwitchToStyle(PlayStyle newStyle)
{
    // Save current Q-table
    if (allowQTableSaving) qAgent.SaveQTable(GetStyleSpecificQTablePath());
    
    // Load new style Q-table
    specializedStyle = newStyle;
    qAgent.LoadQTable(GetStyleSpecificQTablePath());
    
    // Reconfigure parameters
    ConfigureStyleSpecificTraining();
}
```

4.4 STYLE INTENSITY:
Parameter t·ª´ 0.0 ‚Üí 1.0 control m·ª©c ƒë·ªô chuy√™n h√≥a:
- 0.0: Balanced behavior
- 0.5: Moderate specialization
- 1.0: Extreme specialization

================================================================================
                        5. DECISION TREE CLASSIFIER
================================================================================

5.1 PURPOSE:
Decision Tree ph√¢n t√≠ch game context ƒë·ªÉ ch·ªçn strategy ph√π h·ª£p, d·ª±a tr√™n:
- Game phase (Early/Mid/End)
- Position advantage
- Resource advantage
- Mobility situation

5.2 FEATURE EXTRACTION:

```csharp
public struct DecisionFeatures
{
    public int aiDistanceToGoal;      // AI distance to goal line
    public int humanDistanceToGoal;   // Human distance to goal line
    public int aiWallsLeft;           // AI walls remaining
    public int humanWallsLeft;        // Human walls remaining
    public int aiMobility;            // AI possible moves
    public int humanMobility;         // Human possible moves
    public float distanceAdvantage;   // Human - AI distance
    public float wallAdvantage;       // AI - Human walls
    public bool isEarlyGame;          // > 15 moves estimated remaining
    public bool isMidGame;            // 5-15 moves remaining
    public bool isEndGame;            // < 5 moves remaining
}
```

5.3 DECISION LOGIC:

```csharp
public AIStrategy Predict(DecisionFeatures features)
{
    // Early Game: Play conservatively
    if (features.isEarlyGame)
        return AIStrategy.Balanced;
    
    // AI has significant lead: Focus on blocking
    if (features.distanceAdvantage > 2)
        return AIStrategy.Blocking;
    
    // AI is behind: Be aggressive
    if (features.distanceAdvantage < -1)
        return AIStrategy.Aggressive;
    
    // End game with wall advantage: Play defensively
    if (features.isEndGame && features.wallAdvantage > 2)
        return AIStrategy.Defensive;
    
    // Close game: Balanced approach
    return AIStrategy.Balanced;
}
```

5.4 STRATEGY OUTCOMES:
- Balanced: Equal weight to movement v√† walls
- Blocking: Prioritize moves that increase opponent path
- Aggressive: Focus on attacking positions
- Defensive: Protect own position and path

================================================================================
                        6. ENTROPY ANALYSIS & SOFTMAX
================================================================================

6.1 ENTROPY CONCEPT:
Entropy ƒëo ƒë·ªô ƒëa d·∫°ng/uncertainty c·ªßa move scores. High entropy = nhi·ªÅu moves
c√≥ scores t∆∞∆°ng ƒë∆∞∆°ng, low entropy = m·ªôt move r√µ r√†ng t·ªët nh·∫•t.

6.2 ENTROPY CALCULATION:

```csharp
float CalculateEntropy(List<float> moveScores)
{
    // Normalize th√†nh probabilities
    float total = moveScores.Sum();
    var probabilities = moveScores.Select(s => s / total);
    
    // Calculate Shannon entropy
    float entropy = 0f;
    foreach (float p in probabilities)
    {
        if (p > 0f)
            entropy -= p * Mathf.Log(p, 2f);
    }
    
    return entropy; // Range: 0 (deterministic) to log2(n) (uniform)
}
```

6.3 SOFTMAX DISTRIBUTION:

```csharp
float[] CalculateSoftmax(List<float> scores, float temperature)
{
    float[] probabilities = new float[scores.Count];
    float sum = scores.Sum(s => Mathf.Exp(s / temperature));
    
    for (int i = 0; i < scores.Count; i++)
    {
        probabilities[i] = Mathf.Exp(scores[i] / temperature) / sum;
    }
    
    return probabilities;
}
```

6.4 TEMPERATURE PARAMETER:
- Low temperature (0.1): Sharp distribution, focus on best moves
- High temperature (2.0): Flat distribution, more exploration
- Default temperature (1.0): Balanced exploitation/exploration

6.5 ADAPTIVE MOVE SELECTION:

```csharp
public AIMove ChooseActionWithEntropy(GameState state, List<AIMove> possibleMoves)
{
    var moveScores = EvaluateAllMoves(state, possibleMoves);
    float entropy = CalculateEntropy(moveScores);
    
    if (entropy > entropyThreshold) // High diversity
    {
        // Use softmax for probabilistic selection
        var probabilities = CalculateSoftmax(moveScores, softmaxTemperature);
        return SelectMoveByProbability(possibleMoves, probabilities);
    }
    else // Clear best move
    {
        // Use deterministic best move
        return GetBestScoredMove(possibleMoves, moveScores);
    }
}
```

================================================================================
                        7. TRAINING SYSTEM & SELF-PLAY
================================================================================

7.1 SELF-PLAY CONCEPT:
AI h·ªçc b·∫±ng c√°ch ch∆°i v·ªõi ch√≠nh n√≥ h√†ng ng√†n l·∫ßn, gradually improve performance
th√¥ng qua trial v√† error.

7.2 TRAINING PIPELINE:

```csharp
IEnumerator RunSelfPlayTraining()
{
    for (int episode = 1; episode <= maxEpisodes; episode++)
    {
        // Reset game state
        ResetGameForTraining();
        
        while (!IsGameOver())
        {
            // Get current state
            GameState currentState = GetCurrentGameState();
            List<AIMove> possibleMoves = GetPossibleMoves();
            
            // Choose action using Œµ-greedy
            AIMove chosenMove = qLearningAgent.ChooseAction(currentState, possibleMoves);
            
            // Execute move
            ExecuteMove(chosenMove);
            
            // Calculate reward
            float reward = CalculateReward(currentState, chosenMove);
            
            // Update Q-table
            GameState newState = GetCurrentGameState();
            qLearningAgent.UpdateQValue(
                EncodeState(currentState),
                EncodeAction(chosenMove),
                reward,
                EncodeState(newState)
            );
            
            yield return new WaitForEndOfFrame();
        }
        
        // Epsilon decay
        qLearningAgent.DecayEpsilon();
        
        // Save progress periodically
        if (episode % 100 == 0)
        {
            qLearningAgent.SaveQTable();
        }
    }
}
```

7.3 REWARD ENGINEERING:
Critical cho Q-Learning success:

```csharp
float CalculateReward(GameState state, AIMove move)
{
    float reward = 0f;
    
    // Game outcome rewards
    if (IsWin(state)) reward += 1000f;
    if (IsLoss(state)) reward -= 1000f;
    
    // Progress rewards
    if (move.moveType == MoveType.Movement)
    {
        if (IsMoveTowardGoal(move)) reward += 20f;
        else reward -= 5f;
    }
    
    // Strategic rewards
    if (move.moveType == MoveType.WallPlacement)
    {
        if (IsWallBlockingOpponent(move)) reward += 50f;
        if (IsWallStrategic(move)) reward += 30f;
        if (IsWallWasted(move)) reward -= 25f;
    }
    
    // Style-specific rewards
    reward += GetStyleSpecificReward(state, move);
    
    return reward;
}
```

7.4 TRAINING METRICS:
- Episodes: 1000 standard
- Duration: ~15 minutes
- Convergence: Usually around episode 800
- Q-table size: 13,118+ states at completion
- Final epsilon: 0.1 (90% exploitation)

================================================================================
                        8. AI TESTING FRAMEWORK
================================================================================

8.1 TESTING PHILOSOPHY:
Comprehensive testing ensures AI quality v√† reliability across different
game scenarios v√† edge cases.

8.2 TEST CATEGORIES:

A) DECISION TREE TESTS:
```csharp
void TestDecisionTreeStrategies()
{
    // Test Early Game
    var earlyGameFeatures = new DecisionFeatures
    {
        aiDistanceToGoal = 8, humanDistanceToGoal = 8,
        isEarlyGame = true
    };
    var strategy = decisionTree.Predict(earlyGameFeatures);
    Assert.AreEqual(AIStrategy.Balanced, strategy);
    
    // Test AI Leading
    var leadingFeatures = new DecisionFeatures
    {
        distanceAdvantage = 3, // AI leading
        isMidGame = true
    };
    strategy = decisionTree.Predict(leadingFeatures);
    Assert.AreEqual(AIStrategy.Blocking, strategy);
}
```

B) ENTROPY ANALYSIS TESTS:
```csharp
void TestEntropyAnalysis()
{
    // High entropy case (diverse scores)
    var highEntropyScores = new List<float> { 100f, 98f, 97f, 95f, 94f };
    float entropy = CalculateTestEntropy(highEntropyScores);
    Assert.IsTrue(entropy > 0.5f);
    
    // Low entropy case (clear winner)
    var lowEntropyScores = new List<float> { 100f, 50f, 50f, 50f, 50f };
    entropy = CalculateTestEntropy(lowEntropyScores);
    Assert.IsTrue(entropy < 0.5f);
}
```

C) SOFTMAX DISTRIBUTION TESTS:
```csharp
void TestSoftmaxDistribution()
{
    var testScores = new List<float> { 100f, 98f, 95f, 90f };
    float temperature = 1.0f;
    
    var probabilities = CalculateSoftmax(testScores, temperature);
    
    // Verify probabilities sum to 1
    float sum = probabilities.Sum();
    Assert.AreEqual(1.0f, sum, 0.001f);
    
    // Verify highest score gets highest probability
    int maxIndex = Array.IndexOf(probabilities, probabilities.Max());
    Assert.AreEqual(0, maxIndex);
}
```

D) PERFORMANCE BENCHMARKS:
- Move generation time: <100ms
- Q-Learning decision: <50ms
- Minimax decision (depth 4): <200ms
- Memory usage: <256MB

8.3 AUTOMATED TESTING:
Tests run automatically ƒë·ªÉ ensure code quality:
- Unit tests cho individual algorithms
- Integration tests cho AI system interactions
- Performance tests cho response times
- Regression tests ƒë·ªÉ prevent quality degradation

================================================================================
                        9. HYBRID AI ARCHITECTURE
================================================================================

9.1 CONCEPT:
Hybrid system k·∫øt h·ª£p strengths c·ªßa multiple algorithms:
- Q-Learning: Data-driven, learns from experience
- Minimax: Logical, guaranteed optimal given evaluation
- Decision Tree: Fast, context-aware switching
- Entropy: Adaptive, handles uncertainty

9.2 ALGORITHM SELECTION LOGIC:

```csharp
public AIMove GetBestMove()
{
    UpdateGameState();
    List<AIMove> possibleMoves = GeneratePossibleMoves();
    
    // Primary: Use Q-Learning if trained model available
    if (useQLearning && qAgent.IsTrainedModel())
    {
        Debug.Log($"üß† Using {specializedStyle} Q-Learning");
        return qAgent.ChooseAction(currentGameState, possibleMoves);
    }
    
    // Fallback: Use Minimax for guaranteed reasonable play
    else
    {
        Debug.Log($"üîç Using {specializedStyle} Minimax");
        return FindBestMoveWithMinimax();
    }
}
```

9.3 SEAMLESS SWITCHING:
- No interruption in gameplay
- Maintains consistent API
- Preserves game state integrity
- Logs algorithm used for debugging

9.4 BENEFITS:
- Robustness: Fallback ensures AI never fails
- Performance: Q-Learning for trained scenarios, Minimax for edge cases
- Flexibility: Can switch algorithms based on situation
- Development: Easy to test individual components

================================================================================
                        10. PERFORMANCE OPTIMIZATION
================================================================================

10.1 Q-TABLE OPTIMIZATION:
- Sparse representation: Only store visited states
- Compression: String interning for state keys
- Lazy loading: Load Q-table on demand
- Periodic saving: Every 100 training episodes

10.2 MINIMAX OPTIMIZATION:
- Alpha-Beta pruning: 50% search reduction
- Move ordering: Promising moves first
- Iterative deepening: Gradual depth increase
- Transposition tables: Cache evaluated positions

10.3 MEMORY MANAGEMENT:
- Object pooling cho GameState objects
- String caching cho repeated state/action encodings
- Garbage collection optimization
- Memory profiling v√† monitoring

10.4 MULTITHREADING CONSIDERATIONS:
- Q-Learning training: Background thread
- Move calculation: Main thread (Unity requirement)
- File I/O: Async operations where possible
- Thread-safe collections cho shared data

10.5 CACHING STRATEGIES:
- Path finding results
- Move generation results
- Evaluation function results
- Feature extraction results

================================================================================
                            K·∫æT LU·∫¨N
================================================================================

D·ª± √°n AI Quoridor Project th·ªÉ hi·ªán m·ªôt implementation comprehensive c·ªßa modern
game AI techniques:

TECHNICAL INNOVATIONS:
‚úÖ Hybrid Q-Learning + Minimax architecture
‚úÖ 6 specialized AI personalities v·ªõi separate training
‚úÖ Advanced decision making v·ªõi entropy analysis
‚úÖ Comprehensive testing v√† validation framework
‚úÖ Professional development tools v√† debugging

EDUCATIONAL VALUE:
‚úÖ Practical machine learning implementation
‚úÖ Traditional game AI algorithms
‚úÖ Modern software engineering practices
‚úÖ Performance optimization techniques
‚úÖ Automated testing methodologies

FUTURE ENHANCEMENTS:
üöÄ Deep Q-Learning (DQN) integration
üöÄ Multi-agent reinforcement learning
üöÄ Neural network-based evaluation functions
üöÄ Online learning v√† adaptation
üöÄ Transfer learning between game variants

D·ª± √°n n√†y demonstrating successful integration c·ªßa classical AI methods v·ªõi
modern machine learning, t·∫°o ra m·ªôt intelligent, adaptive, v√† highly
interactive gaming experience.

================================================================================
                            APPENDIX
================================================================================

FILES LI√äN QUAN:
- Assets/scripts/QLearningAgent.cs - Q-Learning implementation
- Assets/scripts/QuoridorAI.cs - Main AI controller
- Assets/scripts/DecisionTreeClassifier.cs - Decision tree logic
- Assets/scripts/QuoridorSelfPlayTrainer.cs - Training system
- Assets/scripts/AITestScript.cs - Testing framework
- Assets/scripts/Editor/ForceRetrainAI.cs - Development tools

Q-TABLE FILES:
- StreamingAssets/qtable.json - Balanced style
- StreamingAssets/qtable_aggressive.json - Aggressive style
- StreamingAssets/qtable_defensive.json - Defensive style
- StreamingAssets/qtable_wallmaster.json - Wall Master style
- StreamingAssets/qtable_speedrunner.json - Speed Runner style
- StreamingAssets/qtable_tactical.json - Tactical style

DOCUMENTATION:
- AI_ALGORITHMS_DETAILED_DESCRIPTION.txt - This file
- README.md - Project overview
- API_INTEGRATION_GUIDE.md - Voice/Chat integration
- TRAINING_GUIDE.md - AI training instructions

================================================================================
================================================================================
                    AI QUORIDOR GAME PROJECT - THU·∫¨T TO√ÅN AI CHI TI·∫æT
================================================================================

D·ª± √°n: AI Quoridor Game v·ªõi Machine Learning v√† Multi-Platform Integration
Ng√†y t·∫°o: 25/07/2025
T√°c gi·∫£: [Student Name]
Platform: Unity 3D
Language: C#

================================================================================
                            M·ª§C L·ª§C
================================================================================

1. T·ªîNG QUAN KI·∫æN TR√öC AI
2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
4. SPECIALIZED AI STYLES SYSTEM
5. DECISION TREE CLASSIFIER
6. ENTROPY ANALYSIS & SOFTMAX
7. TRAINING SYSTEM & SELF-PLAY
8. AI TESTING FRAMEWORK
9. HYBRID AI ARCHITECTURE
10. PERFORMANCE OPTIMIZATION

================================================================================
                        1. T·ªîNG QUAN KI·∫æN TR√öC AI
================================================================================

D·ª± √°n AI Quoridor s·ª≠ d·ª•ng m·ªôt ki·∫øn tr√∫c AI hybrid ti√™n ti·∫øn, k·∫øt h·ª£p:

1.1 CORE ALGORITHMS:
- Q-Learning (Reinforcement Learning) - Thu·∫≠t to√°n ch√≠nh
- Minimax + Alpha-Beta Pruning - Thu·∫≠t to√°n fallback
- Decision Tree Classifier - Ph√¢n t√≠ch ng·ªØ c·∫£nh game
- Entropy Analysis - ƒê√°nh gi√° ƒëa d·∫°ng move

1.2 SPECIALIZED SYSTEMS:
- 6 AI Personality Styles v·ªõi training ri√™ng bi·ªát
- Self-Play Training Pipeline t·ª± ƒë·ªông
- Real-time Performance Testing Framework
- Dynamic Algorithm Switching

1.3 TECHNICAL FOUNDATION:
- State Space: 13,118+ unique game states
- Action Space: Movement + Wall placement combinations
- Training Method: Self-play v·ªõi 1000 episodes
- Learning Rate: Œ± = 0.1, Discount Factor: Œ≥ = 0.99

================================================================================
                    2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
================================================================================

2.1 GI·ªöI THI·ªÜU Q-LEARNING:
Q-Learning l√† thu·∫≠t to√°n Reinforcement Learning kh√¥ng c·∫ßn model, h·ªçc th√¥ng qua
t∆∞∆°ng t√°c v·ªõi m√¥i tr∆∞·ªùng ƒë·ªÉ t·ªëi ∆∞u h√≥a policy. Trong Quoridor, AI h·ªçc c√°ch
ch·ªçn moves t·ªëi ∆∞u d·ª±a tr√™n kinh nghi·ªám t·ª´ h√†ng ng√†n game.

2.2 C√îNG TH·ª®C TO√ÅN H·ªåC:
Q(s,a) = Q(s,a) + Œ±[r + Œ≥*max(Q(s',a')) - Q(s,a)]

Trong ƒë√≥:
- Q(s,a): Q-value c·ªßa state s v·ªõi action a
- Œ±: Learning rate (0.1) - t·ªëc ƒë·ªô h·ªçc
- r: Immediate reward t·ª´ action
- Œ≥: Discount factor (0.99) - t·∫ßm quan tr·ªçng future rewards
- s': Next state sau khi th·ª±c hi·ªán action
- max(Q(s',a')): Maximum Q-value c·ªßa next state

2.3 IMPLEMENTATION CHI TI·∫æT:

```csharp
public class QLearningAgent
{
    // Core Q-Table: Dictionary l∆∞u tr·ªØ Q-values
    private Dictionary<string, Dictionary<string, float>> qTable;
    
    // Hyperparameters
    private float learningRate = 0.1f;      // T·ªëc ƒë·ªô h·ªçc
    private float discountFactor = 0.99f;   // Discount future rewards
    private float epsilon = 0.995f;         // Exploration rate
    
    // Q-Learning Update Implementation
    public void UpdateQValue(string state, string action, float reward, string nextState)
    {
        // L·∫•y Q-value hi·ªán t·∫°i
        float currentQ = GetQValue(state, action);
        
        // T√¨m max Q-value c·ªßa next state
        float maxNextQ = GetMaxQValue(nextState);
        
        // √Åp d·ª•ng c√¥ng th·ª©c Q-Learning
        float newQ = currentQ + learningRate * (reward + discountFactor * maxNextQ - currentQ);
        
        // C·∫≠p nh·∫≠t Q-table
        SetQValue(state, action, newQ);
    }
}
```

2.4 STATE ENCODING:
Game state ƒë∆∞·ª£c encode th√†nh string duy nh·∫•t:
"aiPos_x,aiPos_y-humanPos_x,humanPos_y-aiWalls-humanWalls-wallsList"

V√≠ d·ª•: "4,8-4,0-9-10-7,4,False|5,6,True"
- AI ·ªü (4,8), Human ·ªü (4,0)
- AI c√≥ 9 walls, Human c√≥ 10 walls
- Walls ƒë√£ ƒë·∫∑t: (7,4,False), (5,6,True)

2.5 ACTION ENCODING:
Actions ƒë∆∞·ª£c encode th√†nh string:
- Movement: "M:x,y" (v√≠ d·ª•: "M:4,7")
- Wall: "W:x,y,horizontal" (v√≠ d·ª•: "W:3,5,True")

2.6 EPSILON-GREEDY STRATEGY:
```csharp
public AIMove ChooseAction(GameState state, List<AIMove> possibleMoves)
{
    // Exploration vs Exploitation trade-off
    if (Random.Range(0f, 1f) < epsilon)
    {
        // EXPLORATION: Ch·ªçn random ƒë·ªÉ kh√°m ph√°
        return possibleMoves[Random.Range(0, possibleMoves.Count)];
    }
    else
    {
        // EXPLOITATION: Ch·ªçn action c√≥ Q-value cao nh·∫•t
        return GetBestAction(state, possibleMoves);
    }
}
```

2.7 EPSILON DECAY:
Epsilon gi·∫£m d·∫ßn t·ª´ 0.995 ‚Üí 0.1 qua training:
- Ban ƒë·∫ßu: High exploration (99.5%)
- Cu·ªëi training: High exploitation (90%)

2.8 REWARD SYSTEM:
- Win game: +1000
- Lose game: -1000
- Move toward goal: +10
- Block opponent: +50
- Waste move: -5
- Strategic position: +15

================================================================================
                3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
================================================================================

3.1 GI·ªöI THI·ªÜU MINIMAX:
Minimax l√† thu·∫≠t to√°n game theory c·ªï ƒëi·ªÉn, t√¨m ki·∫øm move t·ªëi ∆∞u b·∫±ng c√°ch
d·ª± ƒëo√°n tr∆∞·ªõc nhi·ªÅu moves. Alpha-Beta pruning t·ªëi ∆∞u hi·ªáu su·∫•t b·∫±ng c√°ch
lo·∫°i b·ªè c√°c nh√°nh kh√¥ng c·∫ßn thi·∫øt.

3.2 NGUY√äN L√ù HO·∫†T ƒê·ªòNG:
- Maximizing Player (AI): T·ªëi ƒëa h√≥a score
- Minimizing Player (Human): T·ªëi thi·ªÉu h√≥a score AI
- Search depth: 4 levels (configurable)
- Pruning: Lo·∫°i b·ªè nh√°nh kh√¥ng promising

3.3 IMPLEMENTATION:

```csharp
float Minimax(GameState state, int depth, bool isMaximizing, float alpha, float beta)
{
    // Base case: Depth = 0 ho·∫∑c game over
    if (depth == 0 || IsGameOver(state))
        return EvaluatePosition(state);
    
    if (isMaximizing) // AI turn
    {
        float maxEval = float.NegativeInfinity;
        foreach (var move in GeneratePossibleMoves(state, true))
        {
            GameState newState = ApplyMove(state, move, true);
            float eval = Minimax(newState, depth - 1, false, alpha, beta);
            maxEval = Mathf.Max(maxEval, eval);
            alpha = Mathf.Max(alpha, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return maxEval;
    }
    else // Human turn
    {
        float minEval = float.PositiveInfinity;
        foreach (var move in GeneratePossibleMoves(state, false))
        {
            GameState newState = ApplyMove(state, move, false);
            float eval = Minimax(newState, depth - 1, true, alpha, beta);
            minEval = Mathf.Min(minEval, eval);
            beta = Mathf.Min(beta, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return minEval;
    }
}
```

3.4 EVALUATION FUNCTION:
Position evaluation d·ª±a tr√™n multiple factors:

```csharp
float EvaluatePosition(GameState state)
{
    float score = 0f;
    
    // Distance to goal (c√†ng g·∫ßn c√†ng t·ªët)
    int aiDistance = GetShortestPath(state.aiPosition, goalRow);
    int humanDistance = GetShortestPath(state.humanPosition, goalRow);
    score += (humanDistance - aiDistance) * 100;
    
    // Wall advantage
    score += (state.aiWallsLeft - state.humanWallsLeft) * 50;
    
    // Mobility (s·ªë moves c√≥ th·ªÉ)
    int aiMobility = GetValidMoves(state.aiPosition).Count;
    int humanMobility = GetValidMoves(state.humanPosition).Count;
    score += (aiMobility - humanMobility) * 20;
    
    // Center control
    score += GetCenterControlScore(state) * 30;
    
    return score;
}
```

3.5 ALPHA-BETA PRUNING HI·ªÜU QU·∫¢:
- Reduces search space t·ª´ O(b^d) xu·ªëng O(b^(d/2))
- Best case: 50% reduction in nodes evaluated
- Ordering moves by promising gi√∫p pruning hi·ªáu qu·∫£ h∆°n

================================================================================
                        4. SPECIALIZED AI STYLES SYSTEM
================================================================================

4.1 CONCEPT:
6 AI personalities v·ªõi training v√† behavior ri√™ng bi·ªát, m·ªói style c√≥:
- Ri√™ng Q-table file
- Custom reward structure
- Specific move prioritization
- Unique decision making logic

4.2 AI STYLES CHI TI·∫æT:

üî• AGGRESSIVE STYLE:
- M·ª•c ti√™u: T·∫•n c√¥ng v√† ch·∫∑n ƒë·ªëi th·ªß
- Wall Usage: 70% chance
- Reward Structure:
  * blockingOpponentReward = 50 * intensity
  * wallPlacementReward = 30 * intensity
  * defensiveWallPenalty = -20 * intensity
- Q-table: qtable_aggressive.json

üõ°Ô∏è DEFENSIVE STYLE:
- M·ª•c ti√™u: Ph√≤ng th·ªß v√† di chuy·ªÉn an to√†n
- Wall Usage: 50% chance
- Reward Structure:
  * defensiveWallReward = 35 * intensity
  * movementReward = 20 * intensity
  * aggressiveMovePenalty = -15 * intensity
- Q-table: qtable_defensive.json

üß± WALL MASTER STYLE:
- M·ª•c ti√™u: Chuy√™n gia ƒë·∫∑t t∆∞·ªùng v√† t·∫°o m√™ cung
- Wall Usage: 90% chance
- Reward Structure:
  * wallPlacementReward = 60 * intensity
  * strategicWallReward = 40 * intensity
  * wallComboReward = 50 * intensity
  * movementWithoutWallPenalty = -25 * intensity
- Q-table: qtable_wallmaster.json

‚ö° SPEED RUNNER STYLE:
- M·ª•c ti√™u: ƒê·∫°t ƒë√≠ch nhanh nh·∫•t
- Wall Usage: 10% chance
- Reward Structure:
  * movementReward = 40 * intensity
  * progressToGoalReward = 50 * intensity
  * quickWinBonus = 100 * intensity
  * wallPlacementPenalty = -20 * intensity
- Q-table: qtable_speedrunner.json

üß† TACTICAL STYLE:
- M·ª•c ti√™u: L·ª£i th·∫ø v·ªã tr√≠ v√† k·∫ø ho·∫°ch d√†i h·∫°n
- Wall Usage: Variable (strategic)
- Reward Structure:
  * tacticalMoveReward = 45 * intensity
  * positionAdvantageReward = 35 * intensity
  * mobilityReward = 25 * intensity
  * patternRecognitionBonus = 40 * intensity
- Q-table: qtable_tactical.json

‚öñÔ∏è BALANCED STYLE:
- M·ª•c ti√™u: C√¢n b·∫±ng t·∫•t c·∫£ aspects
- Wall Usage: 30% chance
- Reward Structure: Default balanced values
- Q-table: qtable.json

4.3 STYLE SWITCHING MECHANISM:

```csharp
public void SwitchToStyle(PlayStyle newStyle)
{
    // Save current Q-table
    if (allowQTableSaving) qAgent.SaveQTable(GetStyleSpecificQTablePath());
    
    // Load new style Q-table
    specializedStyle = newStyle;
    qAgent.LoadQTable(GetStyleSpecificQTablePath());
    
    // Reconfigure parameters
    ConfigureStyleSpecificTraining();
}
```

4.4 STYLE INTENSITY:
Parameter t·ª´ 0.0 ‚Üí 1.0 control m·ª©c ƒë·ªô chuy√™n h√≥a:
- 0.0: Balanced behavior
- 0.5: Moderate specialization
- 1.0: Extreme specialization

================================================================================
                        5. DECISION TREE CLASSIFIER
================================================================================

5.1 PURPOSE:
Decision Tree ph√¢n t√≠ch game context ƒë·ªÉ ch·ªçn strategy ph√π h·ª£p, d·ª±a tr√™n:
- Game phase (Early/Mid/End)
- Position advantage
- Resource advantage
- Mobility situation

5.2 FEATURE EXTRACTION:

```csharp
public struct DecisionFeatures
{
    public int aiDistanceToGoal;      // AI distance to goal line
    public int humanDistanceToGoal;   // Human distance to goal line
    public int aiWallsLeft;           // AI walls remaining
    public int humanWallsLeft;        // Human walls remaining
    public int aiMobility;            // AI possible moves
    public int humanMobility;         // Human possible moves
    public float distanceAdvantage;   // Human - AI distance
    public float wallAdvantage;       // AI - Human walls
    public bool isEarlyGame;          // > 15 moves estimated remaining
    public bool isMidGame;            // 5-15 moves remaining
    public bool isEndGame;            // < 5 moves remaining
}
```

5.3 DECISION LOGIC:

```csharp
public AIStrategy Predict(DecisionFeatures features)
{
    // Early Game: Play conservatively
    if (features.isEarlyGame)
        return AIStrategy.Balanced;
    
    // AI has significant lead: Focus on blocking
    if (features.distanceAdvantage > 2)
        return AIStrategy.Blocking;
    
    // AI is behind: Be aggressive
    if (features.distanceAdvantage < -1)
        return AIStrategy.Aggressive;
    
    // End game with wall advantage: Play defensively
    if (features.isEndGame && features.wallAdvantage > 2)
        return AIStrategy.Defensive;
    
    // Close game: Balanced approach
    return AIStrategy.Balanced;
}
```

5.4 STRATEGY OUTCOMES:
- Balanced: Equal weight to movement v√† walls
- Blocking: Prioritize moves that increase opponent path
- Aggressive: Focus on attacking positions
- Defensive: Protect own position and path

================================================================================
                        6. ENTROPY ANALYSIS & SOFTMAX
================================================================================

6.1 ENTROPY CONCEPT:
Entropy ƒëo ƒë·ªô ƒëa d·∫°ng/uncertainty c·ªßa move scores. High entropy = nhi·ªÅu moves
c√≥ scores t∆∞∆°ng ƒë∆∞∆°ng, low entropy = m·ªôt move r√µ r√†ng t·ªët nh·∫•t.

6.2 ENTROPY CALCULATION:

```csharp
float CalculateEntropy(List<float> moveScores)
{
    // Normalize th√†nh probabilities
    float total = moveScores.Sum();
    var probabilities = moveScores.Select(s => s / total);
    
    // Calculate Shannon entropy
    float entropy = 0f;
    foreach (float p in probabilities)
    {
        if (p > 0f)
            entropy -= p * Mathf.Log(p, 2f);
    }
    
    return entropy; // Range: 0 (deterministic) to log2(n) (uniform)
}
```

6.3 SOFTMAX DISTRIBUTION:

```csharp
float[] CalculateSoftmax(List<float> scores, float temperature)
{
    float[] probabilities = new float[scores.Count];
    float sum = scores.Sum(s => Mathf.Exp(s / temperature));
    
    for (int i = 0; i < scores.Count; i++)
    {
        probabilities[i] = Mathf.Exp(scores[i] / temperature) / sum;
    }
    
    return probabilities;
}
```

6.4 TEMPERATURE PARAMETER:
- Low temperature (0.1): Sharp distribution, focus on best moves
- High temperature (2.0): Flat distribution, more exploration
- Default temperature (1.0): Balanced exploitation/exploration

6.5 ADAPTIVE MOVE SELECTION:

```csharp
public AIMove ChooseActionWithEntropy(GameState state, List<AIMove> possibleMoves)
{
    var moveScores = EvaluateAllMoves(state, possibleMoves);
    float entropy = CalculateEntropy(moveScores);
    
    if (entropy > entropyThreshold) // High diversity
    {
        // Use softmax for probabilistic selection
        var probabilities = CalculateSoftmax(moveScores, softmaxTemperature);
        return SelectMoveByProbability(possibleMoves, probabilities);
    }
    else // Clear best move
    {
        // Use deterministic best move
        return GetBestScoredMove(possibleMoves, moveScores);
    }
}
```

================================================================================
                        7. TRAINING SYSTEM & SELF-PLAY
================================================================================

7.1 SELF-PLAY CONCEPT:
AI h·ªçc b·∫±ng c√°ch ch∆°i v·ªõi ch√≠nh n√≥ h√†ng ng√†n l·∫ßn, gradually improve performance
th√¥ng qua trial v√† error.

7.2 TRAINING PIPELINE:

```csharp
IEnumerator RunSelfPlayTraining()
{
    for (int episode = 1; episode <= maxEpisodes; episode++)
    {
        // Reset game state
        ResetGameForTraining();
        
        while (!IsGameOver())
        {
            // Get current state
            GameState currentState = GetCurrentGameState();
            List<AIMove> possibleMoves = GetPossibleMoves();
            
            // Choose action using Œµ-greedy
            AIMove chosenMove = qLearningAgent.ChooseAction(currentState, possibleMoves);
            
            // Execute move
            ExecuteMove(chosenMove);
            
            // Calculate reward
            float reward = CalculateReward(currentState, chosenMove);
            
            // Update Q-table
            GameState newState = GetCurrentGameState();
            qLearningAgent.UpdateQValue(
                EncodeState(currentState),
                EncodeAction(chosenMove),
                reward,
                EncodeState(newState)
            );
            
            yield return new WaitForEndOfFrame();
        }
        
        // Epsilon decay
        qLearningAgent.DecayEpsilon();
        
        // Save progress periodically
        if (episode % 100 == 0)
        {
            qLearningAgent.SaveQTable();
        }
    }
}
```

7.3 REWARD ENGINEERING:
Critical cho Q-Learning success:

```csharp
float CalculateReward(GameState state, AIMove move)
{
    float reward = 0f;
    
    // Game outcome rewards
    if (IsWin(state)) reward += 1000f;
    if (IsLoss(state)) reward -= 1000f;
    
    // Progress rewards
    if (move.moveType == MoveType.Movement)
    {
        if (IsMoveTowardGoal(move)) reward += 20f;
        else reward -= 5f;
    }
    
    // Strategic rewards
    if (move.moveType == MoveType.WallPlacement)
    {
        if (IsWallBlockingOpponent(move)) reward += 50f;
        if (IsWallStrategic(move)) reward += 30f;
        if (IsWallWasted(move)) reward -= 25f;
    }
    
    // Style-specific rewards
    reward += GetStyleSpecificReward(state, move);
    
    return reward;
}
```

7.4 TRAINING METRICS:
- Episodes: 1000 standard
- Duration: ~15 minutes
- Convergence: Usually around episode 800
- Q-table size: 13,118+ states at completion
- Final epsilon: 0.1 (90% exploitation)

================================================================================
                        8. AI TESTING FRAMEWORK
================================================================================

8.1 TESTING PHILOSOPHY:
Comprehensive testing ensures AI quality v√† reliability across different
game scenarios v√† edge cases.

8.2 TEST CATEGORIES:

A) DECISION TREE TESTS:
```csharp
void TestDecisionTreeStrategies()
{
    // Test Early Game
    var earlyGameFeatures = new DecisionFeatures
    {
        aiDistanceToGoal = 8, humanDistanceToGoal = 8,
        isEarlyGame = true
    };
    var strategy = decisionTree.Predict(earlyGameFeatures);
    Assert.AreEqual(AIStrategy.Balanced, strategy);
    
    // Test AI Leading
    var leadingFeatures = new DecisionFeatures
    {
        distanceAdvantage = 3, // AI leading
        isMidGame = true
    };
    strategy = decisionTree.Predict(leadingFeatures);
    Assert.AreEqual(AIStrategy.Blocking, strategy);
}
```

B) ENTROPY ANALYSIS TESTS:
```csharp
void TestEntropyAnalysis()
{
    // High entropy case (diverse scores)
    var highEntropyScores = new List<float> { 100f, 98f, 97f, 95f, 94f };
    float entropy = CalculateTestEntropy(highEntropyScores);
    Assert.IsTrue(entropy > 0.5f);
    
    // Low entropy case (clear winner)
    var lowEntropyScores = new List<float> { 100f, 50f, 50f, 50f, 50f };
    entropy = CalculateTestEntropy(lowEntropyScores);
    Assert.IsTrue(entropy < 0.5f);
}
```

C) SOFTMAX DISTRIBUTION TESTS:
```csharp
void TestSoftmaxDistribution()
{
    var testScores = new List<float> { 100f, 98f, 95f, 90f };
    float temperature = 1.0f;
    
    var probabilities = CalculateSoftmax(testScores, temperature);
    
    // Verify probabilities sum to 1
    float sum = probabilities.Sum();
    Assert.AreEqual(1.0f, sum, 0.001f);
    
    // Verify highest score gets highest probability
    int maxIndex = Array.IndexOf(probabilities, probabilities.Max());
    Assert.AreEqual(0, maxIndex);
}
```

D) PERFORMANCE BENCHMARKS:
- Move generation time: <100ms
- Q-Learning decision: <50ms
- Minimax decision (depth 4): <200ms
- Memory usage: <256MB

8.3 AUTOMATED TESTING:
Tests run automatically ƒë·ªÉ ensure code quality:
- Unit tests cho individual algorithms
- Integration tests cho AI system interactions
- Performance tests cho response times
- Regression tests ƒë·ªÉ prevent quality degradation

================================================================================
                        9. HYBRID AI ARCHITECTURE
================================================================================

9.1 CONCEPT:
Hybrid system k·∫øt h·ª£p strengths c·ªßa multiple algorithms:
- Q-Learning: Data-driven, learns from experience
- Minimax: Logical, guaranteed optimal given evaluation
- Decision Tree: Fast, context-aware switching
- Entropy: Adaptive, handles uncertainty

9.2 ALGORITHM SELECTION LOGIC:

```csharp
public AIMove GetBestMove()
{
    UpdateGameState();
    List<AIMove> possibleMoves = GeneratePossibleMoves();
    
    // Primary: Use Q-Learning if trained model available
    if (useQLearning && qAgent.IsTrainedModel())
    {
        Debug.Log($"üß† Using {specializedStyle} Q-Learning");
        return qAgent.ChooseAction(currentGameState, possibleMoves);
    }
    
    // Fallback: Use Minimax for guaranteed reasonable play
    else
    {
        Debug.Log($"üîç Using {specializedStyle} Minimax");
        return FindBestMoveWithMinimax();
    }
}
```

9.3 SEAMLESS SWITCHING:
- No interruption in gameplay
- Maintains consistent API
- Preserves game state integrity
- Logs algorithm used for debugging

9.4 BENEFITS:
- Robustness: Fallback ensures AI never fails
- Performance: Q-Learning for trained scenarios, Minimax for edge cases
- Flexibility: Can switch algorithms based on situation
- Development: Easy to test individual components

================================================================================
                        10. PERFORMANCE OPTIMIZATION
================================================================================

10.1 Q-TABLE OPTIMIZATION:
- Sparse representation: Only store visited states
- Compression: String interning for state keys
- Lazy loading: Load Q-table on demand
- Periodic saving: Every 100 training episodes

10.2 MINIMAX OPTIMIZATION:
- Alpha-Beta pruning: 50% search reduction
- Move ordering: Promising moves first
- Iterative deepening: Gradual depth increase
- Transposition tables: Cache evaluated positions

10.3 MEMORY MANAGEMENT:
- Object pooling cho GameState objects
- String caching cho repeated state/action encodings
- Garbage collection optimization
- Memory profiling v√† monitoring

10.4 MULTITHREADING CONSIDERATIONS:
- Q-Learning training: Background thread
- Move calculation: Main thread (Unity requirement)
- File I/O: Async operations where possible
- Thread-safe collections cho shared data

10.5 CACHING STRATEGIES:
- Path finding results
- Move generation results
- Evaluation function results
- Feature extraction results

================================================================================
                            K·∫æT LU·∫¨N
================================================================================

D·ª± √°n AI Quoridor Project th·ªÉ hi·ªán m·ªôt implementation comprehensive c·ªßa modern
game AI techniques:

TECHNICAL INNOVATIONS:
‚úÖ Hybrid Q-Learning + Minimax architecture
‚úÖ 6 specialized AI personalities v·ªõi separate training
‚úÖ Advanced decision making v·ªõi entropy analysis
‚úÖ Comprehensive testing v√† validation framework
‚úÖ Professional development tools v√† debugging

EDUCATIONAL VALUE:
‚úÖ Practical machine learning implementation
‚úÖ Traditional game AI algorithms
‚úÖ Modern software engineering practices
‚úÖ Performance optimization techniques
‚úÖ Automated testing methodologies

FUTURE ENHANCEMENTS:
üöÄ Deep Q-Learning (DQN) integration
üöÄ Multi-agent reinforcement learning
üöÄ Neural network-based evaluation functions
üöÄ Online learning v√† adaptation
üöÄ Transfer learning between game variants

D·ª± √°n n√†y demonstrating successful integration c·ªßa classical AI methods v·ªõi
modern machine learning, t·∫°o ra m·ªôt intelligent, adaptive, v√† highly
interactive gaming experience.

================================================================================
                            APPENDIX
================================================================================

FILES LI√äN QUAN:
- Assets/scripts/QLearningAgent.cs - Q-Learning implementation
- Assets/scripts/QuoridorAI.cs - Main AI controller
- Assets/scripts/DecisionTreeClassifier.cs - Decision tree logic
- Assets/scripts/QuoridorSelfPlayTrainer.cs - Training system
- Assets/scripts/AITestScript.cs - Testing framework
- Assets/scripts/Editor/ForceRetrainAI.cs - Development tools

Q-TABLE FILES:
- StreamingAssets/qtable.json - Balanced style
- StreamingAssets/qtable_aggressive.json - Aggressive style
- StreamingAssets/qtable_defensive.json - Defensive style
- StreamingAssets/qtable_wallmaster.json - Wall Master style
- StreamingAssets/qtable_speedrunner.json - Speed Runner style
- StreamingAssets/qtable_tactical.json - Tactical style

DOCUMENTATION:
- AI_ALGORITHMS_DETAILED_DESCRIPTION.txt - This file
- README.md - Project overview
- API_INTEGRATION_GUIDE.md - Voice/Chat integration
- TRAINING_GUIDE.md - AI training instructions

================================================================================
================================================================================
                    AI QUORIDOR GAME PROJECT - THU·∫¨T TO√ÅN AI CHI TI·∫æT
================================================================================

D·ª± √°n: AI Quoridor Game v·ªõi Machine Learning v√† Multi-Platform Integration
Ng√†y t·∫°o: 25/07/2025
T√°c gi·∫£: [Student Name]
Platform: Unity 3D
Language: C#

================================================================================
                            M·ª§C L·ª§C
================================================================================

1. T·ªîNG QUAN KI·∫æN TR√öC AI
2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
4. SPECIALIZED AI STYLES SYSTEM
5. DECISION TREE CLASSIFIER
6. ENTROPY ANALYSIS & SOFTMAX
7. TRAINING SYSTEM & SELF-PLAY
8. AI TESTING FRAMEWORK
9. HYBRID AI ARCHITECTURE
10. PERFORMANCE OPTIMIZATION

================================================================================
                        1. T·ªîNG QUAN KI·∫æN TR√öC AI
================================================================================

D·ª± √°n AI Quoridor s·ª≠ d·ª•ng m·ªôt ki·∫øn tr√∫c AI hybrid ti√™n ti·∫øn, k·∫øt h·ª£p:

1.1 CORE ALGORITHMS:
- Q-Learning (Reinforcement Learning) - Thu·∫≠t to√°n ch√≠nh
- Minimax + Alpha-Beta Pruning - Thu·∫≠t to√°n fallback
- Decision Tree Classifier - Ph√¢n t√≠ch ng·ªØ c·∫£nh game
- Entropy Analysis - ƒê√°nh gi√° ƒëa d·∫°ng move

1.2 SPECIALIZED SYSTEMS:
- 6 AI Personality Styles v·ªõi training ri√™ng bi·ªát
- Self-Play Training Pipeline t·ª± ƒë·ªông
- Real-time Performance Testing Framework
- Dynamic Algorithm Switching

1.3 TECHNICAL FOUNDATION:
- State Space: 13,118+ unique game states
- Action Space: Movement + Wall placement combinations
- Training Method: Self-play v·ªõi 1000 episodes
- Learning Rate: Œ± = 0.1, Discount Factor: Œ≥ = 0.99

================================================================================
                    2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
================================================================================

2.1 GI·ªöI THI·ªÜU Q-LEARNING:
Q-Learning l√† thu·∫≠t to√°n Reinforcement Learning kh√¥ng c·∫ßn model, h·ªçc th√¥ng qua
t∆∞∆°ng t√°c v·ªõi m√¥i tr∆∞·ªùng ƒë·ªÉ t·ªëi ∆∞u h√≥a policy. Trong Quoridor, AI h·ªçc c√°ch
ch·ªçn moves t·ªëi ∆∞u d·ª±a tr√™n kinh nghi·ªám t·ª´ h√†ng ng√†n game.

2.2 C√îNG TH·ª®C TO√ÅN H·ªåC:
Q(s,a) = Q(s,a) + Œ±[r + Œ≥*max(Q(s',a')) - Q(s,a)]

Trong ƒë√≥:
- Q(s,a): Q-value c·ªßa state s v·ªõi action a
- Œ±: Learning rate (0.1) - t·ªëc ƒë·ªô h·ªçc
- r: Immediate reward t·ª´ action
- Œ≥: Discount factor (0.99) - t·∫ßm quan tr·ªçng future rewards
- s': Next state sau khi th·ª±c hi·ªán action
- max(Q(s',a')): Maximum Q-value c·ªßa next state

2.3 IMPLEMENTATION CHI TI·∫æT:

```csharp
public class QLearningAgent
{
    // Core Q-Table: Dictionary l∆∞u tr·ªØ Q-values
    private Dictionary<string, Dictionary<string, float>> qTable;
    
    // Hyperparameters
    private float learningRate = 0.1f;      // T·ªëc ƒë·ªô h·ªçc
    private float discountFactor = 0.99f;   // Discount future rewards
    private float epsilon = 0.995f;         // Exploration rate
    
    // Q-Learning Update Implementation
    public void UpdateQValue(string state, string action, float reward, string nextState)
    {
        // L·∫•y Q-value hi·ªán t·∫°i
        float currentQ = GetQValue(state, action);
        
        // T√¨m max Q-value c·ªßa next state
        float maxNextQ = GetMaxQValue(nextState);
        
        // √Åp d·ª•ng c√¥ng th·ª©c Q-Learning
        float newQ = currentQ + learningRate * (reward + discountFactor * maxNextQ - currentQ);
        
        // C·∫≠p nh·∫≠t Q-table
        SetQValue(state, action, newQ);
    }
}
```

2.4 STATE ENCODING:
Game state ƒë∆∞·ª£c encode th√†nh string duy nh·∫•t:
"aiPos_x,aiPos_y-humanPos_x,humanPos_y-aiWalls-humanWalls-wallsList"

V√≠ d·ª•: "4,8-4,0-9-10-7,4,False|5,6,True"
- AI ·ªü (4,8), Human ·ªü (4,0)
- AI c√≥ 9 walls, Human c√≥ 10 walls
- Walls ƒë√£ ƒë·∫∑t: (7,4,False), (5,6,True)

2.5 ACTION ENCODING:
Actions ƒë∆∞·ª£c encode th√†nh string:
- Movement: "M:x,y" (v√≠ d·ª•: "M:4,7")
- Wall: "W:x,y,horizontal" (v√≠ d·ª•: "W:3,5,True")

2.6 EPSILON-GREEDY STRATEGY:
```csharp
public AIMove ChooseAction(GameState state, List<AIMove> possibleMoves)
{
    // Exploration vs Exploitation trade-off
    if (Random.Range(0f, 1f) < epsilon)
    {
        // EXPLORATION: Ch·ªçn random ƒë·ªÉ kh√°m ph√°
        return possibleMoves[Random.Range(0, possibleMoves.Count)];
    }
    else
    {
        // EXPLOITATION: Ch·ªçn action c√≥ Q-value cao nh·∫•t
        return GetBestAction(state, possibleMoves);
    }
}
```

2.7 EPSILON DECAY:
Epsilon gi·∫£m d·∫ßn t·ª´ 0.995 ‚Üí 0.1 qua training:
- Ban ƒë·∫ßu: High exploration (99.5%)
- Cu·ªëi training: High exploitation (90%)

2.8 REWARD SYSTEM:
- Win game: +1000
- Lose game: -1000
- Move toward goal: +10
- Block opponent: +50
- Waste move: -5
- Strategic position: +15

================================================================================
                3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
================================================================================

3.1 GI·ªöI THI·ªÜU MINIMAX:
Minimax l√† thu·∫≠t to√°n game theory c·ªï ƒëi·ªÉn, t√¨m ki·∫øm move t·ªëi ∆∞u b·∫±ng c√°ch
d·ª± ƒëo√°n tr∆∞·ªõc nhi·ªÅu moves. Alpha-Beta pruning t·ªëi ∆∞u hi·ªáu su·∫•t b·∫±ng c√°ch
lo·∫°i b·ªè c√°c nh√°nh kh√¥ng c·∫ßn thi·∫øt.

3.2 NGUY√äN L√ù HO·∫†T ƒê·ªòNG:
- Maximizing Player (AI): T·ªëi ƒëa h√≥a score
- Minimizing Player (Human): T·ªëi thi·ªÉu h√≥a score AI
- Search depth: 4 levels (configurable)
- Pruning: Lo·∫°i b·ªè nh√°nh kh√¥ng promising

3.3 IMPLEMENTATION:

```csharp
float Minimax(GameState state, int depth, bool isMaximizing, float alpha, float beta)
{
    // Base case: Depth = 0 ho·∫∑c game over
    if (depth == 0 || IsGameOver(state))
        return EvaluatePosition(state);
    
    if (isMaximizing) // AI turn
    {
        float maxEval = float.NegativeInfinity;
        foreach (var move in GeneratePossibleMoves(state, true))
        {
            GameState newState = ApplyMove(state, move, true);
            float eval = Minimax(newState, depth - 1, false, alpha, beta);
            maxEval = Mathf.Max(maxEval, eval);
            alpha = Mathf.Max(alpha, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return maxEval;
    }
    else // Human turn
    {
        float minEval = float.PositiveInfinity;
        foreach (var move in GeneratePossibleMoves(state, false))
        {
            GameState newState = ApplyMove(state, move, false);
            float eval = Minimax(newState, depth - 1, true, alpha, beta);
            minEval = Mathf.Min(minEval, eval);
            beta = Mathf.Min(beta, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return minEval;
    }
}
```

3.4 EVALUATION FUNCTION:
Position evaluation d·ª±a tr√™n multiple factors:

```csharp
float EvaluatePosition(GameState state)
{
    float score = 0f;
    
    // Distance to goal (c√†ng g·∫ßn c√†ng t·ªët)
    int aiDistance = GetShortestPath(state.aiPosition, goalRow);
    int humanDistance = GetShortestPath(state.humanPosition, goalRow);
    score += (humanDistance - aiDistance) * 100;
    
    // Wall advantage
    score += (state.aiWallsLeft - state.humanWallsLeft) * 50;
    
    // Mobility (s·ªë moves c√≥ th·ªÉ)
    int aiMobility = GetValidMoves(state.aiPosition).Count;
    int humanMobility = GetValidMoves(state.humanPosition).Count;
    score += (aiMobility - humanMobility) * 20;
    
    // Center control
    score += GetCenterControlScore(state) * 30;
    
    return score;
}
```

3.5 ALPHA-BETA PRUNING HI·ªÜU QU·∫¢:
- Reduces search space t·ª´ O(b^d) xu·ªëng O(b^(d/2))
- Best case: 50% reduction in nodes evaluated
- Ordering moves by promising gi√∫p pruning hi·ªáu qu·∫£ h∆°n

================================================================================
                        4. SPECIALIZED AI STYLES SYSTEM
================================================================================

4.1 CONCEPT:
6 AI personalities v·ªõi training v√† behavior ri√™ng bi·ªát, m·ªói style c√≥:
- Ri√™ng Q-table file
- Custom reward structure
- Specific move prioritization
- Unique decision making logic

4.2 AI STYLES CHI TI·∫æT:

üî• AGGRESSIVE STYLE:
- M·ª•c ti√™u: T·∫•n c√¥ng v√† ch·∫∑n ƒë·ªëi th·ªß
- Wall Usage: 70% chance
- Reward Structure:
  * blockingOpponentReward = 50 * intensity
  * wallPlacementReward = 30 * intensity
  * defensiveWallPenalty = -20 * intensity
- Q-table: qtable_aggressive.json

üõ°Ô∏è DEFENSIVE STYLE:
- M·ª•c ti√™u: Ph√≤ng th·ªß v√† di chuy·ªÉn an to√†n
- Wall Usage: 50% chance
- Reward Structure:
  * defensiveWallReward = 35 * intensity
  * movementReward = 20 * intensity
  * aggressiveMovePenalty = -15 * intensity
- Q-table: qtable_defensive.json

üß± WALL MASTER STYLE:
- M·ª•c ti√™u: Chuy√™n gia ƒë·∫∑t t∆∞·ªùng v√† t·∫°o m√™ cung
- Wall Usage: 90% chance
- Reward Structure:
  * wallPlacementReward = 60 * intensity
  * strategicWallReward = 40 * intensity
  * wallComboReward = 50 * intensity
  * movementWithoutWallPenalty = -25 * intensity
- Q-table: qtable_wallmaster.json

‚ö° SPEED RUNNER STYLE:
- M·ª•c ti√™u: ƒê·∫°t ƒë√≠ch nhanh nh·∫•t
- Wall Usage: 10% chance
- Reward Structure:
  * movementReward = 40 * intensity
  * progressToGoalReward = 50 * intensity
  * quickWinBonus = 100 * intensity
  * wallPlacementPenalty = -20 * intensity
- Q-table: qtable_speedrunner.json

üß† TACTICAL STYLE:
- M·ª•c ti√™u: L·ª£i th·∫ø v·ªã tr√≠ v√† k·∫ø ho·∫°ch d√†i h·∫°n
- Wall Usage: Variable (strategic)
- Reward Structure:
  * tacticalMoveReward = 45 * intensity
  * positionAdvantageReward = 35 * intensity
  * mobilityReward = 25 * intensity
  * patternRecognitionBonus = 40 * intensity
- Q-table: qtable_tactical.json

‚öñÔ∏è BALANCED STYLE:
- M·ª•c ti√™u: C√¢n b·∫±ng t·∫•t c·∫£ aspects
- Wall Usage: 30% chance
- Reward Structure: Default balanced values
- Q-table: qtable.json

4.3 STYLE SWITCHING MECHANISM:

```csharp
public void SwitchToStyle(PlayStyle newStyle)
{
    // Save current Q-table
    if (allowQTableSaving) qAgent.SaveQTable(GetStyleSpecificQTablePath());
    
    // Load new style Q-table
    specializedStyle = newStyle;
    qAgent.LoadQTable(GetStyleSpecificQTablePath());
    
    // Reconfigure parameters
    ConfigureStyleSpecificTraining();
}
```

4.4 STYLE INTENSITY:
Parameter t·ª´ 0.0 ‚Üí 1.0 control m·ª©c ƒë·ªô chuy√™n h√≥a:
- 0.0: Balanced behavior
- 0.5: Moderate specialization
- 1.0: Extreme specialization

================================================================================
                        5. DECISION TREE CLASSIFIER
================================================================================

5.1 PURPOSE:
Decision Tree ph√¢n t√≠ch game context ƒë·ªÉ ch·ªçn strategy ph√π h·ª£p, d·ª±a tr√™n:
- Game phase (Early/Mid/End)
- Position advantage
- Resource advantage
- Mobility situation

5.2 FEATURE EXTRACTION:

```csharp
public struct DecisionFeatures
{
    public int aiDistanceToGoal;      // AI distance to goal line
    public int humanDistanceToGoal;   // Human distance to goal line
    public int aiWallsLeft;           // AI walls remaining
    public int humanWallsLeft;        // Human walls remaining
    public int aiMobility;            // AI possible moves
    public int humanMobility;         // Human possible moves
    public float distanceAdvantage;   // Human - AI distance
    public float wallAdvantage;       // AI - Human walls
    public bool isEarlyGame;          // > 15 moves estimated remaining
    public bool isMidGame;            // 5-15 moves remaining
    public bool isEndGame;            // < 5 moves remaining
}
```

5.3 DECISION LOGIC:

```csharp
public AIStrategy Predict(DecisionFeatures features)
{
    // Early Game: Play conservatively
    if (features.isEarlyGame)
        return AIStrategy.Balanced;
    
    // AI has significant lead: Focus on blocking
    if (features.distanceAdvantage > 2)
        return AIStrategy.Blocking;
    
    // AI is behind: Be aggressive
    if (features.distanceAdvantage < -1)
        return AIStrategy.Aggressive;
    
    // End game with wall advantage: Play defensively
    if (features.isEndGame && features.wallAdvantage > 2)
        return AIStrategy.Defensive;
    
    // Close game: Balanced approach
    return AIStrategy.Balanced;
}
```

5.4 STRATEGY OUTCOMES:
- Balanced: Equal weight to movement v√† walls
- Blocking: Prioritize moves that increase opponent path
- Aggressive: Focus on attacking positions
- Defensive: Protect own position and path

================================================================================
                        6. ENTROPY ANALYSIS & SOFTMAX
================================================================================

6.1 ENTROPY CONCEPT:
Entropy ƒëo ƒë·ªô ƒëa d·∫°ng/uncertainty c·ªßa move scores. High entropy = nhi·ªÅu moves
c√≥ scores t∆∞∆°ng ƒë∆∞∆°ng, low entropy = m·ªôt move r√µ r√†ng t·ªët nh·∫•t.

6.2 ENTROPY CALCULATION:

```csharp
float CalculateEntropy(List<float> moveScores)
{
    // Normalize th√†nh probabilities
    float total = moveScores.Sum();
    var probabilities = moveScores.Select(s => s / total);
    
    // Calculate Shannon entropy
    float entropy = 0f;
    foreach (float p in probabilities)
    {
        if (p > 0f)
            entropy -= p * Mathf.Log(p, 2f);
    }
    
    return entropy; // Range: 0 (deterministic) to log2(n) (uniform)
}
```

6.3 SOFTMAX DISTRIBUTION:

```csharp
float[] CalculateSoftmax(List<float> scores, float temperature)
{
    float[] probabilities = new float[scores.Count];
    float sum = scores.Sum(s => Mathf.Exp(s / temperature));
    
    for (int i = 0; i < scores.Count; i++)
    {
        probabilities[i] = Mathf.Exp(scores[i] / temperature) / sum;
    }
    
    return probabilities;
}
```

6.4 TEMPERATURE PARAMETER:
- Low temperature (0.1): Sharp distribution, focus on best moves
- High temperature (2.0): Flat distribution, more exploration
- Default temperature (1.0): Balanced exploitation/exploration

6.5 ADAPTIVE MOVE SELECTION:

```csharp
public AIMove ChooseActionWithEntropy(GameState state, List<AIMove> possibleMoves)
{
    var moveScores = EvaluateAllMoves(state, possibleMoves);
    float entropy = CalculateEntropy(moveScores);
    
    if (entropy > entropyThreshold) // High diversity
    {
        // Use softmax for probabilistic selection
        var probabilities = CalculateSoftmax(moveScores, softmaxTemperature);
        return SelectMoveByProbability(possibleMoves, probabilities);
    }
    else // Clear best move
    {
        // Use deterministic best move
        return GetBestScoredMove(possibleMoves, moveScores);
    }
}
```

================================================================================
                        7. TRAINING SYSTEM & SELF-PLAY
================================================================================

7.1 SELF-PLAY CONCEPT:
AI h·ªçc b·∫±ng c√°ch ch∆°i v·ªõi ch√≠nh n√≥ h√†ng ng√†n l·∫ßn, gradually improve performance
th√¥ng qua trial v√† error.

7.2 TRAINING PIPELINE:

```csharp
IEnumerator RunSelfPlayTraining()
{
    for (int episode = 1; episode <= maxEpisodes; episode++)
    {
        // Reset game state
        ResetGameForTraining();
        
        while (!IsGameOver())
        {
            // Get current state
            GameState currentState = GetCurrentGameState();
            List<AIMove> possibleMoves = GetPossibleMoves();
            
            // Choose action using Œµ-greedy
            AIMove chosenMove = qLearningAgent.ChooseAction(currentState, possibleMoves);
            
            // Execute move
            ExecuteMove(chosenMove);
            
            // Calculate reward
            float reward = CalculateReward(currentState, chosenMove);
            
            // Update Q-table
            GameState newState = GetCurrentGameState();
            qLearningAgent.UpdateQValue(
                EncodeState(currentState),
                EncodeAction(chosenMove),
                reward,
                EncodeState(newState)
            );
            
            yield return new WaitForEndOfFrame();
        }
        
        // Epsilon decay
        qLearningAgent.DecayEpsilon();
        
        // Save progress periodically
        if (episode % 100 == 0)
        {
            qLearningAgent.SaveQTable();
        }
    }
}
```

7.3 REWARD ENGINEERING:
Critical cho Q-Learning success:

```csharp
float CalculateReward(GameState state, AIMove move)
{
    float reward = 0f;
    
    // Game outcome rewards
    if (IsWin(state)) reward += 1000f;
    if (IsLoss(state)) reward -= 1000f;
    
    // Progress rewards
    if (move.moveType == MoveType.Movement)
    {
        if (IsMoveTowardGoal(move)) reward += 20f;
        else reward -= 5f;
    }
    
    // Strategic rewards
    if (move.moveType == MoveType.WallPlacement)
    {
        if (IsWallBlockingOpponent(move)) reward += 50f;
        if (IsWallStrategic(move)) reward += 30f;
        if (IsWallWasted(move)) reward -= 25f;
    }
    
    // Style-specific rewards
    reward += GetStyleSpecificReward(state, move);
    
    return reward;
}
```

7.4 TRAINING METRICS:
- Episodes: 1000 standard
- Duration: ~15 minutes
- Convergence: Usually around episode 800
- Q-table size: 13,118+ states at completion
- Final epsilon: 0.1 (90% exploitation)

================================================================================
                        8. AI TESTING FRAMEWORK
================================================================================

8.1 TESTING PHILOSOPHY:
Comprehensive testing ensures AI quality v√† reliability across different
game scenarios v√† edge cases.

8.2 TEST CATEGORIES:

A) DECISION TREE TESTS:
```csharp
void TestDecisionTreeStrategies()
{
    // Test Early Game
    var earlyGameFeatures = new DecisionFeatures
    {
        aiDistanceToGoal = 8, humanDistanceToGoal = 8,
        isEarlyGame = true
    };
    var strategy = decisionTree.Predict(earlyGameFeatures);
    Assert.AreEqual(AIStrategy.Balanced, strategy);
    
    // Test AI Leading
    var leadingFeatures = new DecisionFeatures
    {
        distanceAdvantage = 3, // AI leading
        isMidGame = true
    };
    strategy = decisionTree.Predict(leadingFeatures);
    Assert.AreEqual(AIStrategy.Blocking, strategy);
}
```

B) ENTROPY ANALYSIS TESTS:
```csharp
void TestEntropyAnalysis()
{
    // High entropy case (diverse scores)
    var highEntropyScores = new List<float> { 100f, 98f, 97f, 95f, 94f };
    float entropy = CalculateTestEntropy(highEntropyScores);
    Assert.IsTrue(entropy > 0.5f);
    
    // Low entropy case (clear winner)
    var lowEntropyScores = new List<float> { 100f, 50f, 50f, 50f, 50f };
    entropy = CalculateTestEntropy(lowEntropyScores);
    Assert.IsTrue(entropy < 0.5f);
}
```

C) SOFTMAX DISTRIBUTION TESTS:
```csharp
void TestSoftmaxDistribution()
{
    var testScores = new List<float> { 100f, 98f, 95f, 90f };
    float temperature = 1.0f;
    
    var probabilities = CalculateSoftmax(testScores, temperature);
    
    // Verify probabilities sum to 1
    float sum = probabilities.Sum();
    Assert.AreEqual(1.0f, sum, 0.001f);
    
    // Verify highest score gets highest probability
    int maxIndex = Array.IndexOf(probabilities, probabilities.Max());
    Assert.AreEqual(0, maxIndex);
}
```

D) PERFORMANCE BENCHMARKS:
- Move generation time: <100ms
- Q-Learning decision: <50ms
- Minimax decision (depth 4): <200ms
- Memory usage: <256MB

8.3 AUTOMATED TESTING:
Tests run automatically ƒë·ªÉ ensure code quality:
- Unit tests cho individual algorithms
- Integration tests cho AI system interactions
- Performance tests cho response times
- Regression tests ƒë·ªÉ prevent quality degradation

================================================================================
                        9. HYBRID AI ARCHITECTURE
================================================================================

9.1 CONCEPT:
Hybrid system k·∫øt h·ª£p strengths c·ªßa multiple algorithms:
- Q-Learning: Data-driven, learns from experience
- Minimax: Logical, guaranteed optimal given evaluation
- Decision Tree: Fast, context-aware switching
- Entropy: Adaptive, handles uncertainty

9.2 ALGORITHM SELECTION LOGIC:

```csharp
public AIMove GetBestMove()
{
    UpdateGameState();
    List<AIMove> possibleMoves = GeneratePossibleMoves();
    
    // Primary: Use Q-Learning if trained model available
    if (useQLearning && qAgent.IsTrainedModel())
    {
        Debug.Log($"üß† Using {specializedStyle} Q-Learning");
        return qAgent.ChooseAction(currentGameState, possibleMoves);
    }
    
    // Fallback: Use Minimax for guaranteed reasonable play
    else
    {
        Debug.Log($"üîç Using {specializedStyle} Minimax");
        return FindBestMoveWithMinimax();
    }
}
```

9.3 SEAMLESS SWITCHING:
- No interruption in gameplay
- Maintains consistent API
- Preserves game state integrity
- Logs algorithm used for debugging

9.4 BENEFITS:
- Robustness: Fallback ensures AI never fails
- Performance: Q-Learning for trained scenarios, Minimax for edge cases
- Flexibility: Can switch algorithms based on situation
- Development: Easy to test individual components

================================================================================
                        10. PERFORMANCE OPTIMIZATION
================================================================================

10.1 Q-TABLE OPTIMIZATION:
- Sparse representation: Only store visited states
- Compression: String interning for state keys
- Lazy loading: Load Q-table on demand
- Periodic saving: Every 100 training episodes

10.2 MINIMAX OPTIMIZATION:
- Alpha-Beta pruning: 50% search reduction
- Move ordering: Promising moves first
- Iterative deepening: Gradual depth increase
- Transposition tables: Cache evaluated positions

10.3 MEMORY MANAGEMENT:
- Object pooling cho GameState objects
- String caching cho repeated state/action encodings
- Garbage collection optimization
- Memory profiling v√† monitoring

10.4 MULTITHREADING CONSIDERATIONS:
- Q-Learning training: Background thread
- Move calculation: Main thread (Unity requirement)
- File I/O: Async operations where possible
- Thread-safe collections cho shared data

10.5 CACHING STRATEGIES:
- Path finding results
- Move generation results
- Evaluation function results
- Feature extraction results

================================================================================
                            K·∫æT LU·∫¨N
================================================================================

D·ª± √°n AI Quoridor Project th·ªÉ hi·ªán m·ªôt implementation comprehensive c·ªßa modern
game AI techniques:

TECHNICAL INNOVATIONS:
‚úÖ Hybrid Q-Learning + Minimax architecture
‚úÖ 6 specialized AI personalities v·ªõi separate training
‚úÖ Advanced decision making v·ªõi entropy analysis
‚úÖ Comprehensive testing v√† validation framework
‚úÖ Professional development tools v√† debugging

EDUCATIONAL VALUE:
‚úÖ Practical machine learning implementation
‚úÖ Traditional game AI algorithms
‚úÖ Modern software engineering practices
‚úÖ Performance optimization techniques
‚úÖ Automated testing methodologies

FUTURE ENHANCEMENTS:
üöÄ Deep Q-Learning (DQN) integration
üöÄ Multi-agent reinforcement learning
üöÄ Neural network-based evaluation functions
üöÄ Online learning v√† adaptation
üöÄ Transfer learning between game variants

D·ª± √°n n√†y demonstrating successful integration c·ªßa classical AI methods v·ªõi
modern machine learning, t·∫°o ra m·ªôt intelligent, adaptive, v√† highly
interactive gaming experience.

================================================================================
                            APPENDIX
================================================================================

FILES LI√äN QUAN:
- Assets/scripts/QLearningAgent.cs - Q-Learning implementation
- Assets/scripts/QuoridorAI.cs - Main AI controller
- Assets/scripts/DecisionTreeClassifier.cs - Decision tree logic
- Assets/scripts/QuoridorSelfPlayTrainer.cs - Training system
- Assets/scripts/AITestScript.cs - Testing framework
- Assets/scripts/Editor/ForceRetrainAI.cs - Development tools

Q-TABLE FILES:
- StreamingAssets/qtable.json - Balanced style
- StreamingAssets/qtable_aggressive.json - Aggressive style
- StreamingAssets/qtable_defensive.json - Defensive style
- StreamingAssets/qtable_wallmaster.json - Wall Master style
- StreamingAssets/qtable_speedrunner.json - Speed Runner style
- StreamingAssets/qtable_tactical.json - Tactical style

DOCUMENTATION:
- AI_ALGORITHMS_DETAILED_DESCRIPTION.txt - This file
- README.md - Project overview
- API_INTEGRATION_GUIDE.md - Voice/Chat integration
- TRAINING_GUIDE.md - AI training instructions

================================================================================
================================================================================
                    AI QUORIDOR GAME PROJECT - THU·∫¨T TO√ÅN AI CHI TI·∫æT
================================================================================

D·ª± √°n: AI Quoridor Game v·ªõi Machine Learning v√† Multi-Platform Integration
Ng√†y t·∫°o: 25/07/2025
T√°c gi·∫£: [Student Name]
Platform: Unity 3D
Language: C#

================================================================================
                            M·ª§C L·ª§C
================================================================================

1. T·ªîNG QUAN KI·∫æN TR√öC AI
2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
4. SPECIALIZED AI STYLES SYSTEM
5. DECISION TREE CLASSIFIER
6. ENTROPY ANALYSIS & SOFTMAX
7. TRAINING SYSTEM & SELF-PLAY
8. AI TESTING FRAMEWORK
9. HYBRID AI ARCHITECTURE
10. PERFORMANCE OPTIMIZATION

================================================================================
                        1. T·ªîNG QUAN KI·∫æN TR√öC AI
================================================================================

D·ª± √°n AI Quoridor s·ª≠ d·ª•ng m·ªôt ki·∫øn tr√∫c AI hybrid ti√™n ti·∫øn, k·∫øt h·ª£p:

1.1 CORE ALGORITHMS:
- Q-Learning (Reinforcement Learning) - Thu·∫≠t to√°n ch√≠nh
- Minimax + Alpha-Beta Pruning - Thu·∫≠t to√°n fallback
- Decision Tree Classifier - Ph√¢n t√≠ch ng·ªØ c·∫£nh game
- Entropy Analysis - ƒê√°nh gi√° ƒëa d·∫°ng move

1.2 SPECIALIZED SYSTEMS:
- 6 AI Personality Styles v·ªõi training ri√™ng bi·ªát
- Self-Play Training Pipeline t·ª± ƒë·ªông
- Real-time Performance Testing Framework
- Dynamic Algorithm Switching

1.3 TECHNICAL FOUNDATION:
- State Space: 13,118+ unique game states
- Action Space: Movement + Wall placement combinations
- Training Method: Self-play v·ªõi 1000 episodes
- Learning Rate: Œ± = 0.1, Discount Factor: Œ≥ = 0.99

================================================================================
                    2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
================================================================================

2.1 GI·ªöI THI·ªÜU Q-LEARNING:
Q-Learning l√† thu·∫≠t to√°n Reinforcement Learning kh√¥ng c·∫ßn model, h·ªçc th√¥ng qua
t∆∞∆°ng t√°c v·ªõi m√¥i tr∆∞·ªùng ƒë·ªÉ t·ªëi ∆∞u h√≥a policy. Trong Quoridor, AI h·ªçc c√°ch
ch·ªçn moves t·ªëi ∆∞u d·ª±a tr√™n kinh nghi·ªám t·ª´ h√†ng ng√†n game.

2.2 C√îNG TH·ª®C TO√ÅN H·ªåC:
Q(s,a) = Q(s,a) + Œ±[r + Œ≥*max(Q(s',a')) - Q(s,a)]

Trong ƒë√≥:
- Q(s,a): Q-value c·ªßa state s v·ªõi action a
- Œ±: Learning rate (0.1) - t·ªëc ƒë·ªô h·ªçc
- r: Immediate reward t·ª´ action
- Œ≥: Discount factor (0.99) - t·∫ßm quan tr·ªçng future rewards
- s': Next state sau khi th·ª±c hi·ªán action
- max(Q(s',a')): Maximum Q-value c·ªßa next state

2.3 IMPLEMENTATION CHI TI·∫æT:

```csharp
public class QLearningAgent
{
    // Core Q-Table: Dictionary l∆∞u tr·ªØ Q-values
    private Dictionary<string, Dictionary<string, float>> qTable;
    
    // Hyperparameters
    private float learningRate = 0.1f;      // T·ªëc ƒë·ªô h·ªçc
    private float discountFactor = 0.99f;   // Discount future rewards
    private float epsilon = 0.995f;         // Exploration rate
    
    // Q-Learning Update Implementation
    public void UpdateQValue(string state, string action, float reward, string nextState)
    {
        // L·∫•y Q-value hi·ªán t·∫°i
        float currentQ = GetQValue(state, action);
        
        // T√¨m max Q-value c·ªßa next state
        float maxNextQ = GetMaxQValue(nextState);
        
        // √Åp d·ª•ng c√¥ng th·ª©c Q-Learning
        float newQ = currentQ + learningRate * (reward + discountFactor * maxNextQ - currentQ);
        
        // C·∫≠p nh·∫≠t Q-table
        SetQValue(state, action, newQ);
    }
}
```

2.4 STATE ENCODING:
Game state ƒë∆∞·ª£c encode th√†nh string duy nh·∫•t:
"aiPos_x,aiPos_y-humanPos_x,humanPos_y-aiWalls-humanWalls-wallsList"

V√≠ d·ª•: "4,8-4,0-9-10-7,4,False|5,6,True"
- AI ·ªü (4,8), Human ·ªü (4,0)
- AI c√≥ 9 walls, Human c√≥ 10 walls
- Walls ƒë√£ ƒë·∫∑t: (7,4,False), (5,6,True)

2.5 ACTION ENCODING:
Actions ƒë∆∞·ª£c encode th√†nh string:
- Movement: "M:x,y" (v√≠ d·ª•: "M:4,7")
- Wall: "W:x,y,horizontal" (v√≠ d·ª•: "W:3,5,True")

2.6 EPSILON-GREEDY STRATEGY:
```csharp
public AIMove ChooseAction(GameState state, List<AIMove> possibleMoves)
{
    // Exploration vs Exploitation trade-off
    if (Random.Range(0f, 1f) < epsilon)
    {
        // EXPLORATION: Ch·ªçn random ƒë·ªÉ kh√°m ph√°
        return possibleMoves[Random.Range(0, possibleMoves.Count)];
    }
    else
    {
        // EXPLOITATION: Ch·ªçn action c√≥ Q-value cao nh·∫•t
        return GetBestAction(state, possibleMoves);
    }
}
```

2.7 EPSILON DECAY:
Epsilon gi·∫£m d·∫ßn t·ª´ 0.995 ‚Üí 0.1 qua training:
- Ban ƒë·∫ßu: High exploration (99.5%)
- Cu·ªëi training: High exploitation (90%)

2.8 REWARD SYSTEM:
- Win game: +1000
- Lose game: -1000
- Move toward goal: +10
- Block opponent: +50
- Waste move: -5
- Strategic position: +15

================================================================================
                3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
================================================================================

3.1 GI·ªöI THI·ªÜU MINIMAX:
Minimax l√† thu·∫≠t to√°n game theory c·ªï ƒëi·ªÉn, t√¨m ki·∫øm move t·ªëi ∆∞u b·∫±ng c√°ch
d·ª± ƒëo√°n tr∆∞·ªõc nhi·ªÅu moves. Alpha-Beta pruning t·ªëi ∆∞u hi·ªáu su·∫•t b·∫±ng c√°ch
lo·∫°i b·ªè c√°c nh√°nh kh√¥ng c·∫ßn thi·∫øt.

3.2 NGUY√äN L√ù HO·∫†T ƒê·ªòNG:
- Maximizing Player (AI): T·ªëi ƒëa h√≥a score
- Minimizing Player (Human): T·ªëi thi·ªÉu h√≥a score AI
- Search depth: 4 levels (configurable)
- Pruning: Lo·∫°i b·ªè nh√°nh kh√¥ng promising

3.3 IMPLEMENTATION:

```csharp
float Minimax(GameState state, int depth, bool isMaximizing, float alpha, float beta)
{
    // Base case: Depth = 0 ho·∫∑c game over
    if (depth == 0 || IsGameOver(state))
        return EvaluatePosition(state);
    
    if (isMaximizing) // AI turn
    {
        float maxEval = float.NegativeInfinity;
        foreach (var move in GeneratePossibleMoves(state, true))
        {
            GameState newState = ApplyMove(state, move, true);
            float eval = Minimax(newState, depth - 1, false, alpha, beta);
            maxEval = Mathf.Max(maxEval, eval);
            alpha = Mathf.Max(alpha, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return maxEval;
    }
    else // Human turn
    {
        float minEval = float.PositiveInfinity;
        foreach (var move in GeneratePossibleMoves(state, false))
        {
            GameState newState = ApplyMove(state, move, false);
            float eval = Minimax(newState, depth - 1, true, alpha, beta);
            minEval = Mathf.Min(minEval, eval);
            beta = Mathf.Min(beta, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return minEval;
    }
}
```

3.4 EVALUATION FUNCTION:
Position evaluation d·ª±a tr√™n multiple factors:

```csharp
float EvaluatePosition(GameState state)
{
    float score = 0f;
    
    // Distance to goal (c√†ng g·∫ßn c√†ng t·ªët)
    int aiDistance = GetShortestPath(state.aiPosition, goalRow);
    int humanDistance = GetShortestPath(state.humanPosition, goalRow);
    score += (humanDistance - aiDistance) * 100;
    
    // Wall advantage
    score += (state.aiWallsLeft - state.humanWallsLeft) * 50;
    
    // Mobility (s·ªë moves c√≥ th·ªÉ)
    int aiMobility = GetValidMoves(state.aiPosition).Count;
    int humanMobility = GetValidMoves(state.humanPosition).Count;
    score += (aiMobility - humanMobility) * 20;
    
    // Center control
    score += GetCenterControlScore(state) * 30;
    
    return score;
}
```

3.5 ALPHA-BETA PRUNING HI·ªÜU QU·∫¢:
- Reduces search space t·ª´ O(b^d) xu·ªëng O(b^(d/2))
- Best case: 50% reduction in nodes evaluated
- Ordering moves by promising gi√∫p pruning hi·ªáu qu·∫£ h∆°n

================================================================================
                        4. SPECIALIZED AI STYLES SYSTEM
================================================================================

4.1 CONCEPT:
6 AI personalities v·ªõi training v√† behavior ri√™ng bi·ªát, m·ªói style c√≥:
- Ri√™ng Q-table file
- Custom reward structure
- Specific move prioritization
- Unique decision making logic

4.2 AI STYLES CHI TI·∫æT:

üî• AGGRESSIVE STYLE:
- M·ª•c ti√™u: T·∫•n c√¥ng v√† ch·∫∑n ƒë·ªëi th·ªß
- Wall Usage: 70% chance
- Reward Structure:
  * blockingOpponentReward = 50 * intensity
  * wallPlacementReward = 30 * intensity
  * defensiveWallPenalty = -20 * intensity
- Q-table: qtable_aggressive.json

üõ°Ô∏è DEFENSIVE STYLE:
- M·ª•c ti√™u: Ph√≤ng th·ªß v√† di chuy·ªÉn an to√†n
- Wall Usage: 50% chance
- Reward Structure:
  * defensiveWallReward = 35 * intensity
  * movementReward = 20 * intensity
  * aggressiveMovePenalty = -15 * intensity
- Q-table: qtable_defensive.json

üß± WALL MASTER STYLE:
- M·ª•c ti√™u: Chuy√™n gia ƒë·∫∑t t∆∞·ªùng v√† t·∫°o m√™ cung
- Wall Usage: 90% chance
- Reward Structure:
  * wallPlacementReward = 60 * intensity
  * strategicWallReward = 40 * intensity
  * wallComboReward = 50 * intensity
  * movementWithoutWallPenalty = -25 * intensity
- Q-table: qtable_wallmaster.json

‚ö° SPEED RUNNER STYLE:
- M·ª•c ti√™u: ƒê·∫°t ƒë√≠ch nhanh nh·∫•t
- Wall Usage: 10% chance
- Reward Structure:
  * movementReward = 40 * intensity
  * progressToGoalReward = 50 * intensity
  * quickWinBonus = 100 * intensity
  * wallPlacementPenalty = -20 * intensity
- Q-table: qtable_speedrunner.json

üß† TACTICAL STYLE:
- M·ª•c ti√™u: L·ª£i th·∫ø v·ªã tr√≠ v√† k·∫ø ho·∫°ch d√†i h·∫°n
- Wall Usage: Variable (strategic)
- Reward Structure:
  * tacticalMoveReward = 45 * intensity
  * positionAdvantageReward = 35 * intensity
  * mobilityReward = 25 * intensity
  * patternRecognitionBonus = 40 * intensity
- Q-table: qtable_tactical.json

‚öñÔ∏è BALANCED STYLE:
- M·ª•c ti√™u: C√¢n b·∫±ng t·∫•t c·∫£ aspects
- Wall Usage: 30% chance
- Reward Structure: Default balanced values
- Q-table: qtable.json

4.3 STYLE SWITCHING MECHANISM:

```csharp
public void SwitchToStyle(PlayStyle newStyle)
{
    // Save current Q-table
    if (allowQTableSaving) qAgent.SaveQTable(GetStyleSpecificQTablePath());
    
    // Load new style Q-table
    specializedStyle = newStyle;
    qAgent.LoadQTable(GetStyleSpecificQTablePath());
    
    // Reconfigure parameters
    ConfigureStyleSpecificTraining();
}
```

4.4 STYLE INTENSITY:
Parameter t·ª´ 0.0 ‚Üí 1.0 control m·ª©c ƒë·ªô chuy√™n h√≥a:
- 0.0: Balanced behavior
- 0.5: Moderate specialization
- 1.0: Extreme specialization

================================================================================
                        5. DECISION TREE CLASSIFIER
================================================================================

5.1 PURPOSE:
Decision Tree ph√¢n t√≠ch game context ƒë·ªÉ ch·ªçn strategy ph√π h·ª£p, d·ª±a tr√™n:
- Game phase (Early/Mid/End)
- Position advantage
- Resource advantage
- Mobility situation

5.2 FEATURE EXTRACTION:

```csharp
public struct DecisionFeatures
{
    public int aiDistanceToGoal;      // AI distance to goal line
    public int humanDistanceToGoal;   // Human distance to goal line
    public int aiWallsLeft;           // AI walls remaining
    public int humanWallsLeft;        // Human walls remaining
    public int aiMobility;            // AI possible moves
    public int humanMobility;         // Human possible moves
    public float distanceAdvantage;   // Human - AI distance
    public float wallAdvantage;       // AI - Human walls
    public bool isEarlyGame;          // > 15 moves estimated remaining
    public bool isMidGame;            // 5-15 moves remaining
    public bool isEndGame;            // < 5 moves remaining
}
```

5.3 DECISION LOGIC:

```csharp
public AIStrategy Predict(DecisionFeatures features)
{
    // Early Game: Play conservatively
    if (features.isEarlyGame)
        return AIStrategy.Balanced;
    
    // AI has significant lead: Focus on blocking
    if (features.distanceAdvantage > 2)
        return AIStrategy.Blocking;
    
    // AI is behind: Be aggressive
    if (features.distanceAdvantage < -1)
        return AIStrategy.Aggressive;
    
    // End game with wall advantage: Play defensively
    if (features.isEndGame && features.wallAdvantage > 2)
        return AIStrategy.Defensive;
    
    // Close game: Balanced approach
    return AIStrategy.Balanced;
}
```

5.4 STRATEGY OUTCOMES:
- Balanced: Equal weight to movement v√† walls
- Blocking: Prioritize moves that increase opponent path
- Aggressive: Focus on attacking positions
- Defensive: Protect own position and path

================================================================================
                        6. ENTROPY ANALYSIS & SOFTMAX
================================================================================

6.1 ENTROPY CONCEPT:
Entropy ƒëo ƒë·ªô ƒëa d·∫°ng/uncertainty c·ªßa move scores. High entropy = nhi·ªÅu moves
c√≥ scores t∆∞∆°ng ƒë∆∞∆°ng, low entropy = m·ªôt move r√µ r√†ng t·ªët nh·∫•t.

6.2 ENTROPY CALCULATION:

```csharp
float CalculateEntropy(List<float> moveScores)
{
    // Normalize th√†nh probabilities
    float total = moveScores.Sum();
    var probabilities = moveScores.Select(s => s / total);
    
    // Calculate Shannon entropy
    float entropy = 0f;
    foreach (float p in probabilities)
    {
        if (p > 0f)
            entropy -= p * Mathf.Log(p, 2f);
    }
    
    return entropy; // Range: 0 (deterministic) to log2(n) (uniform)
}
```

6.3 SOFTMAX DISTRIBUTION:

```csharp
float[] CalculateSoftmax(List<float> scores, float temperature)
{
    float[] probabilities = new float[scores.Count];
    float sum = scores.Sum(s => Mathf.Exp(s / temperature));
    
    for (int i = 0; i < scores.Count; i++)
    {
        probabilities[i] = Mathf.Exp(scores[i] / temperature) / sum;
    }
    
    return probabilities;
}
```

6.4 TEMPERATURE PARAMETER:
- Low temperature (0.1): Sharp distribution, focus on best moves
- High temperature (2.0): Flat distribution, more exploration
- Default temperature (1.0): Balanced exploitation/exploration

6.5 ADAPTIVE MOVE SELECTION:

```csharp
public AIMove ChooseActionWithEntropy(GameState state, List<AIMove> possibleMoves)
{
    var moveScores = EvaluateAllMoves(state, possibleMoves);
    float entropy = CalculateEntropy(moveScores);
    
    if (entropy > entropyThreshold) // High diversity
    {
        // Use softmax for probabilistic selection
        var probabilities = CalculateSoftmax(moveScores, softmaxTemperature);
        return SelectMoveByProbability(possibleMoves, probabilities);
    }
    else // Clear best move
    {
        // Use deterministic best move
        return GetBestScoredMove(possibleMoves, moveScores);
    }
}
```

================================================================================
                        7. TRAINING SYSTEM & SELF-PLAY
================================================================================

7.1 SELF-PLAY CONCEPT:
AI h·ªçc b·∫±ng c√°ch ch∆°i v·ªõi ch√≠nh n√≥ h√†ng ng√†n l·∫ßn, gradually improve performance
th√¥ng qua trial v√† error.

7.2 TRAINING PIPELINE:

```csharp
IEnumerator RunSelfPlayTraining()
{
    for (int episode = 1; episode <= maxEpisodes; episode++)
    {
        // Reset game state
        ResetGameForTraining();
        
        while (!IsGameOver())
        {
            // Get current state
            GameState currentState = GetCurrentGameState();
            List<AIMove> possibleMoves = GetPossibleMoves();
            
            // Choose action using Œµ-greedy
            AIMove chosenMove = qLearningAgent.ChooseAction(currentState, possibleMoves);
            
            // Execute move
            ExecuteMove(chosenMove);
            
            // Calculate reward
            float reward = CalculateReward(currentState, chosenMove);
            
            // Update Q-table
            GameState newState = GetCurrentGameState();
            qLearningAgent.UpdateQValue(
                EncodeState(currentState),
                EncodeAction(chosenMove),
                reward,
                EncodeState(newState)
            );
            
            yield return new WaitForEndOfFrame();
        }
        
        // Epsilon decay
        qLearningAgent.DecayEpsilon();
        
        // Save progress periodically
        if (episode % 100 == 0)
        {
            qLearningAgent.SaveQTable();
        }
    }
}
```

7.3 REWARD ENGINEERING:
Critical cho Q-Learning success:

```csharp
float CalculateReward(GameState state, AIMove move)
{
    float reward = 0f;
    
    // Game outcome rewards
    if (IsWin(state)) reward += 1000f;
    if (IsLoss(state)) reward -= 1000f;
    
    // Progress rewards
    if (move.moveType == MoveType.Movement)
    {
        if (IsMoveTowardGoal(move)) reward += 20f;
        else reward -= 5f;
    }
    
    // Strategic rewards
    if (move.moveType == MoveType.WallPlacement)
    {
        if (IsWallBlockingOpponent(move)) reward += 50f;
        if (IsWallStrategic(move)) reward += 30f;
        if (IsWallWasted(move)) reward -= 25f;
    }
    
    // Style-specific rewards
    reward += GetStyleSpecificReward(state, move);
    
    return reward;
}
```

7.4 TRAINING METRICS:
- Episodes: 1000 standard
- Duration: ~15 minutes
- Convergence: Usually around episode 800
- Q-table size: 13,118+ states at completion
- Final epsilon: 0.1 (90% exploitation)

================================================================================
                        8. AI TESTING FRAMEWORK
================================================================================

8.1 TESTING PHILOSOPHY:
Comprehensive testing ensures AI quality v√† reliability across different
game scenarios v√† edge cases.

8.2 TEST CATEGORIES:

A) DECISION TREE TESTS:
```csharp
void TestDecisionTreeStrategies()
{
    // Test Early Game
    var earlyGameFeatures = new DecisionFeatures
    {
        aiDistanceToGoal = 8, humanDistanceToGoal = 8,
        isEarlyGame = true
    };
    var strategy = decisionTree.Predict(earlyGameFeatures);
    Assert.AreEqual(AIStrategy.Balanced, strategy);
    
    // Test AI Leading
    var leadingFeatures = new DecisionFeatures
    {
        distanceAdvantage = 3, // AI leading
        isMidGame = true
    };
    strategy = decisionTree.Predict(leadingFeatures);
    Assert.AreEqual(AIStrategy.Blocking, strategy);
}
```

B) ENTROPY ANALYSIS TESTS:
```csharp
void TestEntropyAnalysis()
{
    // High entropy case (diverse scores)
    var highEntropyScores = new List<float> { 100f, 98f, 97f, 95f, 94f };
    float entropy = CalculateTestEntropy(highEntropyScores);
    Assert.IsTrue(entropy > 0.5f);
    
    // Low entropy case (clear winner)
    var lowEntropyScores = new List<float> { 100f, 50f, 50f, 50f, 50f };
    entropy = CalculateTestEntropy(lowEntropyScores);
    Assert.IsTrue(entropy < 0.5f);
}
```

C) SOFTMAX DISTRIBUTION TESTS:
```csharp
void TestSoftmaxDistribution()
{
    var testScores = new List<float> { 100f, 98f, 95f, 90f };
    float temperature = 1.0f;
    
    var probabilities = CalculateSoftmax(testScores, temperature);
    
    // Verify probabilities sum to 1
    float sum = probabilities.Sum();
    Assert.AreEqual(1.0f, sum, 0.001f);
    
    // Verify highest score gets highest probability
    int maxIndex = Array.IndexOf(probabilities, probabilities.Max());
    Assert.AreEqual(0, maxIndex);
}
```

D) PERFORMANCE BENCHMARKS:
- Move generation time: <100ms
- Q-Learning decision: <50ms
- Minimax decision (depth 4): <200ms
- Memory usage: <256MB

8.3 AUTOMATED TESTING:
Tests run automatically ƒë·ªÉ ensure code quality:
- Unit tests cho individual algorithms
- Integration tests cho AI system interactions
- Performance tests cho response times
- Regression tests ƒë·ªÉ prevent quality degradation

================================================================================
                        9. HYBRID AI ARCHITECTURE
================================================================================

9.1 CONCEPT:
Hybrid system k·∫øt h·ª£p strengths c·ªßa multiple algorithms:
- Q-Learning: Data-driven, learns from experience
- Minimax: Logical, guaranteed optimal given evaluation
- Decision Tree: Fast, context-aware switching
- Entropy: Adaptive, handles uncertainty

9.2 ALGORITHM SELECTION LOGIC:

```csharp
public AIMove GetBestMove()
{
    UpdateGameState();
    List<AIMove> possibleMoves = GeneratePossibleMoves();
    
    // Primary: Use Q-Learning if trained model available
    if (useQLearning && qAgent.IsTrainedModel())
    {
        Debug.Log($"üß† Using {specializedStyle} Q-Learning");
        return qAgent.ChooseAction(currentGameState, possibleMoves);
    }
    
    // Fallback: Use Minimax for guaranteed reasonable play
    else
    {
        Debug.Log($"üîç Using {specializedStyle} Minimax");
        return FindBestMoveWithMinimax();
    }
}
```

9.3 SEAMLESS SWITCHING:
- No interruption in gameplay
- Maintains consistent API
- Preserves game state integrity
- Logs algorithm used for debugging

9.4 BENEFITS:
- Robustness: Fallback ensures AI never fails
- Performance: Q-Learning for trained scenarios, Minimax for edge cases
- Flexibility: Can switch algorithms based on situation
- Development: Easy to test individual components

================================================================================
                        10. PERFORMANCE OPTIMIZATION
================================================================================

10.1 Q-TABLE OPTIMIZATION:
- Sparse representation: Only store visited states
- Compression: String interning for state keys
- Lazy loading: Load Q-table on demand
- Periodic saving: Every 100 training episodes

10.2 MINIMAX OPTIMIZATION:
- Alpha-Beta pruning: 50% search reduction
- Move ordering: Promising moves first
- Iterative deepening: Gradual depth increase
- Transposition tables: Cache evaluated positions

10.3 MEMORY MANAGEMENT:
- Object pooling cho GameState objects
- String caching cho repeated state/action encodings
- Garbage collection optimization
- Memory profiling v√† monitoring

10.4 MULTITHREADING CONSIDERATIONS:
- Q-Learning training: Background thread
- Move calculation: Main thread (Unity requirement)
- File I/O: Async operations where possible
- Thread-safe collections cho shared data

10.5 CACHING STRATEGIES:
- Path finding results
- Move generation results
- Evaluation function results
- Feature extraction results

================================================================================
                            K·∫æT LU·∫¨N
================================================================================

D·ª± √°n AI Quoridor Project th·ªÉ hi·ªán m·ªôt implementation comprehensive c·ªßa modern
game AI techniques:

TECHNICAL INNOVATIONS:
‚úÖ Hybrid Q-Learning + Minimax architecture
‚úÖ 6 specialized AI personalities v·ªõi separate training
‚úÖ Advanced decision making v·ªõi entropy analysis
‚úÖ Comprehensive testing v√† validation framework
‚úÖ Professional development tools v√† debugging

EDUCATIONAL VALUE:
‚úÖ Practical machine learning implementation
‚úÖ Traditional game AI algorithms
‚úÖ Modern software engineering practices
‚úÖ Performance optimization techniques
‚úÖ Automated testing methodologies

FUTURE ENHANCEMENTS:
üöÄ Deep Q-Learning (DQN) integration
üöÄ Multi-agent reinforcement learning
üöÄ Neural network-based evaluation functions
üöÄ Online learning v√† adaptation
üöÄ Transfer learning between game variants

D·ª± √°n n√†y demonstrating successful integration c·ªßa classical AI methods v·ªõi
modern machine learning, t·∫°o ra m·ªôt intelligent, adaptive, v√† highly
interactive gaming experience.

================================================================================
                            APPENDIX
================================================================================

FILES LI√äN QUAN:
- Assets/scripts/QLearningAgent.cs - Q-Learning implementation
- Assets/scripts/QuoridorAI.cs - Main AI controller
- Assets/scripts/DecisionTreeClassifier.cs - Decision tree logic
- Assets/scripts/QuoridorSelfPlayTrainer.cs - Training system
- Assets/scripts/AITestScript.cs - Testing framework
- Assets/scripts/Editor/ForceRetrainAI.cs - Development tools

Q-TABLE FILES:
- StreamingAssets/qtable.json - Balanced style
- StreamingAssets/qtable_aggressive.json - Aggressive style
- StreamingAssets/qtable_defensive.json - Defensive style
- StreamingAssets/qtable_wallmaster.json - Wall Master style
- StreamingAssets/qtable_speedrunner.json - Speed Runner style
- StreamingAssets/qtable_tactical.json - Tactical style

DOCUMENTATION:
- AI_ALGORITHMS_DETAILED_DESCRIPTION.txt - This file
- README.md - Project overview
- API_INTEGRATION_GUIDE.md - Voice/Chat integration
- TRAINING_GUIDE.md - AI training instructions

================================================================================
================================================================================
                    AI QUORIDOR GAME PROJECT - THU·∫¨T TO√ÅN AI CHI TI·∫æT
================================================================================

D·ª± √°n: AI Quoridor Game v·ªõi Machine Learning v√† Multi-Platform Integration
Ng√†y t·∫°o: 25/07/2025
T√°c gi·∫£: [Student Name]
Platform: Unity 3D
Language: C#

================================================================================
                            M·ª§C L·ª§C
================================================================================

1. T·ªîNG QUAN KI·∫æN TR√öC AI
2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
4. SPECIALIZED AI STYLES SYSTEM
5. DECISION TREE CLASSIFIER
6. ENTROPY ANALYSIS & SOFTMAX
7. TRAINING SYSTEM & SELF-PLAY
8. AI TESTING FRAMEWORK
9. HYBRID AI ARCHITECTURE
10. PERFORMANCE OPTIMIZATION

================================================================================
                        1. T·ªîNG QUAN KI·∫æN TR√öC AI
================================================================================

D·ª± √°n AI Quoridor s·ª≠ d·ª•ng m·ªôt ki·∫øn tr√∫c AI hybrid ti√™n ti·∫øn, k·∫øt h·ª£p:

1.1 CORE ALGORITHMS:
- Q-Learning (Reinforcement Learning) - Thu·∫≠t to√°n ch√≠nh
- Minimax + Alpha-Beta Pruning - Thu·∫≠t to√°n fallback
- Decision Tree Classifier - Ph√¢n t√≠ch ng·ªØ c·∫£nh game
- Entropy Analysis - ƒê√°nh gi√° ƒëa d·∫°ng move

1.2 SPECIALIZED SYSTEMS:
- 6 AI Personality Styles v·ªõi training ri√™ng bi·ªát
- Self-Play Training Pipeline t·ª± ƒë·ªông
- Real-time Performance Testing Framework
- Dynamic Algorithm Switching

1.3 TECHNICAL FOUNDATION:
- State Space: 13,118+ unique game states
- Action Space: Movement + Wall placement combinations
- Training Method: Self-play v·ªõi 1000 episodes
- Learning Rate: Œ± = 0.1, Discount Factor: Œ≥ = 0.99

================================================================================
                    2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
================================================================================

2.1 GI·ªöI THI·ªÜU Q-LEARNING:
Q-Learning l√† thu·∫≠t to√°n Reinforcement Learning kh√¥ng c·∫ßn model, h·ªçc th√¥ng qua
t∆∞∆°ng t√°c v·ªõi m√¥i tr∆∞·ªùng ƒë·ªÉ t·ªëi ∆∞u h√≥a policy. Trong Quoridor, AI h·ªçc c√°ch
ch·ªçn moves t·ªëi ∆∞u d·ª±a tr√™n kinh nghi·ªám t·ª´ h√†ng ng√†n game.

2.2 C√îNG TH·ª®C TO√ÅN H·ªåC:
Q(s,a) = Q(s,a) + Œ±[r + Œ≥*max(Q(s',a')) - Q(s,a)]

Trong ƒë√≥:
- Q(s,a): Q-value c·ªßa state s v·ªõi action a
- Œ±: Learning rate (0.1) - t·ªëc ƒë·ªô h·ªçc
- r: Immediate reward t·ª´ action
- Œ≥: Discount factor (0.99) - t·∫ßm quan tr·ªçng future rewards
- s': Next state sau khi th·ª±c hi·ªán action
- max(Q(s',a')): Maximum Q-value c·ªßa next state

2.3 IMPLEMENTATION CHI TI·∫æT:

```csharp
public class QLearningAgent
{
    // Core Q-Table: Dictionary l∆∞u tr·ªØ Q-values
    private Dictionary<string, Dictionary<string, float>> qTable;
    
    // Hyperparameters
    private float learningRate = 0.1f;      // T·ªëc ƒë·ªô h·ªçc
    private float discountFactor = 0.99f;   // Discount future rewards
    private float epsilon = 0.995f;         // Exploration rate
    
    // Q-Learning Update Implementation
    public void UpdateQValue(string state, string action, float reward, string nextState)
    {
        // L·∫•y Q-value hi·ªán t·∫°i
        float currentQ = GetQValue(state, action);
        
        // T√¨m max Q-value c·ªßa next state
        float maxNextQ = GetMaxQValue(nextState);
        
        // √Åp d·ª•ng c√¥ng th·ª©c Q-Learning
        float newQ = currentQ + learningRate * (reward + discountFactor * maxNextQ - currentQ);
        
        // C·∫≠p nh·∫≠t Q-table
        SetQValue(state, action, newQ);
    }
}
```

2.4 STATE ENCODING:
Game state ƒë∆∞·ª£c encode th√†nh string duy nh·∫•t:
"aiPos_x,aiPos_y-humanPos_x,humanPos_y-aiWalls-humanWalls-wallsList"

V√≠ d·ª•: "4,8-4,0-9-10-7,4,False|5,6,True"
- AI ·ªü (4,8), Human ·ªü (4,0)
- AI c√≥ 9 walls, Human c√≥ 10 walls
- Walls ƒë√£ ƒë·∫∑t: (7,4,False), (5,6,True)

2.5 ACTION ENCODING:
Actions ƒë∆∞·ª£c encode th√†nh string:
- Movement: "M:x,y" (v√≠ d·ª•: "M:4,7")
- Wall: "W:x,y,horizontal" (v√≠ d·ª•: "W:3,5,True")

2.6 EPSILON-GREEDY STRATEGY:
```csharp
public AIMove ChooseAction(GameState state, List<AIMove> possibleMoves)
{
    // Exploration vs Exploitation trade-off
    if (Random.Range(0f, 1f) < epsilon)
    {
        // EXPLORATION: Ch·ªçn random ƒë·ªÉ kh√°m ph√°
        return possibleMoves[Random.Range(0, possibleMoves.Count)];
    }
    else
    {
        // EXPLOITATION: Ch·ªçn action c√≥ Q-value cao nh·∫•t
        return GetBestAction(state, possibleMoves);
    }
}
```

2.7 EPSILON DECAY:
Epsilon gi·∫£m d·∫ßn t·ª´ 0.995 ‚Üí 0.1 qua training:
- Ban ƒë·∫ßu: High exploration (99.5%)
- Cu·ªëi training: High exploitation (90%)

2.8 REWARD SYSTEM:
- Win game: +1000
- Lose game: -1000
- Move toward goal: +10
- Block opponent: +50
- Waste move: -5
- Strategic position: +15

================================================================================
                3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
================================================================================

3.1 GI·ªöI THI·ªÜU MINIMAX:
Minimax l√† thu·∫≠t to√°n game theory c·ªï ƒëi·ªÉn, t√¨m ki·∫øm move t·ªëi ∆∞u b·∫±ng c√°ch
d·ª± ƒëo√°n tr∆∞·ªõc nhi·ªÅu moves. Alpha-Beta pruning t·ªëi ∆∞u hi·ªáu su·∫•t b·∫±ng c√°ch
lo·∫°i b·ªè c√°c nh√°nh kh√¥ng c·∫ßn thi·∫øt.

3.2 NGUY√äN L√ù HO·∫†T ƒê·ªòNG:
- Maximizing Player (AI): T·ªëi ƒëa h√≥a score
- Minimizing Player (Human): T·ªëi thi·ªÉu h√≥a score AI
- Search depth: 4 levels (configurable)
- Pruning: Lo·∫°i b·ªè nh√°nh kh√¥ng promising

3.3 IMPLEMENTATION:

```csharp
float Minimax(GameState state, int depth, bool isMaximizing, float alpha, float beta)
{
    // Base case: Depth = 0 ho·∫∑c game over
    if (depth == 0 || IsGameOver(state))
        return EvaluatePosition(state);
    
    if (isMaximizing) // AI turn
    {
        float maxEval = float.NegativeInfinity;
        foreach (var move in GeneratePossibleMoves(state, true))
        {
            GameState newState = ApplyMove(state, move, true);
            float eval = Minimax(newState, depth - 1, false, alpha, beta);
            maxEval = Mathf.Max(maxEval, eval);
            alpha = Mathf.Max(alpha, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return maxEval;
    }
    else // Human turn
    {
        float minEval = float.PositiveInfinity;
        foreach (var move in GeneratePossibleMoves(state, false))
        {
            GameState newState = ApplyMove(state, move, false);
            float eval = Minimax(newState, depth - 1, true, alpha, beta);
            minEval = Mathf.Min(minEval, eval);
            beta = Mathf.Min(beta, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return minEval;
    }
}
```

3.4 EVALUATION FUNCTION:
Position evaluation d·ª±a tr√™n multiple factors:

```csharp
float EvaluatePosition(GameState state)
{
    float score = 0f;
    
    // Distance to goal (c√†ng g·∫ßn c√†ng t·ªët)
    int aiDistance = GetShortestPath(state.aiPosition, goalRow);
    int humanDistance = GetShortestPath(state.humanPosition, goalRow);
    score += (humanDistance - aiDistance) * 100;
    
    // Wall advantage
    score += (state.aiWallsLeft - state.humanWallsLeft) * 50;
    
    // Mobility (s·ªë moves c√≥ th·ªÉ)
    int aiMobility = GetValidMoves(state.aiPosition).Count;
    int humanMobility = GetValidMoves(state.humanPosition).Count;
    score += (aiMobility - humanMobility) * 20;
    
    // Center control
    score += GetCenterControlScore(state) * 30;
    
    return score;
}
```

3.5 ALPHA-BETA PRUNING HI·ªÜU QU·∫¢:
- Reduces search space t·ª´ O(b^d) xu·ªëng O(b^(d/2))
- Best case: 50% reduction in nodes evaluated
- Ordering moves by promising gi√∫p pruning hi·ªáu qu·∫£ h∆°n

================================================================================
                        4. SPECIALIZED AI STYLES SYSTEM
================================================================================

4.1 CONCEPT:
6 AI personalities v·ªõi training v√† behavior ri√™ng bi·ªát, m·ªói style c√≥:
- Ri√™ng Q-table file
- Custom reward structure
- Specific move prioritization
- Unique decision making logic

4.2 AI STYLES CHI TI·∫æT:

üî• AGGRESSIVE STYLE:
- M·ª•c ti√™u: T·∫•n c√¥ng v√† ch·∫∑n ƒë·ªëi th·ªß
- Wall Usage: 70% chance
- Reward Structure:
  * blockingOpponentReward = 50 * intensity
  * wallPlacementReward = 30 * intensity
  * defensiveWallPenalty = -20 * intensity
- Q-table: qtable_aggressive.json

üõ°Ô∏è DEFENSIVE STYLE:
- M·ª•c ti√™u: Ph√≤ng th·ªß v√† di chuy·ªÉn an to√†n
- Wall Usage: 50% chance
- Reward Structure:
  * defensiveWallReward = 35 * intensity
  * movementReward = 20 * intensity
  * aggressiveMovePenalty = -15 * intensity
- Q-table: qtable_defensive.json

üß± WALL MASTER STYLE:
- M·ª•c ti√™u: Chuy√™n gia ƒë·∫∑t t∆∞·ªùng v√† t·∫°o m√™ cung
- Wall Usage: 90% chance
- Reward Structure:
  * wallPlacementReward = 60 * intensity
  * strategicWallReward = 40 * intensity
  * wallComboReward = 50 * intensity
  * movementWithoutWallPenalty = -25 * intensity
- Q-table: qtable_wallmaster.json

‚ö° SPEED RUNNER STYLE:
- M·ª•c ti√™u: ƒê·∫°t ƒë√≠ch nhanh nh·∫•t
- Wall Usage: 10% chance
- Reward Structure:
  * movementReward = 40 * intensity
  * progressToGoalReward = 50 * intensity
  * quickWinBonus = 100 * intensity
  * wallPlacementPenalty = -20 * intensity
- Q-table: qtable_speedrunner.json

üß† TACTICAL STYLE:
- M·ª•c ti√™u: L·ª£i th·∫ø v·ªã tr√≠ v√† k·∫ø ho·∫°ch d√†i h·∫°n
- Wall Usage: Variable (strategic)
- Reward Structure:
  * tacticalMoveReward = 45 * intensity
  * positionAdvantageReward = 35 * intensity
  * mobilityReward = 25 * intensity
  * patternRecognitionBonus = 40 * intensity
- Q-table: qtable_tactical.json

‚öñÔ∏è BALANCED STYLE:
- M·ª•c ti√™u: C√¢n b·∫±ng t·∫•t c·∫£ aspects
- Wall Usage: 30% chance
- Reward Structure: Default balanced values
- Q-table: qtable.json

4.3 STYLE SWITCHING MECHANISM:

```csharp
public void SwitchToStyle(PlayStyle newStyle)
{
    // Save current Q-table
    if (allowQTableSaving) qAgent.SaveQTable(GetStyleSpecificQTablePath());
    
    // Load new style Q-table
    specializedStyle = newStyle;
    qAgent.LoadQTable(GetStyleSpecificQTablePath());
    
    // Reconfigure parameters
    ConfigureStyleSpecificTraining();
}
```

4.4 STYLE INTENSITY:
Parameter t·ª´ 0.0 ‚Üí 1.0 control m·ª©c ƒë·ªô chuy√™n h√≥a:
- 0.0: Balanced behavior
- 0.5: Moderate specialization
- 1.0: Extreme specialization

================================================================================
                        5. DECISION TREE CLASSIFIER
================================================================================

5.1 PURPOSE:
Decision Tree ph√¢n t√≠ch game context ƒë·ªÉ ch·ªçn strategy ph√π h·ª£p, d·ª±a tr√™n:
- Game phase (Early/Mid/End)
- Position advantage
- Resource advantage
- Mobility situation

5.2 FEATURE EXTRACTION:

```csharp
public struct DecisionFeatures
{
    public int aiDistanceToGoal;      // AI distance to goal line
    public int humanDistanceToGoal;   // Human distance to goal line
    public int aiWallsLeft;           // AI walls remaining
    public int humanWallsLeft;        // Human walls remaining
    public int aiMobility;            // AI possible moves
    public int humanMobility;         // Human possible moves
    public float distanceAdvantage;   // Human - AI distance
    public float wallAdvantage;       // AI - Human walls
    public bool isEarlyGame;          // > 15 moves estimated remaining
    public bool isMidGame;            // 5-15 moves remaining
    public bool isEndGame;            // < 5 moves remaining
}
```

5.3 DECISION LOGIC:

```csharp
public AIStrategy Predict(DecisionFeatures features)
{
    // Early Game: Play conservatively
    if (features.isEarlyGame)
        return AIStrategy.Balanced;
    
    // AI has significant lead: Focus on blocking
    if (features.distanceAdvantage > 2)
        return AIStrategy.Blocking;
    
    // AI is behind: Be aggressive
    if (features.distanceAdvantage < -1)
        return AIStrategy.Aggressive;
    
    // End game with wall advantage: Play defensively
    if (features.isEndGame && features.wallAdvantage > 2)
        return AIStrategy.Defensive;
    
    // Close game: Balanced approach
    return AIStrategy.Balanced;
}
```

5.4 STRATEGY OUTCOMES:
- Balanced: Equal weight to movement v√† walls
- Blocking: Prioritize moves that increase opponent path
- Aggressive: Focus on attacking positions
- Defensive: Protect own position and path

================================================================================
                        6. ENTROPY ANALYSIS & SOFTMAX
================================================================================

6.1 ENTROPY CONCEPT:
Entropy ƒëo ƒë·ªô ƒëa d·∫°ng/uncertainty c·ªßa move scores. High entropy = nhi·ªÅu moves
c√≥ scores t∆∞∆°ng ƒë∆∞∆°ng, low entropy = m·ªôt move r√µ r√†ng t·ªët nh·∫•t.

6.2 ENTROPY CALCULATION:

```csharp
float CalculateEntropy(List<float> moveScores)
{
    // Normalize th√†nh probabilities
    float total = moveScores.Sum();
    var probabilities = moveScores.Select(s => s / total);
    
    // Calculate Shannon entropy
    float entropy = 0f;
    foreach (float p in probabilities)
    {
        if (p > 0f)
            entropy -= p * Mathf.Log(p, 2f);
    }
    
    return entropy; // Range: 0 (deterministic) to log2(n) (uniform)
}
```

6.3 SOFTMAX DISTRIBUTION:

```csharp
float[] CalculateSoftmax(List<float> scores, float temperature)
{
    float[] probabilities = new float[scores.Count];
    float sum = scores.Sum(s => Mathf.Exp(s / temperature));
    
    for (int i = 0; i < scores.Count; i++)
    {
        probabilities[i] = Mathf.Exp(scores[i] / temperature) / sum;
    }
    
    return probabilities;
}
```

6.4 TEMPERATURE PARAMETER:
- Low temperature (0.1): Sharp distribution, focus on best moves
- High temperature (2.0): Flat distribution, more exploration
- Default temperature (1.0): Balanced exploitation/exploration

6.5 ADAPTIVE MOVE SELECTION:

```csharp
public AIMove ChooseActionWithEntropy(GameState state, List<AIMove> possibleMoves)
{
    var moveScores = EvaluateAllMoves(state, possibleMoves);
    float entropy = CalculateEntropy(moveScores);
    
    if (entropy > entropyThreshold) // High diversity
    {
        // Use softmax for probabilistic selection
        var probabilities = CalculateSoftmax(moveScores, softmaxTemperature);
        return SelectMoveByProbability(possibleMoves, probabilities);
    }
    else // Clear best move
    {
        // Use deterministic best move
        return GetBestScoredMove(possibleMoves, moveScores);
    }
}
```

================================================================================
                        7. TRAINING SYSTEM & SELF-PLAY
================================================================================

7.1 SELF-PLAY CONCEPT:
AI h·ªçc b·∫±ng c√°ch ch∆°i v·ªõi ch√≠nh n√≥ h√†ng ng√†n l·∫ßn, gradually improve performance
th√¥ng qua trial v√† error.

7.2 TRAINING PIPELINE:

```csharp
IEnumerator RunSelfPlayTraining()
{
    for (int episode = 1; episode <= maxEpisodes; episode++)
    {
        // Reset game state
        ResetGameForTraining();
        
        while (!IsGameOver())
        {
            // Get current state
            GameState currentState = GetCurrentGameState();
            List<AIMove> possibleMoves = GetPossibleMoves();
            
            // Choose action using Œµ-greedy
            AIMove chosenMove = qLearningAgent.ChooseAction(currentState, possibleMoves);
            
            // Execute move
            ExecuteMove(chosenMove);
            
            // Calculate reward
            float reward = CalculateReward(currentState, chosenMove);
            
            // Update Q-table
            GameState newState = GetCurrentGameState();
            qLearningAgent.UpdateQValue(
                EncodeState(currentState),
                EncodeAction(chosenMove),
                reward,
                EncodeState(newState)
            );
            
            yield return new WaitForEndOfFrame();
        }
        
        // Epsilon decay
        qLearningAgent.DecayEpsilon();
        
        // Save progress periodically
        if (episode % 100 == 0)
        {
            qLearningAgent.SaveQTable();
        }
    }
}
```

7.3 REWARD ENGINEERING:
Critical cho Q-Learning success:

```csharp
float CalculateReward(GameState state, AIMove move)
{
    float reward = 0f;
    
    // Game outcome rewards
    if (IsWin(state)) reward += 1000f;
    if (IsLoss(state)) reward -= 1000f;
    
    // Progress rewards
    if (move.moveType == MoveType.Movement)
    {
        if (IsMoveTowardGoal(move)) reward += 20f;
        else reward -= 5f;
    }
    
    // Strategic rewards
    if (move.moveType == MoveType.WallPlacement)
    {
        if (IsWallBlockingOpponent(move)) reward += 50f;
        if (IsWallStrategic(move)) reward += 30f;
        if (IsWallWasted(move)) reward -= 25f;
    }
    
    // Style-specific rewards
    reward += GetStyleSpecificReward(state, move);
    
    return reward;
}
```

7.4 TRAINING METRICS:
- Episodes: 1000 standard
- Duration: ~15 minutes
- Convergence: Usually around episode 800
- Q-table size: 13,118+ states at completion
- Final epsilon: 0.1 (90% exploitation)

================================================================================
                        8. AI TESTING FRAMEWORK
================================================================================

8.1 TESTING PHILOSOPHY:
Comprehensive testing ensures AI quality v√† reliability across different
game scenarios v√† edge cases.

8.2 TEST CATEGORIES:

A) DECISION TREE TESTS:
```csharp
void TestDecisionTreeStrategies()
{
    // Test Early Game
    var earlyGameFeatures = new DecisionFeatures
    {
        aiDistanceToGoal = 8, humanDistanceToGoal = 8,
        isEarlyGame = true
    };
    var strategy = decisionTree.Predict(earlyGameFeatures);
    Assert.AreEqual(AIStrategy.Balanced, strategy);
    
    // Test AI Leading
    var leadingFeatures = new DecisionFeatures
    {
        distanceAdvantage = 3, // AI leading
        isMidGame = true
    };
    strategy = decisionTree.Predict(leadingFeatures);
    Assert.AreEqual(AIStrategy.Blocking, strategy);
}
```

B) ENTROPY ANALYSIS TESTS:
```csharp
void TestEntropyAnalysis()
{
    // High entropy case (diverse scores)
    var highEntropyScores = new List<float> { 100f, 98f, 97f, 95f, 94f };
    float entropy = CalculateTestEntropy(highEntropyScores);
    Assert.IsTrue(entropy > 0.5f);
    
    // Low entropy case (clear winner)
    var lowEntropyScores = new List<float> { 100f, 50f, 50f, 50f, 50f };
    entropy = CalculateTestEntropy(lowEntropyScores);
    Assert.IsTrue(entropy < 0.5f);
}
```

C) SOFTMAX DISTRIBUTION TESTS:
```csharp
void TestSoftmaxDistribution()
{
    var testScores = new List<float> { 100f, 98f, 95f, 90f };
    float temperature = 1.0f;
    
    var probabilities = CalculateSoftmax(testScores, temperature);
    
    // Verify probabilities sum to 1
    float sum = probabilities.Sum();
    Assert.AreEqual(1.0f, sum, 0.001f);
    
    // Verify highest score gets highest probability
    int maxIndex = Array.IndexOf(probabilities, probabilities.Max());
    Assert.AreEqual(0, maxIndex);
}
```

D) PERFORMANCE BENCHMARKS:
- Move generation time: <100ms
- Q-Learning decision: <50ms
- Minimax decision (depth 4): <200ms
- Memory usage: <256MB

8.3 AUTOMATED TESTING:
Tests run automatically ƒë·ªÉ ensure code quality:
- Unit tests cho individual algorithms
- Integration tests cho AI system interactions
- Performance tests cho response times
- Regression tests ƒë·ªÉ prevent quality degradation

================================================================================
                        9. HYBRID AI ARCHITECTURE
================================================================================

9.1 CONCEPT:
Hybrid system k·∫øt h·ª£p strengths c·ªßa multiple algorithms:
- Q-Learning: Data-driven, learns from experience
- Minimax: Logical, guaranteed optimal given evaluation
- Decision Tree: Fast, context-aware switching
- Entropy: Adaptive, handles uncertainty

9.2 ALGORITHM SELECTION LOGIC:

```csharp
public AIMove GetBestMove()
{
    UpdateGameState();
    List<AIMove> possibleMoves = GeneratePossibleMoves();
    
    // Primary: Use Q-Learning if trained model available
    if (useQLearning && qAgent.IsTrainedModel())
    {
        Debug.Log($"üß† Using {specializedStyle} Q-Learning");
        return qAgent.ChooseAction(currentGameState, possibleMoves);
    }
    
    // Fallback: Use Minimax for guaranteed reasonable play
    else
    {
        Debug.Log($"üîç Using {specializedStyle} Minimax");
        return FindBestMoveWithMinimax();
    }
}
```

9.3 SEAMLESS SWITCHING:
- No interruption in gameplay
- Maintains consistent API
- Preserves game state integrity
- Logs algorithm used for debugging

9.4 BENEFITS:
- Robustness: Fallback ensures AI never fails
- Performance: Q-Learning for trained scenarios, Minimax for edge cases
- Flexibility: Can switch algorithms based on situation
- Development: Easy to test individual components

================================================================================
                        10. PERFORMANCE OPTIMIZATION
================================================================================

10.1 Q-TABLE OPTIMIZATION:
- Sparse representation: Only store visited states
- Compression: String interning for state keys
- Lazy loading: Load Q-table on demand
- Periodic saving: Every 100 training episodes

10.2 MINIMAX OPTIMIZATION:
- Alpha-Beta pruning: 50% search reduction
- Move ordering: Promising moves first
- Iterative deepening: Gradual depth increase
- Transposition tables: Cache evaluated positions

10.3 MEMORY MANAGEMENT:
- Object pooling cho GameState objects
- String caching cho repeated state/action encodings
- Garbage collection optimization
- Memory profiling v√† monitoring

10.4 MULTITHREADING CONSIDERATIONS:
- Q-Learning training: Background thread
- Move calculation: Main thread (Unity requirement)
- File I/O: Async operations where possible
- Thread-safe collections cho shared data

10.5 CACHING STRATEGIES:
- Path finding results
- Move generation results
- Evaluation function results
- Feature extraction results

================================================================================
                            K·∫æT LU·∫¨N
================================================================================

D·ª± √°n AI Quoridor Project th·ªÉ hi·ªán m·ªôt implementation comprehensive c·ªßa modern
game AI techniques:

TECHNICAL INNOVATIONS:
‚úÖ Hybrid Q-Learning + Minimax architecture
‚úÖ 6 specialized AI personalities v·ªõi separate training
‚úÖ Advanced decision making v·ªõi entropy analysis
‚úÖ Comprehensive testing v√† validation framework
‚úÖ Professional development tools v√† debugging

EDUCATIONAL VALUE:
‚úÖ Practical machine learning implementation
‚úÖ Traditional game AI algorithms
‚úÖ Modern software engineering practices
‚úÖ Performance optimization techniques
‚úÖ Automated testing methodologies

FUTURE ENHANCEMENTS:
üöÄ Deep Q-Learning (DQN) integration
üöÄ Multi-agent reinforcement learning
üöÄ Neural network-based evaluation functions
üöÄ Online learning v√† adaptation
üöÄ Transfer learning between game variants

D·ª± √°n n√†y demonstrating successful integration c·ªßa classical AI methods v·ªõi
modern machine learning, t·∫°o ra m·ªôt intelligent, adaptive, v√† highly
interactive gaming experience.

================================================================================
                            APPENDIX
================================================================================

FILES LI√äN QUAN:
- Assets/scripts/QLearningAgent.cs - Q-Learning implementation
- Assets/scripts/QuoridorAI.cs - Main AI controller
- Assets/scripts/DecisionTreeClassifier.cs - Decision tree logic
- Assets/scripts/QuoridorSelfPlayTrainer.cs - Training system
- Assets/scripts/AITestScript.cs - Testing framework
- Assets/scripts/Editor/ForceRetrainAI.cs - Development tools

Q-TABLE FILES:
- StreamingAssets/qtable.json - Balanced style
- StreamingAssets/qtable_aggressive.json - Aggressive style
- StreamingAssets/qtable_defensive.json - Defensive style
- StreamingAssets/qtable_wallmaster.json - Wall Master style
- StreamingAssets/qtable_speedrunner.json - Speed Runner style
- StreamingAssets/qtable_tactical.json - Tactical style

DOCUMENTATION:
- AI_ALGORITHMS_DETAILED_DESCRIPTION.txt - This file
- README.md - Project overview
- API_INTEGRATION_GUIDE.md - Voice/Chat integration
- TRAINING_GUIDE.md - AI training instructions

================================================================================
================================================================================
                    AI QUORIDOR GAME PROJECT - THU·∫¨T TO√ÅN AI CHI TI·∫æT
================================================================================

D·ª± √°n: AI Quoridor Game v·ªõi Machine Learning v√† Multi-Platform Integration
Ng√†y t·∫°o: 25/07/2025
T√°c gi·∫£: [Student Name]
Platform: Unity 3D
Language: C#

================================================================================
                            M·ª§C L·ª§C
================================================================================

1. T·ªîNG QUAN KI·∫æN TR√öC AI
2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
4. SPECIALIZED AI STYLES SYSTEM
5. DECISION TREE CLASSIFIER
6. ENTROPY ANALYSIS & SOFTMAX
7. TRAINING SYSTEM & SELF-PLAY
8. AI TESTING FRAMEWORK
9. HYBRID AI ARCHITECTURE
10. PERFORMANCE OPTIMIZATION

================================================================================
                        1. T·ªîNG QUAN KI·∫æN TR√öC AI
================================================================================

D·ª± √°n AI Quoridor s·ª≠ d·ª•ng m·ªôt ki·∫øn tr√∫c AI hybrid ti√™n ti·∫øn, k·∫øt h·ª£p:

1.1 CORE ALGORITHMS:
- Q-Learning (Reinforcement Learning) - Thu·∫≠t to√°n ch√≠nh
- Minimax + Alpha-Beta Pruning - Thu·∫≠t to√°n fallback
- Decision Tree Classifier - Ph√¢n t√≠ch ng·ªØ c·∫£nh game
- Entropy Analysis - ƒê√°nh gi√° ƒëa d·∫°ng move

1.2 SPECIALIZED SYSTEMS:
- 6 AI Personality Styles v·ªõi training ri√™ng bi·ªát
- Self-Play Training Pipeline t·ª± ƒë·ªông
- Real-time Performance Testing Framework
- Dynamic Algorithm Switching

1.3 TECHNICAL FOUNDATION:
- State Space: 13,118+ unique game states
- Action Space: Movement + Wall placement combinations
- Training Method: Self-play v·ªõi 1000 episodes
- Learning Rate: Œ± = 0.1, Discount Factor: Œ≥ = 0.99

================================================================================
                    2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
================================================================================

2.1 GI·ªöI THI·ªÜU Q-LEARNING:
Q-Learning l√† thu·∫≠t to√°n Reinforcement Learning kh√¥ng c·∫ßn model, h·ªçc th√¥ng qua
t∆∞∆°ng t√°c v·ªõi m√¥i tr∆∞·ªùng ƒë·ªÉ t·ªëi ∆∞u h√≥a policy. Trong Quoridor, AI h·ªçc c√°ch
ch·ªçn moves t·ªëi ∆∞u d·ª±a tr√™n kinh nghi·ªám t·ª´ h√†ng ng√†n game.

2.2 C√îNG TH·ª®C TO√ÅN H·ªåC:
Q(s,a) = Q(s,a) + Œ±[r + Œ≥*max(Q(s',a')) - Q(s,a)]

Trong ƒë√≥:
- Q(s,a): Q-value c·ªßa state s v·ªõi action a
- Œ±: Learning rate (0.1) - t·ªëc ƒë·ªô h·ªçc
- r: Immediate reward t·ª´ action
- Œ≥: Discount factor (0.99) - t·∫ßm quan tr·ªçng future rewards
- s': Next state sau khi th·ª±c hi·ªán action
- max(Q(s',a')): Maximum Q-value c·ªßa next state

2.3 IMPLEMENTATION CHI TI·∫æT:

```csharp
public class QLearningAgent
{
    // Core Q-Table: Dictionary l∆∞u tr·ªØ Q-values
    private Dictionary<string, Dictionary<string, float>> qTable;
    
    // Hyperparameters
    private float learningRate = 0.1f;      // T·ªëc ƒë·ªô h·ªçc
    private float discountFactor = 0.99f;   // Discount future rewards
    private float epsilon = 0.995f;         // Exploration rate
    
    // Q-Learning Update Implementation
    public void UpdateQValue(string state, string action, float reward, string nextState)
    {
        // L·∫•y Q-value hi·ªán t·∫°i
        float currentQ = GetQValue(state, action);
        
        // T√¨m max Q-value c·ªßa next state
        float maxNextQ = GetMaxQValue(nextState);
        
        // √Åp d·ª•ng c√¥ng th·ª©c Q-Learning
        float newQ = currentQ + learningRate * (reward + discountFactor * maxNextQ - currentQ);
        
        // C·∫≠p nh·∫≠t Q-table
        SetQValue(state, action, newQ);
    }
}
```

2.4 STATE ENCODING:
Game state ƒë∆∞·ª£c encode th√†nh string duy nh·∫•t:
"aiPos_x,aiPos_y-humanPos_x,humanPos_y-aiWalls-humanWalls-wallsList"

V√≠ d·ª•: "4,8-4,0-9-10-7,4,False|5,6,True"
- AI ·ªü (4,8), Human ·ªü (4,0)
- AI c√≥ 9 walls, Human c√≥ 10 walls
- Walls ƒë√£ ƒë·∫∑t: (7,4,False), (5,6,True)

2.5 ACTION ENCODING:
Actions ƒë∆∞·ª£c encode th√†nh string:
- Movement: "M:x,y" (v√≠ d·ª•: "M:4,7")
- Wall: "W:x,y,horizontal" (v√≠ d·ª•: "W:3,5,True")

2.6 EPSILON-GREEDY STRATEGY:
```csharp
public AIMove ChooseAction(GameState state, List<AIMove> possibleMoves)
{
    // Exploration vs Exploitation trade-off
    if (Random.Range(0f, 1f) < epsilon)
    {
        // EXPLORATION: Ch·ªçn random ƒë·ªÉ kh√°m ph√°
        return possibleMoves[Random.Range(0, possibleMoves.Count)];
    }
    else
    {
        // EXPLOITATION: Ch·ªçn action c√≥ Q-value cao nh·∫•t
        return GetBestAction(state, possibleMoves);
    }
}
```

2.7 EPSILON DECAY:
Epsilon gi·∫£m d·∫ßn t·ª´ 0.995 ‚Üí 0.1 qua training:
- Ban ƒë·∫ßu: High exploration (99.5%)
- Cu·ªëi training: High exploitation (90%)

2.8 REWARD SYSTEM:
- Win game: +1000
- Lose game: -1000
- Move toward goal: +10
- Block opponent: +50
- Waste move: -5
- Strategic position: +15

================================================================================
                3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
================================================================================

3.1 GI·ªöI THI·ªÜU MINIMAX:
Minimax l√† thu·∫≠t to√°n game theory c·ªï ƒëi·ªÉn, t√¨m ki·∫øm move t·ªëi ∆∞u b·∫±ng c√°ch
d·ª± ƒëo√°n tr∆∞·ªõc nhi·ªÅu moves. Alpha-Beta pruning t·ªëi ∆∞u hi·ªáu su·∫•t b·∫±ng c√°ch
lo·∫°i b·ªè c√°c nh√°nh kh√¥ng c·∫ßn thi·∫øt.

3.2 NGUY√äN L√ù HO·∫†T ƒê·ªòNG:
- Maximizing Player (AI): T·ªëi ƒëa h√≥a score
- Minimizing Player (Human): T·ªëi thi·ªÉu h√≥a score AI
- Search depth: 4 levels (configurable)
- Pruning: Lo·∫°i b·ªè nh√°nh kh√¥ng promising

3.3 IMPLEMENTATION:

```csharp
float Minimax(GameState state, int depth, bool isMaximizing, float alpha, float beta)
{
    // Base case: Depth = 0 ho·∫∑c game over
    if (depth == 0 || IsGameOver(state))
        return EvaluatePosition(state);
    
    if (isMaximizing) // AI turn
    {
        float maxEval = float.NegativeInfinity;
        foreach (var move in GeneratePossibleMoves(state, true))
        {
            GameState newState = ApplyMove(state, move, true);
            float eval = Minimax(newState, depth - 1, false, alpha, beta);
            maxEval = Mathf.Max(maxEval, eval);
            alpha = Mathf.Max(alpha, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return maxEval;
    }
    else // Human turn
    {
        float minEval = float.PositiveInfinity;
        foreach (var move in GeneratePossibleMoves(state, false))
        {
            GameState newState = ApplyMove(state, move, false);
            float eval = Minimax(newState, depth - 1, true, alpha, beta);
            minEval = Mathf.Min(minEval, eval);
            beta = Mathf.Min(beta, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return minEval;
    }
}
```

3.4 EVALUATION FUNCTION:
Position evaluation d·ª±a tr√™n multiple factors:

```csharp
float EvaluatePosition(GameState state)
{
    float score = 0f;
    
    // Distance to goal (c√†ng g·∫ßn c√†ng t·ªët)
    int aiDistance = GetShortestPath(state.aiPosition, goalRow);
    int humanDistance = GetShortestPath(state.humanPosition, goalRow);
    score += (humanDistance - aiDistance) * 100;
    
    // Wall advantage
    score += (state.aiWallsLeft - state.humanWallsLeft) * 50;
    
    // Mobility (s·ªë moves c√≥ th·ªÉ)
    int aiMobility = GetValidMoves(state.aiPosition).Count;
    int humanMobility = GetValidMoves(state.humanPosition).Count;
    score += (aiMobility - humanMobility) * 20;
    
    // Center control
    score += GetCenterControlScore(state) * 30;
    
    return score;
}
```

3.5 ALPHA-BETA PRUNING HI·ªÜU QU·∫¢:
- Reduces search space t·ª´ O(b^d) xu·ªëng O(b^(d/2))
- Best case: 50% reduction in nodes evaluated
- Ordering moves by promising gi√∫p pruning hi·ªáu qu·∫£ h∆°n

================================================================================
                        4. SPECIALIZED AI STYLES SYSTEM
================================================================================

4.1 CONCEPT:
6 AI personalities v·ªõi training v√† behavior ri√™ng bi·ªát, m·ªói style c√≥:
- Ri√™ng Q-table file
- Custom reward structure
- Specific move prioritization
- Unique decision making logic

4.2 AI STYLES CHI TI·∫æT:

üî• AGGRESSIVE STYLE:
- M·ª•c ti√™u: T·∫•n c√¥ng v√† ch·∫∑n ƒë·ªëi th·ªß
- Wall Usage: 70% chance
- Reward Structure:
  * blockingOpponentReward = 50 * intensity
  * wallPlacementReward = 30 * intensity
  * defensiveWallPenalty = -20 * intensity
- Q-table: qtable_aggressive.json

üõ°Ô∏è DEFENSIVE STYLE:
- M·ª•c ti√™u: Ph√≤ng th·ªß v√† di chuy·ªÉn an to√†n
- Wall Usage: 50% chance
- Reward Structure:
  * defensiveWallReward = 35 * intensity
  * movementReward = 20 * intensity
  * aggressiveMovePenalty = -15 * intensity
- Q-table: qtable_defensive.json

üß± WALL MASTER STYLE:
- M·ª•c ti√™u: Chuy√™n gia ƒë·∫∑t t∆∞·ªùng v√† t·∫°o m√™ cung
- Wall Usage: 90% chance
- Reward Structure:
  * wallPlacementReward = 60 * intensity
  * strategicWallReward = 40 * intensity
  * wallComboReward = 50 * intensity
  * movementWithoutWallPenalty = -25 * intensity
- Q-table: qtable_wallmaster.json

‚ö° SPEED RUNNER STYLE:
- M·ª•c ti√™u: ƒê·∫°t ƒë√≠ch nhanh nh·∫•t
- Wall Usage: 10% chance
- Reward Structure:
  * movementReward = 40 * intensity
  * progressToGoalReward = 50 * intensity
  * quickWinBonus = 100 * intensity
  * wallPlacementPenalty = -20 * intensity
- Q-table: qtable_speedrunner.json

üß† TACTICAL STYLE:
- M·ª•c ti√™u: L·ª£i th·∫ø v·ªã tr√≠ v√† k·∫ø ho·∫°ch d√†i h·∫°n
- Wall Usage: Variable (strategic)
- Reward Structure:
  * tacticalMoveReward = 45 * intensity
  * positionAdvantageReward = 35 * intensity
  * mobilityReward = 25 * intensity
  * patternRecognitionBonus = 40 * intensity
- Q-table: qtable_tactical.json

‚öñÔ∏è BALANCED STYLE:
- M·ª•c ti√™u: C√¢n b·∫±ng t·∫•t c·∫£ aspects
- Wall Usage: 30% chance
- Reward Structure: Default balanced values
- Q-table: qtable.json

4.3 STYLE SWITCHING MECHANISM:

```csharp
public void SwitchToStyle(PlayStyle newStyle)
{
    // Save current Q-table
    if (allowQTableSaving) qAgent.SaveQTable(GetStyleSpecificQTablePath());
    
    // Load new style Q-table
    specializedStyle = newStyle;
    qAgent.LoadQTable(GetStyleSpecificQTablePath());
    
    // Reconfigure parameters
    ConfigureStyleSpecificTraining();
}
```

4.4 STYLE INTENSITY:
Parameter t·ª´ 0.0 ‚Üí 1.0 control m·ª©c ƒë·ªô chuy√™n h√≥a:
- 0.0: Balanced behavior
- 0.5: Moderate specialization
- 1.0: Extreme specialization

================================================================================
                        5. DECISION TREE CLASSIFIER
================================================================================

5.1 PURPOSE:
Decision Tree ph√¢n t√≠ch game context ƒë·ªÉ ch·ªçn strategy ph√π h·ª£p, d·ª±a tr√™n:
- Game phase (Early/Mid/End)
- Position advantage
- Resource advantage
- Mobility situation

5.2 FEATURE EXTRACTION:

```csharp
public struct DecisionFeatures
{
    public int aiDistanceToGoal;      // AI distance to goal line
    public int humanDistanceToGoal;   // Human distance to goal line
    public int aiWallsLeft;           // AI walls remaining
    public int humanWallsLeft;        // Human walls remaining
    public int aiMobility;            // AI possible moves
    public int humanMobility;         // Human possible moves
    public float distanceAdvantage;   // Human - AI distance
    public float wallAdvantage;       // AI - Human walls
    public bool isEarlyGame;          // > 15 moves estimated remaining
    public bool isMidGame;            // 5-15 moves remaining
    public bool isEndGame;            // < 5 moves remaining
}
```

5.3 DECISION LOGIC:

```csharp
public AIStrategy Predict(DecisionFeatures features)
{
    // Early Game: Play conservatively
    if (features.isEarlyGame)
        return AIStrategy.Balanced;
    
    // AI has significant lead: Focus on blocking
    if (features.distanceAdvantage > 2)
        return AIStrategy.Blocking;
    
    // AI is behind: Be aggressive
    if (features.distanceAdvantage < -1)
        return AIStrategy.Aggressive;
    
    // End game with wall advantage: Play defensively
    if (features.isEndGame && features.wallAdvantage > 2)
        return AIStrategy.Defensive;
    
    // Close game: Balanced approach
    return AIStrategy.Balanced;
}
```

5.4 STRATEGY OUTCOMES:
- Balanced: Equal weight to movement v√† walls
- Blocking: Prioritize moves that increase opponent path
- Aggressive: Focus on attacking positions
- Defensive: Protect own position and path

================================================================================
                        6. ENTROPY ANALYSIS & SOFTMAX
================================================================================

6.1 ENTROPY CONCEPT:
Entropy ƒëo ƒë·ªô ƒëa d·∫°ng/uncertainty c·ªßa move scores. High entropy = nhi·ªÅu moves
c√≥ scores t∆∞∆°ng ƒë∆∞∆°ng, low entropy = m·ªôt move r√µ r√†ng t·ªët nh·∫•t.

6.2 ENTROPY CALCULATION:

```csharp
float CalculateEntropy(List<float> moveScores)
{
    // Normalize th√†nh probabilities
    float total = moveScores.Sum();
    var probabilities = moveScores.Select(s => s / total);
    
    // Calculate Shannon entropy
    float entropy = 0f;
    foreach (float p in probabilities)
    {
        if (p > 0f)
            entropy -= p * Mathf.Log(p, 2f);
    }
    
    return entropy; // Range: 0 (deterministic) to log2(n) (uniform)
}
```

6.3 SOFTMAX DISTRIBUTION:

```csharp
float[] CalculateSoftmax(List<float> scores, float temperature)
{
    float[] probabilities = new float[scores.Count];
    float sum = scores.Sum(s => Mathf.Exp(s / temperature));
    
    for (int i = 0; i < scores.Count; i++)
    {
        probabilities[i] = Mathf.Exp(scores[i] / temperature) / sum;
    }
    
    return probabilities;
}
```

6.4 TEMPERATURE PARAMETER:
- Low temperature (0.1): Sharp distribution, focus on best moves
- High temperature (2.0): Flat distribution, more exploration
- Default temperature (1.0): Balanced exploitation/exploration

6.5 ADAPTIVE MOVE SELECTION:

```csharp
public AIMove ChooseActionWithEntropy(GameState state, List<AIMove> possibleMoves)
{
    var moveScores = EvaluateAllMoves(state, possibleMoves);
    float entropy = CalculateEntropy(moveScores);
    
    if (entropy > entropyThreshold) // High diversity
    {
        // Use softmax for probabilistic selection
        var probabilities = CalculateSoftmax(moveScores, softmaxTemperature);
        return SelectMoveByProbability(possibleMoves, probabilities);
    }
    else // Clear best move
    {
        // Use deterministic best move
        return GetBestScoredMove(possibleMoves, moveScores);
    }
}
```

================================================================================
                        7. TRAINING SYSTEM & SELF-PLAY
================================================================================

7.1 SELF-PLAY CONCEPT:
AI h·ªçc b·∫±ng c√°ch ch∆°i v·ªõi ch√≠nh n√≥ h√†ng ng√†n l·∫ßn, gradually improve performance
th√¥ng qua trial v√† error.

7.2 TRAINING PIPELINE:

```csharp
IEnumerator RunSelfPlayTraining()
{
    for (int episode = 1; episode <= maxEpisodes; episode++)
    {
        // Reset game state
        ResetGameForTraining();
        
        while (!IsGameOver())
        {
            // Get current state
            GameState currentState = GetCurrentGameState();
            List<AIMove> possibleMoves = GetPossibleMoves();
            
            // Choose action using Œµ-greedy
            AIMove chosenMove = qLearningAgent.ChooseAction(currentState, possibleMoves);
            
            // Execute move
            ExecuteMove(chosenMove);
            
            // Calculate reward
            float reward = CalculateReward(currentState, chosenMove);
            
            // Update Q-table
            GameState newState = GetCurrentGameState();
            qLearningAgent.UpdateQValue(
                EncodeState(currentState),
                EncodeAction(chosenMove),
                reward,
                EncodeState(newState)
            );
            
            yield return new WaitForEndOfFrame();
        }
        
        // Epsilon decay
        qLearningAgent.DecayEpsilon();
        
        // Save progress periodically
        if (episode % 100 == 0)
        {
            qLearningAgent.SaveQTable();
        }
    }
}
```

7.3 REWARD ENGINEERING:
Critical cho Q-Learning success:

```csharp
float CalculateReward(GameState state, AIMove move)
{
    float reward = 0f;
    
    // Game outcome rewards
    if (IsWin(state)) reward += 1000f;
    if (IsLoss(state)) reward -= 1000f;
    
    // Progress rewards
    if (move.moveType == MoveType.Movement)
    {
        if (IsMoveTowardGoal(move)) reward += 20f;
        else reward -= 5f;
    }
    
    // Strategic rewards
    if (move.moveType == MoveType.WallPlacement)
    {
        if (IsWallBlockingOpponent(move)) reward += 50f;
        if (IsWallStrategic(move)) reward += 30f;
        if (IsWallWasted(move)) reward -= 25f;
    }
    
    // Style-specific rewards
    reward += GetStyleSpecificReward(state, move);
    
    return reward;
}
```

7.4 TRAINING METRICS:
- Episodes: 1000 standard
- Duration: ~15 minutes
- Convergence: Usually around episode 800
- Q-table size: 13,118+ states at completion
- Final epsilon: 0.1 (90% exploitation)

================================================================================
                        8. AI TESTING FRAMEWORK
================================================================================

8.1 TESTING PHILOSOPHY:
Comprehensive testing ensures AI quality v√† reliability across different
game scenarios v√† edge cases.

8.2 TEST CATEGORIES:

A) DECISION TREE TESTS:
```csharp
void TestDecisionTreeStrategies()
{
    // Test Early Game
    var earlyGameFeatures = new DecisionFeatures
    {
        aiDistanceToGoal = 8, humanDistanceToGoal = 8,
        isEarlyGame = true
    };
    var strategy = decisionTree.Predict(earlyGameFeatures);
    Assert.AreEqual(AIStrategy.Balanced, strategy);
    
    // Test AI Leading
    var leadingFeatures = new DecisionFeatures
    {
        distanceAdvantage = 3, // AI leading
        isMidGame = true
    };
    strategy = decisionTree.Predict(leadingFeatures);
    Assert.AreEqual(AIStrategy.Blocking, strategy);
}
```

B) ENTROPY ANALYSIS TESTS:
```csharp
void TestEntropyAnalysis()
{
    // High entropy case (diverse scores)
    var highEntropyScores = new List<float> { 100f, 98f, 97f, 95f, 94f };
    float entropy = CalculateTestEntropy(highEntropyScores);
    Assert.IsTrue(entropy > 0.5f);
    
    // Low entropy case (clear winner)
    var lowEntropyScores = new List<float> { 100f, 50f, 50f, 50f, 50f };
    entropy = CalculateTestEntropy(lowEntropyScores);
    Assert.IsTrue(entropy < 0.5f);
}
```

C) SOFTMAX DISTRIBUTION TESTS:
```csharp
void TestSoftmaxDistribution()
{
    var testScores = new List<float> { 100f, 98f, 95f, 90f };
    float temperature = 1.0f;
    
    var probabilities = CalculateSoftmax(testScores, temperature);
    
    // Verify probabilities sum to 1
    float sum = probabilities.Sum();
    Assert.AreEqual(1.0f, sum, 0.001f);
    
    // Verify highest score gets highest probability
    int maxIndex = Array.IndexOf(probabilities, probabilities.Max());
    Assert.AreEqual(0, maxIndex);
}
```

D) PERFORMANCE BENCHMARKS:
- Move generation time: <100ms
- Q-Learning decision: <50ms
- Minimax decision (depth 4): <200ms
- Memory usage: <256MB

8.3 AUTOMATED TESTING:
Tests run automatically ƒë·ªÉ ensure code quality:
- Unit tests cho individual algorithms
- Integration tests cho AI system interactions
- Performance tests cho response times
- Regression tests ƒë·ªÉ prevent quality degradation

================================================================================
                        9. HYBRID AI ARCHITECTURE
================================================================================

9.1 CONCEPT:
Hybrid system k·∫øt h·ª£p strengths c·ªßa multiple algorithms:
- Q-Learning: Data-driven, learns from experience
- Minimax: Logical, guaranteed optimal given evaluation
- Decision Tree: Fast, context-aware switching
- Entropy: Adaptive, handles uncertainty

9.2 ALGORITHM SELECTION LOGIC:

```csharp
public AIMove GetBestMove()
{
    UpdateGameState();
    List<AIMove> possibleMoves = GeneratePossibleMoves();
    
    // Primary: Use Q-Learning if trained model available
    if (useQLearning && qAgent.IsTrainedModel())
    {
        Debug.Log($"üß† Using {specializedStyle} Q-Learning");
        return qAgent.ChooseAction(currentGameState, possibleMoves);
    }
    
    // Fallback: Use Minimax for guaranteed reasonable play
    else
    {
        Debug.Log($"üîç Using {specializedStyle} Minimax");
        return FindBestMoveWithMinimax();
    }
}
```

9.3 SEAMLESS SWITCHING:
- No interruption in gameplay
- Maintains consistent API
- Preserves game state integrity
- Logs algorithm used for debugging

9.4 BENEFITS:
- Robustness: Fallback ensures AI never fails
- Performance: Q-Learning for trained scenarios, Minimax for edge cases
- Flexibility: Can switch algorithms based on situation
- Development: Easy to test individual components

================================================================================
                        10. PERFORMANCE OPTIMIZATION
================================================================================

10.1 Q-TABLE OPTIMIZATION:
- Sparse representation: Only store visited states
- Compression: String interning for state keys
- Lazy loading: Load Q-table on demand
- Periodic saving: Every 100 training episodes

10.2 MINIMAX OPTIMIZATION:
- Alpha-Beta pruning: 50% search reduction
- Move ordering: Promising moves first
- Iterative deepening: Gradual depth increase
- Transposition tables: Cache evaluated positions

10.3 MEMORY MANAGEMENT:
- Object pooling cho GameState objects
- String caching cho repeated state/action encodings
- Garbage collection optimization
- Memory profiling v√† monitoring

10.4 MULTITHREADING CONSIDERATIONS:
- Q-Learning training: Background thread
- Move calculation: Main thread (Unity requirement)
- File I/O: Async operations where possible
- Thread-safe collections cho shared data

10.5 CACHING STRATEGIES:
- Path finding results
- Move generation results
- Evaluation function results
- Feature extraction results

================================================================================
                            K·∫æT LU·∫¨N
================================================================================

D·ª± √°n AI Quoridor Project th·ªÉ hi·ªán m·ªôt implementation comprehensive c·ªßa modern
game AI techniques:

TECHNICAL INNOVATIONS:
‚úÖ Hybrid Q-Learning + Minimax architecture
‚úÖ 6 specialized AI personalities v·ªõi separate training
‚úÖ Advanced decision making v·ªõi entropy analysis
‚úÖ Comprehensive testing v√† validation framework
‚úÖ Professional development tools v√† debugging

EDUCATIONAL VALUE:
‚úÖ Practical machine learning implementation
‚úÖ Traditional game AI algorithms
‚úÖ Modern software engineering practices
‚úÖ Performance optimization techniques
‚úÖ Automated testing methodologies

FUTURE ENHANCEMENTS:
üöÄ Deep Q-Learning (DQN) integration
üöÄ Multi-agent reinforcement learning
üöÄ Neural network-based evaluation functions
üöÄ Online learning v√† adaptation
üöÄ Transfer learning between game variants

D·ª± √°n n√†y demonstrating successful integration c·ªßa classical AI methods v·ªõi
modern machine learning, t·∫°o ra m·ªôt intelligent, adaptive, v√† highly
interactive gaming experience.

================================================================================
                            APPENDIX
================================================================================

FILES LI√äN QUAN:
- Assets/scripts/QLearningAgent.cs - Q-Learning implementation
- Assets/scripts/QuoridorAI.cs - Main AI controller
- Assets/scripts/DecisionTreeClassifier.cs - Decision tree logic
- Assets/scripts/QuoridorSelfPlayTrainer.cs - Training system
- Assets/scripts/AITestScript.cs - Testing framework
- Assets/scripts/Editor/ForceRetrainAI.cs - Development tools

Q-TABLE FILES:
- StreamingAssets/qtable.json - Balanced style
- StreamingAssets/qtable_aggressive.json - Aggressive style
- StreamingAssets/qtable_defensive.json - Defensive style
- StreamingAssets/qtable_wallmaster.json - Wall Master style
- StreamingAssets/qtable_speedrunner.json - Speed Runner style
- StreamingAssets/qtable_tactical.json - Tactical style

DOCUMENTATION:
- AI_ALGORITHMS_DETAILED_DESCRIPTION.txt - This file
- README.md - Project overview
- API_INTEGRATION_GUIDE.md - Voice/Chat integration
- TRAINING_GUIDE.md - AI training instructions

================================================================================
================================================================================
                    AI QUORIDOR GAME PROJECT - THU·∫¨T TO√ÅN AI CHI TI·∫æT
================================================================================

D·ª± √°n: AI Quoridor Game v·ªõi Machine Learning v√† Multi-Platform Integration
Ng√†y t·∫°o: 25/07/2025
T√°c gi·∫£: [Student Name]
Platform: Unity 3D
Language: C#

================================================================================
                            M·ª§C L·ª§C
================================================================================

1. T·ªîNG QUAN KI·∫æN TR√öC AI
2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
4. SPECIALIZED AI STYLES SYSTEM
5. DECISION TREE CLASSIFIER
6. ENTROPY ANALYSIS & SOFTMAX
7. TRAINING SYSTEM & SELF-PLAY
8. AI TESTING FRAMEWORK
9. HYBRID AI ARCHITECTURE
10. PERFORMANCE OPTIMIZATION

================================================================================
                        1. T·ªîNG QUAN KI·∫æN TR√öC AI
================================================================================

D·ª± √°n AI Quoridor s·ª≠ d·ª•ng m·ªôt ki·∫øn tr√∫c AI hybrid ti√™n ti·∫øn, k·∫øt h·ª£p:

1.1 CORE ALGORITHMS:
- Q-Learning (Reinforcement Learning) - Thu·∫≠t to√°n ch√≠nh
- Minimax + Alpha-Beta Pruning - Thu·∫≠t to√°n fallback
- Decision Tree Classifier - Ph√¢n t√≠ch ng·ªØ c·∫£nh game
- Entropy Analysis - ƒê√°nh gi√° ƒëa d·∫°ng move

1.2 SPECIALIZED SYSTEMS:
- 6 AI Personality Styles v·ªõi training ri√™ng bi·ªát
- Self-Play Training Pipeline t·ª± ƒë·ªông
- Real-time Performance Testing Framework
- Dynamic Algorithm Switching

1.3 TECHNICAL FOUNDATION:
- State Space: 13,118+ unique game states
- Action Space: Movement + Wall placement combinations
- Training Method: Self-play v·ªõi 1000 episodes
- Learning Rate: Œ± = 0.1, Discount Factor: Œ≥ = 0.99

================================================================================
                    2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
================================================================================

2.1 GI·ªöI THI·ªÜU Q-LEARNING:
Q-Learning l√† thu·∫≠t to√°n Reinforcement Learning kh√¥ng c·∫ßn model, h·ªçc th√¥ng qua
t∆∞∆°ng t√°c v·ªõi m√¥i tr∆∞·ªùng ƒë·ªÉ t·ªëi ∆∞u h√≥a policy. Trong Quoridor, AI h·ªçc c√°ch
ch·ªçn moves t·ªëi ∆∞u d·ª±a tr√™n kinh nghi·ªám t·ª´ h√†ng ng√†n game.

2.2 C√îNG TH·ª®C TO√ÅN H·ªåC:
Q(s,a) = Q(s,a) + Œ±[r + Œ≥*max(Q(s',a')) - Q(s,a)]

Trong ƒë√≥:
- Q(s,a): Q-value c·ªßa state s v·ªõi action a
- Œ±: Learning rate (0.1) - t·ªëc ƒë·ªô h·ªçc
- r: Immediate reward t·ª´ action
- Œ≥: Discount factor (0.99) - t·∫ßm quan tr·ªçng future rewards
- s': Next state sau khi th·ª±c hi·ªán action
- max(Q(s',a')): Maximum Q-value c·ªßa next state

2.3 IMPLEMENTATION CHI TI·∫æT:

```csharp
public class QLearningAgent
{
    // Core Q-Table: Dictionary l∆∞u tr·ªØ Q-values
    private Dictionary<string, Dictionary<string, float>> qTable;
    
    // Hyperparameters
    private float learningRate = 0.1f;      // T·ªëc ƒë·ªô h·ªçc
    private float discountFactor = 0.99f;   // Discount future rewards
    private float epsilon = 0.995f;         // Exploration rate
    
    // Q-Learning Update Implementation
    public void UpdateQValue(string state, string action, float reward, string nextState)
    {
        // L·∫•y Q-value hi·ªán t·∫°i
        float currentQ = GetQValue(state, action);
        
        // T√¨m max Q-value c·ªßa next state
        float maxNextQ = GetMaxQValue(nextState);
        
        // √Åp d·ª•ng c√¥ng th·ª©c Q-Learning
        float newQ = currentQ + learningRate * (reward + discountFactor * maxNextQ - currentQ);
        
        // C·∫≠p nh·∫≠t Q-table
        SetQValue(state, action, newQ);
    }
}
```

2.4 STATE ENCODING:
Game state ƒë∆∞·ª£c encode th√†nh string duy nh·∫•t:
"aiPos_x,aiPos_y-humanPos_x,humanPos_y-aiWalls-humanWalls-wallsList"

V√≠ d·ª•: "4,8-4,0-9-10-7,4,False|5,6,True"
- AI ·ªü (4,8), Human ·ªü (4,0)
- AI c√≥ 9 walls, Human c√≥ 10 walls
- Walls ƒë√£ ƒë·∫∑t: (7,4,False), (5,6,True)

2.5 ACTION ENCODING:
Actions ƒë∆∞·ª£c encode th√†nh string:
- Movement: "M:x,y" (v√≠ d·ª•: "M:4,7")
- Wall: "W:x,y,horizontal" (v√≠ d·ª•: "W:3,5,True")

2.6 EPSILON-GREEDY STRATEGY:
```csharp
public AIMove ChooseAction(GameState state, List<AIMove> possibleMoves)
{
    // Exploration vs Exploitation trade-off
    if (Random.Range(0f, 1f) < epsilon)
    {
        // EXPLORATION: Ch·ªçn random ƒë·ªÉ kh√°m ph√°
        return possibleMoves[Random.Range(0, possibleMoves.Count)];
    }
    else
    {
        // EXPLOITATION: Ch·ªçn action c√≥ Q-value cao nh·∫•t
        return GetBestAction(state, possibleMoves);
    }
}
```

2.7 EPSILON DECAY:
Epsilon gi·∫£m d·∫ßn t·ª´ 0.995 ‚Üí 0.1 qua training:
- Ban ƒë·∫ßu: High exploration (99.5%)
- Cu·ªëi training: High exploitation (90%)

2.8 REWARD SYSTEM:
- Win game: +1000
- Lose game: -1000
- Move toward goal: +10
- Block opponent: +50
- Waste move: -5
- Strategic position: +15

================================================================================
                3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
================================================================================

3.1 GI·ªöI THI·ªÜU MINIMAX:
Minimax l√† thu·∫≠t to√°n game theory c·ªï ƒëi·ªÉn, t√¨m ki·∫øm move t·ªëi ∆∞u b·∫±ng c√°ch
d·ª± ƒëo√°n tr∆∞·ªõc nhi·ªÅu moves. Alpha-Beta pruning t·ªëi ∆∞u hi·ªáu su·∫•t b·∫±ng c√°ch
lo·∫°i b·ªè c√°c nh√°nh kh√¥ng c·∫ßn thi·∫øt.

3.2 NGUY√äN L√ù HO·∫†T ƒê·ªòNG:
- Maximizing Player (AI): T·ªëi ƒëa h√≥a score
- Minimizing Player (Human): T·ªëi thi·ªÉu h√≥a score AI
- Search depth: 4 levels (configurable)
- Pruning: Lo·∫°i b·ªè nh√°nh kh√¥ng promising

3.3 IMPLEMENTATION:

```csharp
float Minimax(GameState state, int depth, bool isMaximizing, float alpha, float beta)
{
    // Base case: Depth = 0 ho·∫∑c game over
    if (depth == 0 || IsGameOver(state))
        return EvaluatePosition(state);
    
    if (isMaximizing) // AI turn
    {
        float maxEval = float.NegativeInfinity;
        foreach (var move in GeneratePossibleMoves(state, true))
        {
            GameState newState = ApplyMove(state, move, true);
            float eval = Minimax(newState, depth - 1, false, alpha, beta);
            maxEval = Mathf.Max(maxEval, eval);
            alpha = Mathf.Max(alpha, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return maxEval;
    }
    else // Human turn
    {
        float minEval = float.PositiveInfinity;
        foreach (var move in GeneratePossibleMoves(state, false))
        {
            GameState newState = ApplyMove(state, move, false);
            float eval = Minimax(newState, depth - 1, true, alpha, beta);
            minEval = Mathf.Min(minEval, eval);
            beta = Mathf.Min(beta, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return minEval;
    }
}
```

3.4 EVALUATION FUNCTION:
Position evaluation d·ª±a tr√™n multiple factors:

```csharp
float EvaluatePosition(GameState state)
{
    float score = 0f;
    
    // Distance to goal (c√†ng g·∫ßn c√†ng t·ªët)
    int aiDistance = GetShortestPath(state.aiPosition, goalRow);
    int humanDistance = GetShortestPath(state.humanPosition, goalRow);
    score += (humanDistance - aiDistance) * 100;
    
    // Wall advantage
    score += (state.aiWallsLeft - state.humanWallsLeft) * 50;
    
    // Mobility (s·ªë moves c√≥ th·ªÉ)
    int aiMobility = GetValidMoves(state.aiPosition).Count;
    int humanMobility = GetValidMoves(state.humanPosition).Count;
    score += (aiMobility - humanMobility) * 20;
    
    // Center control
    score += GetCenterControlScore(state) * 30;
    
    return score;
}
```

3.5 ALPHA-BETA PRUNING HI·ªÜU QU·∫¢:
- Reduces search space t·ª´ O(b^d) xu·ªëng O(b^(d/2))
- Best case: 50% reduction in nodes evaluated
- Ordering moves by promising gi√∫p pruning hi·ªáu qu·∫£ h∆°n

================================================================================
                        4. SPECIALIZED AI STYLES SYSTEM
================================================================================

4.1 CONCEPT:
6 AI personalities v·ªõi training v√† behavior ri√™ng bi·ªát, m·ªói style c√≥:
- Ri√™ng Q-table file
- Custom reward structure
- Specific move prioritization
- Unique decision making logic

4.2 AI STYLES CHI TI·∫æT:

üî• AGGRESSIVE STYLE:
- M·ª•c ti√™u: T·∫•n c√¥ng v√† ch·∫∑n ƒë·ªëi th·ªß
- Wall Usage: 70% chance
- Reward Structure:
  * blockingOpponentReward = 50 * intensity
  * wallPlacementReward = 30 * intensity
  * defensiveWallPenalty = -20 * intensity
- Q-table: qtable_aggressive.json

üõ°Ô∏è DEFENSIVE STYLE:
- M·ª•c ti√™u: Ph√≤ng th·ªß v√† di chuy·ªÉn an to√†n
- Wall Usage: 50% chance
- Reward Structure:
  * defensiveWallReward = 35 * intensity
  * movementReward = 20 * intensity
  * aggressiveMovePenalty = -15 * intensity
- Q-table: qtable_defensive.json

üß± WALL MASTER STYLE:
- M·ª•c ti√™u: Chuy√™n gia ƒë·∫∑t t∆∞·ªùng v√† t·∫°o m√™ cung
- Wall Usage: 90% chance
- Reward Structure:
  * wallPlacementReward = 60 * intensity
  * strategicWallReward = 40 * intensity
  * wallComboReward = 50 * intensity
  * movementWithoutWallPenalty = -25 * intensity
- Q-table: qtable_wallmaster.json

‚ö° SPEED RUNNER STYLE:
- M·ª•c ti√™u: ƒê·∫°t ƒë√≠ch nhanh nh·∫•t
- Wall Usage: 10% chance
- Reward Structure:
  * movementReward = 40 * intensity
  * progressToGoalReward = 50 * intensity
  * quickWinBonus = 100 * intensity
  * wallPlacementPenalty = -20 * intensity
- Q-table: qtable_speedrunner.json

üß† TACTICAL STYLE:
- M·ª•c ti√™u: L·ª£i th·∫ø v·ªã tr√≠ v√† k·∫ø ho·∫°ch d√†i h·∫°n
- Wall Usage: Variable (strategic)
- Reward Structure:
  * tacticalMoveReward = 45 * intensity
  * positionAdvantageReward = 35 * intensity
  * mobilityReward = 25 * intensity
  * patternRecognitionBonus = 40 * intensity
- Q-table: qtable_tactical.json

‚öñÔ∏è BALANCED STYLE:
- M·ª•c ti√™u: C√¢n b·∫±ng t·∫•t c·∫£ aspects
- Wall Usage: 30% chance
- Reward Structure: Default balanced values
- Q-table: qtable.json

4.3 STYLE SWITCHING MECHANISM:

```csharp
public void SwitchToStyle(PlayStyle newStyle)
{
    // Save current Q-table
    if (allowQTableSaving) qAgent.SaveQTable(GetStyleSpecificQTablePath());
    
    // Load new style Q-table
    specializedStyle = newStyle;
    qAgent.LoadQTable(GetStyleSpecificQTablePath());
    
    // Reconfigure parameters
    ConfigureStyleSpecificTraining();
}
```

4.4 STYLE INTENSITY:
Parameter t·ª´ 0.0 ‚Üí 1.0 control m·ª©c ƒë·ªô chuy√™n h√≥a:
- 0.0: Balanced behavior
- 0.5: Moderate specialization
- 1.0: Extreme specialization

================================================================================
                        5. DECISION TREE CLASSIFIER
================================================================================

5.1 PURPOSE:
Decision Tree ph√¢n t√≠ch game context ƒë·ªÉ ch·ªçn strategy ph√π h·ª£p, d·ª±a tr√™n:
- Game phase (Early/Mid/End)
- Position advantage
- Resource advantage
- Mobility situation

5.2 FEATURE EXTRACTION:

```csharp
public struct DecisionFeatures
{
    public int aiDistanceToGoal;      // AI distance to goal line
    public int humanDistanceToGoal;   // Human distance to goal line
    public int aiWallsLeft;           // AI walls remaining
    public int humanWallsLeft;        // Human walls remaining
    public int aiMobility;            // AI possible moves
    public int humanMobility;         // Human possible moves
    public float distanceAdvantage;   // Human - AI distance
    public float wallAdvantage;       // AI - Human walls
    public bool isEarlyGame;          // > 15 moves estimated remaining
    public bool isMidGame;            // 5-15 moves remaining
    public bool isEndGame;            // < 5 moves remaining
}
```

5.3 DECISION LOGIC:

```csharp
public AIStrategy Predict(DecisionFeatures features)
{
    // Early Game: Play conservatively
    if (features.isEarlyGame)
        return AIStrategy.Balanced;
    
    // AI has significant lead: Focus on blocking
    if (features.distanceAdvantage > 2)
        return AIStrategy.Blocking;
    
    // AI is behind: Be aggressive
    if (features.distanceAdvantage < -1)
        return AIStrategy.Aggressive;
    
    // End game with wall advantage: Play defensively
    if (features.isEndGame && features.wallAdvantage > 2)
        return AIStrategy.Defensive;
    
    // Close game: Balanced approach
    return AIStrategy.Balanced;
}
```

5.4 STRATEGY OUTCOMES:
- Balanced: Equal weight to movement v√† walls
- Blocking: Prioritize moves that increase opponent path
- Aggressive: Focus on attacking positions
- Defensive: Protect own position and path

================================================================================
                        6. ENTROPY ANALYSIS & SOFTMAX
================================================================================

6.1 ENTROPY CONCEPT:
Entropy ƒëo ƒë·ªô ƒëa d·∫°ng/uncertainty c·ªßa move scores. High entropy = nhi·ªÅu moves
c√≥ scores t∆∞∆°ng ƒë∆∞∆°ng, low entropy = m·ªôt move r√µ r√†ng t·ªët nh·∫•t.

6.2 ENTROPY CALCULATION:

```csharp
float CalculateEntropy(List<float> moveScores)
{
    // Normalize th√†nh probabilities
    float total = moveScores.Sum();
    var probabilities = moveScores.Select(s => s / total);
    
    // Calculate Shannon entropy
    float entropy = 0f;
    foreach (float p in probabilities)
    {
        if (p > 0f)
            entropy -= p * Mathf.Log(p, 2f);
    }
    
    return entropy; // Range: 0 (deterministic) to log2(n) (uniform)
}
```

6.3 SOFTMAX DISTRIBUTION:

```csharp
float[] CalculateSoftmax(List<float> scores, float temperature)
{
    float[] probabilities = new float[scores.Count];
    float sum = scores.Sum(s => Mathf.Exp(s / temperature));
    
    for (int i = 0; i < scores.Count; i++)
    {
        probabilities[i] = Mathf.Exp(scores[i] / temperature) / sum;
    }
    
    return probabilities;
}
```

6.4 TEMPERATURE PARAMETER:
- Low temperature (0.1): Sharp distribution, focus on best moves
- High temperature (2.0): Flat distribution, more exploration
- Default temperature (1.0): Balanced exploitation/exploration

6.5 ADAPTIVE MOVE SELECTION:

```csharp
public AIMove ChooseActionWithEntropy(GameState state, List<AIMove> possibleMoves)
{
    var moveScores = EvaluateAllMoves(state, possibleMoves);
    float entropy = CalculateEntropy(moveScores);
    
    if (entropy > entropyThreshold) // High diversity
    {
        // Use softmax for probabilistic selection
        var probabilities = CalculateSoftmax(moveScores, softmaxTemperature);
        return SelectMoveByProbability(possibleMoves, probabilities);
    }
    else // Clear best move
    {
        // Use deterministic best move
        return GetBestScoredMove(possibleMoves, moveScores);
    }
}
```

================================================================================
                        7. TRAINING SYSTEM & SELF-PLAY
================================================================================

7.1 SELF-PLAY CONCEPT:
AI h·ªçc b·∫±ng c√°ch ch∆°i v·ªõi ch√≠nh n√≥ h√†ng ng√†n l·∫ßn, gradually improve performance
th√¥ng qua trial v√† error.

7.2 TRAINING PIPELINE:

```csharp
IEnumerator RunSelfPlayTraining()
{
    for (int episode = 1; episode <= maxEpisodes; episode++)
    {
        // Reset game state
        ResetGameForTraining();
        
        while (!IsGameOver())
        {
            // Get current state
            GameState currentState = GetCurrentGameState();
            List<AIMove> possibleMoves = GetPossibleMoves();
            
            // Choose action using Œµ-greedy
            AIMove chosenMove = qLearningAgent.ChooseAction(currentState, possibleMoves);
            
            // Execute move
            ExecuteMove(chosenMove);
            
            // Calculate reward
            float reward = CalculateReward(currentState, chosenMove);
            
            // Update Q-table
            GameState newState = GetCurrentGameState();
            qLearningAgent.UpdateQValue(
                EncodeState(currentState),
                EncodeAction(chosenMove),
                reward,
                EncodeState(newState)
            );
            
            yield return new WaitForEndOfFrame();
        }
        
        // Epsilon decay
        qLearningAgent.DecayEpsilon();
        
        // Save progress periodically
        if (episode % 100 == 0)
        {
            qLearningAgent.SaveQTable();
        }
    }
}
```

7.3 REWARD ENGINEERING:
Critical cho Q-Learning success:

```csharp
float CalculateReward(GameState state, AIMove move)
{
    float reward = 0f;
    
    // Game outcome rewards
    if (IsWin(state)) reward += 1000f;
    if (IsLoss(state)) reward -= 1000f;
    
    // Progress rewards
    if (move.moveType == MoveType.Movement)
    {
        if (IsMoveTowardGoal(move)) reward += 20f;
        else reward -= 5f;
    }
    
    // Strategic rewards
    if (move.moveType == MoveType.WallPlacement)
    {
        if (IsWallBlockingOpponent(move)) reward += 50f;
        if (IsWallStrategic(move)) reward += 30f;
        if (IsWallWasted(move)) reward -= 25f;
    }
    
    // Style-specific rewards
    reward += GetStyleSpecificReward(state, move);
    
    return reward;
}
```

7.4 TRAINING METRICS:
- Episodes: 1000 standard
- Duration: ~15 minutes
- Convergence: Usually around episode 800
- Q-table size: 13,118+ states at completion
- Final epsilon: 0.1 (90% exploitation)

================================================================================
                        8. AI TESTING FRAMEWORK
================================================================================

8.1 TESTING PHILOSOPHY:
Comprehensive testing ensures AI quality v√† reliability across different
game scenarios v√† edge cases.

8.2 TEST CATEGORIES:

A) DECISION TREE TESTS:
```csharp
void TestDecisionTreeStrategies()
{
    // Test Early Game
    var earlyGameFeatures = new DecisionFeatures
    {
        aiDistanceToGoal = 8, humanDistanceToGoal = 8,
        isEarlyGame = true
    };
    var strategy = decisionTree.Predict(earlyGameFeatures);
    Assert.AreEqual(AIStrategy.Balanced, strategy);
    
    // Test AI Leading
    var leadingFeatures = new DecisionFeatures
    {
        distanceAdvantage = 3, // AI leading
        isMidGame = true
    };
    strategy = decisionTree.Predict(leadingFeatures);
    Assert.AreEqual(AIStrategy.Blocking, strategy);
}
```

B) ENTROPY ANALYSIS TESTS:
```csharp
void TestEntropyAnalysis()
{
    // High entropy case (diverse scores)
    var highEntropyScores = new List<float> { 100f, 98f, 97f, 95f, 94f };
    float entropy = CalculateTestEntropy(highEntropyScores);
    Assert.IsTrue(entropy > 0.5f);
    
    // Low entropy case (clear winner)
    var lowEntropyScores = new List<float> { 100f, 50f, 50f, 50f, 50f };
    entropy = CalculateTestEntropy(lowEntropyScores);
    Assert.IsTrue(entropy < 0.5f);
}
```

C) SOFTMAX DISTRIBUTION TESTS:
```csharp
void TestSoftmaxDistribution()
{
    var testScores = new List<float> { 100f, 98f, 95f, 90f };
    float temperature = 1.0f;
    
    var probabilities = CalculateSoftmax(testScores, temperature);
    
    // Verify probabilities sum to 1
    float sum = probabilities.Sum();
    Assert.AreEqual(1.0f, sum, 0.001f);
    
    // Verify highest score gets highest probability
    int maxIndex = Array.IndexOf(probabilities, probabilities.Max());
    Assert.AreEqual(0, maxIndex);
}
```

D) PERFORMANCE BENCHMARKS:
- Move generation time: <100ms
- Q-Learning decision: <50ms
- Minimax decision (depth 4): <200ms
- Memory usage: <256MB

8.3 AUTOMATED TESTING:
Tests run automatically ƒë·ªÉ ensure code quality:
- Unit tests cho individual algorithms
- Integration tests cho AI system interactions
- Performance tests cho response times
- Regression tests ƒë·ªÉ prevent quality degradation

================================================================================
                        9. HYBRID AI ARCHITECTURE
================================================================================

9.1 CONCEPT:
Hybrid system k·∫øt h·ª£p strengths c·ªßa multiple algorithms:
- Q-Learning: Data-driven, learns from experience
- Minimax: Logical, guaranteed optimal given evaluation
- Decision Tree: Fast, context-aware switching
- Entropy: Adaptive, handles uncertainty

9.2 ALGORITHM SELECTION LOGIC:

```csharp
public AIMove GetBestMove()
{
    UpdateGameState();
    List<AIMove> possibleMoves = GeneratePossibleMoves();
    
    // Primary: Use Q-Learning if trained model available
    if (useQLearning && qAgent.IsTrainedModel())
    {
        Debug.Log($"üß† Using {specializedStyle} Q-Learning");
        return qAgent.ChooseAction(currentGameState, possibleMoves);
    }
    
    // Fallback: Use Minimax for guaranteed reasonable play
    else
    {
        Debug.Log($"üîç Using {specializedStyle} Minimax");
        return FindBestMoveWithMinimax();
    }
}
```

9.3 SEAMLESS SWITCHING:
- No interruption in gameplay
- Maintains consistent API
- Preserves game state integrity
- Logs algorithm used for debugging

9.4 BENEFITS:
- Robustness: Fallback ensures AI never fails
- Performance: Q-Learning for trained scenarios, Minimax for edge cases
- Flexibility: Can switch algorithms based on situation
- Development: Easy to test individual components

================================================================================
                        10. PERFORMANCE OPTIMIZATION
================================================================================

10.1 Q-TABLE OPTIMIZATION:
- Sparse representation: Only store visited states
- Compression: String interning for state keys
- Lazy loading: Load Q-table on demand
- Periodic saving: Every 100 training episodes

10.2 MINIMAX OPTIMIZATION:
- Alpha-Beta pruning: 50% search reduction
- Move ordering: Promising moves first
- Iterative deepening: Gradual depth increase
- Transposition tables: Cache evaluated positions

10.3 MEMORY MANAGEMENT:
- Object pooling cho GameState objects
- String caching cho repeated state/action encodings
- Garbage collection optimization
- Memory profiling v√† monitoring

10.4 MULTITHREADING CONSIDERATIONS:
- Q-Learning training: Background thread
- Move calculation: Main thread (Unity requirement)
- File I/O: Async operations where possible
- Thread-safe collections cho shared data

10.5 CACHING STRATEGIES:
- Path finding results
- Move generation results
- Evaluation function results
- Feature extraction results

================================================================================
                            K·∫æT LU·∫¨N
================================================================================

D·ª± √°n AI Quoridor Project th·ªÉ hi·ªán m·ªôt implementation comprehensive c·ªßa modern
game AI techniques:

TECHNICAL INNOVATIONS:
‚úÖ Hybrid Q-Learning + Minimax architecture
‚úÖ 6 specialized AI personalities v·ªõi separate training
‚úÖ Advanced decision making v·ªõi entropy analysis
‚úÖ Comprehensive testing v√† validation framework
‚úÖ Professional development tools v√† debugging

EDUCATIONAL VALUE:
‚úÖ Practical machine learning implementation
‚úÖ Traditional game AI algorithms
‚úÖ Modern software engineering practices
‚úÖ Performance optimization techniques
‚úÖ Automated testing methodologies

FUTURE ENHANCEMENTS:
üöÄ Deep Q-Learning (DQN) integration
üöÄ Multi-agent reinforcement learning
üöÄ Neural network-based evaluation functions
üöÄ Online learning v√† adaptation
üöÄ Transfer learning between game variants

D·ª± √°n n√†y demonstrating successful integration c·ªßa classical AI methods v·ªõi
modern machine learning, t·∫°o ra m·ªôt intelligent, adaptive, v√† highly
interactive gaming experience.

================================================================================
                            APPENDIX
================================================================================

FILES LI√äN QUAN:
- Assets/scripts/QLearningAgent.cs - Q-Learning implementation
- Assets/scripts/QuoridorAI.cs - Main AI controller
- Assets/scripts/DecisionTreeClassifier.cs - Decision tree logic
- Assets/scripts/QuoridorSelfPlayTrainer.cs - Training system
- Assets/scripts/AITestScript.cs - Testing framework
- Assets/scripts/Editor/ForceRetrainAI.cs - Development tools

Q-TABLE FILES:
- StreamingAssets/qtable.json - Balanced style
- StreamingAssets/qtable_aggressive.json - Aggressive style
- StreamingAssets/qtable_defensive.json - Defensive style
- StreamingAssets/qtable_wallmaster.json - Wall Master style
- StreamingAssets/qtable_speedrunner.json - Speed Runner style
- StreamingAssets/qtable_tactical.json - Tactical style

DOCUMENTATION:
- AI_ALGORITHMS_DETAILED_DESCRIPTION.txt - This file
- README.md - Project overview
- API_INTEGRATION_GUIDE.md - Voice/Chat integration
- TRAINING_GUIDE.md - AI training instructions

================================================================================
================================================================================
                    AI QUORIDOR GAME PROJECT - THU·∫¨T TO√ÅN AI CHI TI·∫æT
================================================================================

D·ª± √°n: AI Quoridor Game v·ªõi Machine Learning v√† Multi-Platform Integration
Ng√†y t·∫°o: 25/07/2025
T√°c gi·∫£: [Student Name]
Platform: Unity 3D
Language: C#

================================================================================
                            M·ª§C L·ª§C
================================================================================

1. T·ªîNG QUAN KI·∫æN TR√öC AI
2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
4. SPECIALIZED AI STYLES SYSTEM
5. DECISION TREE CLASSIFIER
6. ENTROPY ANALYSIS & SOFTMAX
7. TRAINING SYSTEM & SELF-PLAY
8. AI TESTING FRAMEWORK
9. HYBRID AI ARCHITECTURE
10. PERFORMANCE OPTIMIZATION

================================================================================
                        1. T·ªîNG QUAN KI·∫æN TR√öC AI
================================================================================

D·ª± √°n AI Quoridor s·ª≠ d·ª•ng m·ªôt ki·∫øn tr√∫c AI hybrid ti√™n ti·∫øn, k·∫øt h·ª£p:

1.1 CORE ALGORITHMS:
- Q-Learning (Reinforcement Learning) - Thu·∫≠t to√°n ch√≠nh
- Minimax + Alpha-Beta Pruning - Thu·∫≠t to√°n fallback
- Decision Tree Classifier - Ph√¢n t√≠ch ng·ªØ c·∫£nh game
- Entropy Analysis - ƒê√°nh gi√° ƒëa d·∫°ng move

1.2 SPECIALIZED SYSTEMS:
- 6 AI Personality Styles v·ªõi training ri√™ng bi·ªát
- Self-Play Training Pipeline t·ª± ƒë·ªông
- Real-time Performance Testing Framework
- Dynamic Algorithm Switching

1.3 TECHNICAL FOUNDATION:
- State Space: 13,118+ unique game states
- Action Space: Movement + Wall placement combinations
- Training Method: Self-play v·ªõi 1000 episodes
- Learning Rate: Œ± = 0.1, Discount Factor: Œ≥ = 0.99

================================================================================
                    2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
================================================================================

2.1 GI·ªöI THI·ªÜU Q-LEARNING:
Q-Learning l√† thu·∫≠t to√°n Reinforcement Learning kh√¥ng c·∫ßn model, h·ªçc th√¥ng qua
t∆∞∆°ng t√°c v·ªõi m√¥i tr∆∞·ªùng ƒë·ªÉ t·ªëi ∆∞u h√≥a policy. Trong Quoridor, AI h·ªçc c√°ch
ch·ªçn moves t·ªëi ∆∞u d·ª±a tr√™n kinh nghi·ªám t·ª´ h√†ng ng√†n game.

2.2 C√îNG TH·ª®C TO√ÅN H·ªåC:
Q(s,a) = Q(s,a) + Œ±[r + Œ≥*max(Q(s',a')) - Q(s,a)]

Trong ƒë√≥:
- Q(s,a): Q-value c·ªßa state s v·ªõi action a
- Œ±: Learning rate (0.1) - t·ªëc ƒë·ªô h·ªçc
- r: Immediate reward t·ª´ action
- Œ≥: Discount factor (0.99) - t·∫ßm quan tr·ªçng future rewards
- s': Next state sau khi th·ª±c hi·ªán action
- max(Q(s',a')): Maximum Q-value c·ªßa next state

2.3 IMPLEMENTATION CHI TI·∫æT:

```csharp
public class QLearningAgent
{
    // Core Q-Table: Dictionary l∆∞u tr·ªØ Q-values
    private Dictionary<string, Dictionary<string, float>> qTable;
    
    // Hyperparameters
    private float learningRate = 0.1f;      // T·ªëc ƒë·ªô h·ªçc
    private float discountFactor = 0.99f;   // Discount future rewards
    private float epsilon = 0.995f;         // Exploration rate
    
    // Q-Learning Update Implementation
    public void UpdateQValue(string state, string action, float reward, string nextState)
    {
        // L·∫•y Q-value hi·ªán t·∫°i
        float currentQ = GetQValue(state, action);
        
        // T√¨m max Q-value c·ªßa next state
        float maxNextQ = GetMaxQValue(nextState);
        
        // √Åp d·ª•ng c√¥ng th·ª©c Q-Learning
        float newQ = currentQ + learningRate * (reward + discountFactor * maxNextQ - currentQ);
        
        // C·∫≠p nh·∫≠t Q-table
        SetQValue(state, action, newQ);
    }
}
```

2.4 STATE ENCODING:
Game state ƒë∆∞·ª£c encode th√†nh string duy nh·∫•t:
"aiPos_x,aiPos_y-humanPos_x,humanPos_y-aiWalls-humanWalls-wallsList"

V√≠ d·ª•: "4,8-4,0-9-10-7,4,False|5,6,True"
- AI ·ªü (4,8), Human ·ªü (4,0)
- AI c√≥ 9 walls, Human c√≥ 10 walls
- Walls ƒë√£ ƒë·∫∑t: (7,4,False), (5,6,True)

2.5 ACTION ENCODING:
Actions ƒë∆∞·ª£c encode th√†nh string:
- Movement: "M:x,y" (v√≠ d·ª•: "M:4,7")
- Wall: "W:x,y,horizontal" (v√≠ d·ª•: "W:3,5,True")

2.6 EPSILON-GREEDY STRATEGY:
```csharp
public AIMove ChooseAction(GameState state, List<AIMove> possibleMoves)
{
    // Exploration vs Exploitation trade-off
    if (Random.Range(0f, 1f) < epsilon)
    {
        // EXPLORATION: Ch·ªçn random ƒë·ªÉ kh√°m ph√°
        return possibleMoves[Random.Range(0, possibleMoves.Count)];
    }
    else
    {
        // EXPLOITATION: Ch·ªçn action c√≥ Q-value cao nh·∫•t
        return GetBestAction(state, possibleMoves);
    }
}
```

2.7 EPSILON DECAY:
Epsilon gi·∫£m d·∫ßn t·ª´ 0.995 ‚Üí 0.1 qua training:
- Ban ƒë·∫ßu: High exploration (99.5%)
- Cu·ªëi training: High exploitation (90%)

2.8 REWARD SYSTEM:
- Win game: +1000
- Lose game: -1000
- Move toward goal: +10
- Block opponent: +50
- Waste move: -5
- Strategic position: +15

================================================================================
                3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
================================================================================

3.1 GI·ªöI THI·ªÜU MINIMAX:
Minimax l√† thu·∫≠t to√°n game theory c·ªï ƒëi·ªÉn, t√¨m ki·∫øm move t·ªëi ∆∞u b·∫±ng c√°ch
d·ª± ƒëo√°n tr∆∞·ªõc nhi·ªÅu moves. Alpha-Beta pruning t·ªëi ∆∞u hi·ªáu su·∫•t b·∫±ng c√°ch
lo·∫°i b·ªè c√°c nh√°nh kh√¥ng c·∫ßn thi·∫øt.

3.2 NGUY√äN L√ù HO·∫†T ƒê·ªòNG:
- Maximizing Player (AI): T·ªëi ƒëa h√≥a score
- Minimizing Player (Human): T·ªëi thi·ªÉu h√≥a score AI
- Search depth: 4 levels (configurable)
- Pruning: Lo·∫°i b·ªè nh√°nh kh√¥ng promising

3.3 IMPLEMENTATION:

```csharp
float Minimax(GameState state, int depth, bool isMaximizing, float alpha, float beta)
{
    // Base case: Depth = 0 ho·∫∑c game over
    if (depth == 0 || IsGameOver(state))
        return EvaluatePosition(state);
    
    if (isMaximizing) // AI turn
    {
        float maxEval = float.NegativeInfinity;
        foreach (var move in GeneratePossibleMoves(state, true))
        {
            GameState newState = ApplyMove(state, move, true);
            float eval = Minimax(newState, depth - 1, false, alpha, beta);
            maxEval = Mathf.Max(maxEval, eval);
            alpha = Mathf.Max(alpha, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return maxEval;
    }
    else // Human turn
    {
        float minEval = float.PositiveInfinity;
        foreach (var move in GeneratePossibleMoves(state, false))
        {
            GameState newState = ApplyMove(state, move, false);
            float eval = Minimax(newState, depth - 1, true, alpha, beta);
            minEval = Mathf.Min(minEval, eval);
            beta = Mathf.Min(beta, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return minEval;
    }
}
```

3.4 EVALUATION FUNCTION:
Position evaluation d·ª±a tr√™n multiple factors:

```csharp
float EvaluatePosition(GameState state)
{
    float score = 0f;
    
    // Distance to goal (c√†ng g·∫ßn c√†ng t·ªët)
    int aiDistance = GetShortestPath(state.aiPosition, goalRow);
    int humanDistance = GetShortestPath(state.humanPosition, goalRow);
    score += (humanDistance - aiDistance) * 100;
    
    // Wall advantage
    score += (state.aiWallsLeft - state.humanWallsLeft) * 50;
    
    // Mobility (s·ªë moves c√≥ th·ªÉ)
    int aiMobility = GetValidMoves(state.aiPosition).Count;
    int humanMobility = GetValidMoves(state.humanPosition).Count;
    score += (aiMobility - humanMobility) * 20;
    
    // Center control
    score += GetCenterControlScore(state) * 30;
    
    return score;
}
```

3.5 ALPHA-BETA PRUNING HI·ªÜU QU·∫¢:
- Reduces search space t·ª´ O(b^d) xu·ªëng O(b^(d/2))
- Best case: 50% reduction in nodes evaluated
- Ordering moves by promising gi√∫p pruning hi·ªáu qu·∫£ h∆°n

================================================================================
                        4. SPECIALIZED AI STYLES SYSTEM
================================================================================

4.1 CONCEPT:
6 AI personalities v·ªõi training v√† behavior ri√™ng bi·ªát, m·ªói style c√≥:
- Ri√™ng Q-table file
- Custom reward structure
- Specific move prioritization
- Unique decision making logic

4.2 AI STYLES CHI TI·∫æT:

üî• AGGRESSIVE STYLE:
- M·ª•c ti√™u: T·∫•n c√¥ng v√† ch·∫∑n ƒë·ªëi th·ªß
- Wall Usage: 70% chance
- Reward Structure:
  * blockingOpponentReward = 50 * intensity
  * wallPlacementReward = 30 * intensity
  * defensiveWallPenalty = -20 * intensity
- Q-table: qtable_aggressive.json

üõ°Ô∏è DEFENSIVE STYLE:
- M·ª•c ti√™u: Ph√≤ng th·ªß v√† di chuy·ªÉn an to√†n
- Wall Usage: 50% chance
- Reward Structure:
  * defensiveWallReward = 35 * intensity
  * movementReward = 20 * intensity
  * aggressiveMovePenalty = -15 * intensity
- Q-table: qtable_defensive.json

üß± WALL MASTER STYLE:
- M·ª•c ti√™u: Chuy√™n gia ƒë·∫∑t t∆∞·ªùng v√† t·∫°o m√™ cung
- Wall Usage: 90% chance
- Reward Structure:
  * wallPlacementReward = 60 * intensity
  * strategicWallReward = 40 * intensity
  * wallComboReward = 50 * intensity
  * movementWithoutWallPenalty = -25 * intensity
- Q-table: qtable_wallmaster.json

‚ö° SPEED RUNNER STYLE:
- M·ª•c ti√™u: ƒê·∫°t ƒë√≠ch nhanh nh·∫•t
- Wall Usage: 10% chance
- Reward Structure:
  * movementReward = 40 * intensity
  * progressToGoalReward = 50 * intensity
  * quickWinBonus = 100 * intensity
  * wallPlacementPenalty = -20 * intensity
- Q-table: qtable_speedrunner.json

üß† TACTICAL STYLE:
- M·ª•c ti√™u: L·ª£i th·∫ø v·ªã tr√≠ v√† k·∫ø ho·∫°ch d√†i h·∫°n
- Wall Usage: Variable (strategic)
- Reward Structure:
  * tacticalMoveReward = 45 * intensity
  * positionAdvantageReward = 35 * intensity
  * mobilityReward = 25 * intensity
  * patternRecognitionBonus = 40 * intensity
- Q-table: qtable_tactical.json

‚öñÔ∏è BALANCED STYLE:
- M·ª•c ti√™u: C√¢n b·∫±ng t·∫•t c·∫£ aspects
- Wall Usage: 30% chance
- Reward Structure: Default balanced values
- Q-table: qtable.json

4.3 STYLE SWITCHING MECHANISM:

```csharp
public void SwitchToStyle(PlayStyle newStyle)
{
    // Save current Q-table
    if (allowQTableSaving) qAgent.SaveQTable(GetStyleSpecificQTablePath());
    
    // Load new style Q-table
    specializedStyle = newStyle;
    qAgent.LoadQTable(GetStyleSpecificQTablePath());
    
    // Reconfigure parameters
    ConfigureStyleSpecificTraining();
}
```

4.4 STYLE INTENSITY:
Parameter t·ª´ 0.0 ‚Üí 1.0 control m·ª©c ƒë·ªô chuy√™n h√≥a:
- 0.0: Balanced behavior
- 0.5: Moderate specialization
- 1.0: Extreme specialization

================================================================================
                        5. DECISION TREE CLASSIFIER
================================================================================

5.1 PURPOSE:
Decision Tree ph√¢n t√≠ch game context ƒë·ªÉ ch·ªçn strategy ph√π h·ª£p, d·ª±a tr√™n:
- Game phase (Early/Mid/End)
- Position advantage
- Resource advantage
- Mobility situation

5.2 FEATURE EXTRACTION:

```csharp
public struct DecisionFeatures
{
    public int aiDistanceToGoal;      // AI distance to goal line
    public int humanDistanceToGoal;   // Human distance to goal line
    public int aiWallsLeft;           // AI walls remaining
    public int humanWallsLeft;        // Human walls remaining
    public int aiMobility;            // AI possible moves
    public int humanMobility;         // Human possible moves
    public float distanceAdvantage;   // Human - AI distance
    public float wallAdvantage;       // AI - Human walls
    public bool isEarlyGame;          // > 15 moves estimated remaining
    public bool isMidGame;            // 5-15 moves remaining
    public bool isEndGame;            // < 5 moves remaining
}
```

5.3 DECISION LOGIC:

```csharp
public AIStrategy Predict(DecisionFeatures features)
{
    // Early Game: Play conservatively
    if (features.isEarlyGame)
        return AIStrategy.Balanced;
    
    // AI has significant lead: Focus on blocking
    if (features.distanceAdvantage > 2)
        return AIStrategy.Blocking;
    
    // AI is behind: Be aggressive
    if (features.distanceAdvantage < -1)
        return AIStrategy.Aggressive;
    
    // End game with wall advantage: Play defensively
    if (features.isEndGame && features.wallAdvantage > 2)
        return AIStrategy.Defensive;
    
    // Close game: Balanced approach
    return AIStrategy.Balanced;
}
```

5.4 STRATEGY OUTCOMES:
- Balanced: Equal weight to movement v√† walls
- Blocking: Prioritize moves that increase opponent path
- Aggressive: Focus on attacking positions
- Defensive: Protect own position and path

================================================================================
                        6. ENTROPY ANALYSIS & SOFTMAX
================================================================================

6.1 ENTROPY CONCEPT:
Entropy ƒëo ƒë·ªô ƒëa d·∫°ng/uncertainty c·ªßa move scores. High entropy = nhi·ªÅu moves
c√≥ scores t∆∞∆°ng ƒë∆∞∆°ng, low entropy = m·ªôt move r√µ r√†ng t·ªët nh·∫•t.

6.2 ENTROPY CALCULATION:

```csharp
float CalculateEntropy(List<float> moveScores)
{
    // Normalize th√†nh probabilities
    float total = moveScores.Sum();
    var probabilities = moveScores.Select(s => s / total);
    
    // Calculate Shannon entropy
    float entropy = 0f;
    foreach (float p in probabilities)
    {
        if (p > 0f)
            entropy -= p * Mathf.Log(p, 2f);
    }
    
    return entropy; // Range: 0 (deterministic) to log2(n) (uniform)
}
```

6.3 SOFTMAX DISTRIBUTION:

```csharp
float[] CalculateSoftmax(List<float> scores, float temperature)
{
    float[] probabilities = new float[scores.Count];
    float sum = scores.Sum(s => Mathf.Exp(s / temperature));
    
    for (int i = 0; i < scores.Count; i++)
    {
        probabilities[i] = Mathf.Exp(scores[i] / temperature) / sum;
    }
    
    return probabilities;
}
```

6.4 TEMPERATURE PARAMETER:
- Low temperature (0.1): Sharp distribution, focus on best moves
- High temperature (2.0): Flat distribution, more exploration
- Default temperature (1.0): Balanced exploitation/exploration

6.5 ADAPTIVE MOVE SELECTION:

```csharp
public AIMove ChooseActionWithEntropy(GameState state, List<AIMove> possibleMoves)
{
    var moveScores = EvaluateAllMoves(state, possibleMoves);
    float entropy = CalculateEntropy(moveScores);
    
    if (entropy > entropyThreshold) // High diversity
    {
        // Use softmax for probabilistic selection
        var probabilities = CalculateSoftmax(moveScores, softmaxTemperature);
        return SelectMoveByProbability(possibleMoves, probabilities);
    }
    else // Clear best move
    {
        // Use deterministic best move
        return GetBestScoredMove(possibleMoves, moveScores);
    }
}
```

================================================================================
                        7. TRAINING SYSTEM & SELF-PLAY
================================================================================

7.1 SELF-PLAY CONCEPT:
AI h·ªçc b·∫±ng c√°ch ch∆°i v·ªõi ch√≠nh n√≥ h√†ng ng√†n l·∫ßn, gradually improve performance
th√¥ng qua trial v√† error.

7.2 TRAINING PIPELINE:

```csharp
IEnumerator RunSelfPlayTraining()
{
    for (int episode = 1; episode <= maxEpisodes; episode++)
    {
        // Reset game state
        ResetGameForTraining();
        
        while (!IsGameOver())
        {
            // Get current state
            GameState currentState = GetCurrentGameState();
            List<AIMove> possibleMoves = GetPossibleMoves();
            
            // Choose action using Œµ-greedy
            AIMove chosenMove = qLearningAgent.ChooseAction(currentState, possibleMoves);
            
            // Execute move
            ExecuteMove(chosenMove);
            
            // Calculate reward
            float reward = CalculateReward(currentState, chosenMove);
            
            // Update Q-table
            GameState newState = GetCurrentGameState();
            qLearningAgent.UpdateQValue(
                EncodeState(currentState),
                EncodeAction(chosenMove),
                reward,
                EncodeState(newState)
            );
            
            yield return new WaitForEndOfFrame();
        }
        
        // Epsilon decay
        qLearningAgent.DecayEpsilon();
        
        // Save progress periodically
        if (episode % 100 == 0)
        {
            qLearningAgent.SaveQTable();
        }
    }
}
```

7.3 REWARD ENGINEERING:
Critical cho Q-Learning success:

```csharp
float CalculateReward(GameState state, AIMove move)
{
    float reward = 0f;
    
    // Game outcome rewards
    if (IsWin(state)) reward += 1000f;
    if (IsLoss(state)) reward -= 1000f;
    
    // Progress rewards
    if (move.moveType == MoveType.Movement)
    {
        if (IsMoveTowardGoal(move)) reward += 20f;
        else reward -= 5f;
    }
    
    // Strategic rewards
    if (move.moveType == MoveType.WallPlacement)
    {
        if (IsWallBlockingOpponent(move)) reward += 50f;
        if (IsWallStrategic(move)) reward += 30f;
        if (IsWallWasted(move)) reward -= 25f;
    }
    
    // Style-specific rewards
    reward += GetStyleSpecificReward(state, move);
    
    return reward;
}
```

7.4 TRAINING METRICS:
- Episodes: 1000 standard
- Duration: ~15 minutes
- Convergence: Usually around episode 800
- Q-table size: 13,118+ states at completion
- Final epsilon: 0.1 (90% exploitation)

================================================================================
                        8. AI TESTING FRAMEWORK
================================================================================

8.1 TESTING PHILOSOPHY:
Comprehensive testing ensures AI quality v√† reliability across different
game scenarios v√† edge cases.

8.2 TEST CATEGORIES:

A) DECISION TREE TESTS:
```csharp
void TestDecisionTreeStrategies()
{
    // Test Early Game
    var earlyGameFeatures = new DecisionFeatures
    {
        aiDistanceToGoal = 8, humanDistanceToGoal = 8,
        isEarlyGame = true
    };
    var strategy = decisionTree.Predict(earlyGameFeatures);
    Assert.AreEqual(AIStrategy.Balanced, strategy);
    
    // Test AI Leading
    var leadingFeatures = new DecisionFeatures
    {
        distanceAdvantage = 3, // AI leading
        isMidGame = true
    };
    strategy = decisionTree.Predict(leadingFeatures);
    Assert.AreEqual(AIStrategy.Blocking, strategy);
}
```

B) ENTROPY ANALYSIS TESTS:
```csharp
void TestEntropyAnalysis()
{
    // High entropy case (diverse scores)
    var highEntropyScores = new List<float> { 100f, 98f, 97f, 95f, 94f };
    float entropy = CalculateTestEntropy(highEntropyScores);
    Assert.IsTrue(entropy > 0.5f);
    
    // Low entropy case (clear winner)
    var lowEntropyScores = new List<float> { 100f, 50f, 50f, 50f, 50f };
    entropy = CalculateTestEntropy(lowEntropyScores);
    Assert.IsTrue(entropy < 0.5f);
}
```

C) SOFTMAX DISTRIBUTION TESTS:
```csharp
void TestSoftmaxDistribution()
{
    var testScores = new List<float> { 100f, 98f, 95f, 90f };
    float temperature = 1.0f;
    
    var probabilities = CalculateSoftmax(testScores, temperature);
    
    // Verify probabilities sum to 1
    float sum = probabilities.Sum();
    Assert.AreEqual(1.0f, sum, 0.001f);
    
    // Verify highest score gets highest probability
    int maxIndex = Array.IndexOf(probabilities, probabilities.Max());
    Assert.AreEqual(0, maxIndex);
}
```

D) PERFORMANCE BENCHMARKS:
- Move generation time: <100ms
- Q-Learning decision: <50ms
- Minimax decision (depth 4): <200ms
- Memory usage: <256MB

8.3 AUTOMATED TESTING:
Tests run automatically ƒë·ªÉ ensure code quality:
- Unit tests cho individual algorithms
- Integration tests cho AI system interactions
- Performance tests cho response times
- Regression tests ƒë·ªÉ prevent quality degradation

================================================================================
                        9. HYBRID AI ARCHITECTURE
================================================================================

9.1 CONCEPT:
Hybrid system k·∫øt h·ª£p strengths c·ªßa multiple algorithms:
- Q-Learning: Data-driven, learns from experience
- Minimax: Logical, guaranteed optimal given evaluation
- Decision Tree: Fast, context-aware switching
- Entropy: Adaptive, handles uncertainty

9.2 ALGORITHM SELECTION LOGIC:

```csharp
public AIMove GetBestMove()
{
    UpdateGameState();
    List<AIMove> possibleMoves = GeneratePossibleMoves();
    
    // Primary: Use Q-Learning if trained model available
    if (useQLearning && qAgent.IsTrainedModel())
    {
        Debug.Log($"üß† Using {specializedStyle} Q-Learning");
        return qAgent.ChooseAction(currentGameState, possibleMoves);
    }
    
    // Fallback: Use Minimax for guaranteed reasonable play
    else
    {
        Debug.Log($"üîç Using {specializedStyle} Minimax");
        return FindBestMoveWithMinimax();
    }
}
```

9.3 SEAMLESS SWITCHING:
- No interruption in gameplay
- Maintains consistent API
- Preserves game state integrity
- Logs algorithm used for debugging

9.4 BENEFITS:
- Robustness: Fallback ensures AI never fails
- Performance: Q-Learning for trained scenarios, Minimax for edge cases
- Flexibility: Can switch algorithms based on situation
- Development: Easy to test individual components

================================================================================
                        10. PERFORMANCE OPTIMIZATION
================================================================================

10.1 Q-TABLE OPTIMIZATION:
- Sparse representation: Only store visited states
- Compression: String interning for state keys
- Lazy loading: Load Q-table on demand
- Periodic saving: Every 100 training episodes

10.2 MINIMAX OPTIMIZATION:
- Alpha-Beta pruning: 50% search reduction
- Move ordering: Promising moves first
- Iterative deepening: Gradual depth increase
- Transposition tables: Cache evaluated positions

10.3 MEMORY MANAGEMENT:
- Object pooling cho GameState objects
- String caching cho repeated state/action encodings
- Garbage collection optimization
- Memory profiling v√† monitoring

10.4 MULTITHREADING CONSIDERATIONS:
- Q-Learning training: Background thread
- Move calculation: Main thread (Unity requirement)
- File I/O: Async operations where possible
- Thread-safe collections cho shared data

10.5 CACHING STRATEGIES:
- Path finding results
- Move generation results
- Evaluation function results
- Feature extraction results

================================================================================
                            K·∫æT LU·∫¨N
================================================================================

D·ª± √°n AI Quoridor Project th·ªÉ hi·ªán m·ªôt implementation comprehensive c·ªßa modern
game AI techniques:

TECHNICAL INNOVATIONS:
‚úÖ Hybrid Q-Learning + Minimax architecture
‚úÖ 6 specialized AI personalities v·ªõi separate training
‚úÖ Advanced decision making v·ªõi entropy analysis
‚úÖ Comprehensive testing v√† validation framework
‚úÖ Professional development tools v√† debugging

EDUCATIONAL VALUE:
‚úÖ Practical machine learning implementation
‚úÖ Traditional game AI algorithms
‚úÖ Modern software engineering practices
‚úÖ Performance optimization techniques
‚úÖ Automated testing methodologies

FUTURE ENHANCEMENTS:
üöÄ Deep Q-Learning (DQN) integration
üöÄ Multi-agent reinforcement learning
üöÄ Neural network-based evaluation functions
üöÄ Online learning v√† adaptation
üöÄ Transfer learning between game variants

D·ª± √°n n√†y demonstrating successful integration c·ªßa classical AI methods v·ªõi
modern machine learning, t·∫°o ra m·ªôt intelligent, adaptive, v√† highly
interactive gaming experience.

================================================================================
                            APPENDIX
================================================================================

FILES LI√äN QUAN:
- Assets/scripts/QLearningAgent.cs - Q-Learning implementation
- Assets/scripts/QuoridorAI.cs - Main AI controller
- Assets/scripts/DecisionTreeClassifier.cs - Decision tree logic
- Assets/scripts/QuoridorSelfPlayTrainer.cs - Training system
- Assets/scripts/AITestScript.cs - Testing framework
- Assets/scripts/Editor/ForceRetrainAI.cs - Development tools

Q-TABLE FILES:
- StreamingAssets/qtable.json - Balanced style
- StreamingAssets/qtable_aggressive.json - Aggressive style
- StreamingAssets/qtable_defensive.json - Defensive style
- StreamingAssets/qtable_wallmaster.json - Wall Master style
- StreamingAssets/qtable_speedrunner.json - Speed Runner style
- StreamingAssets/qtable_tactical.json - Tactical style

DOCUMENTATION:
- AI_ALGORITHMS_DETAILED_DESCRIPTION.txt - This file
- README.md - Project overview
- API_INTEGRATION_GUIDE.md - Voice/Chat integration
- TRAINING_GUIDE.md - AI training instructions

================================================================================
================================================================================
                    AI QUORIDOR GAME PROJECT - THU·∫¨T TO√ÅN AI CHI TI·∫æT
================================================================================

D·ª± √°n: AI Quoridor Game v·ªõi Machine Learning v√† Multi-Platform Integration
Ng√†y t·∫°o: 25/07/2025
T√°c gi·∫£: [Student Name]
Platform: Unity 3D
Language: C#

================================================================================
                            M·ª§C L·ª§C
================================================================================

1. T·ªîNG QUAN KI·∫æN TR√öC AI
2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
4. SPECIALIZED AI STYLES SYSTEM
5. DECISION TREE CLASSIFIER
6. ENTROPY ANALYSIS & SOFTMAX
7. TRAINING SYSTEM & SELF-PLAY
8. AI TESTING FRAMEWORK
9. HYBRID AI ARCHITECTURE
10. PERFORMANCE OPTIMIZATION

================================================================================
                        1. T·ªîNG QUAN KI·∫æN TR√öC AI
================================================================================

D·ª± √°n AI Quoridor s·ª≠ d·ª•ng m·ªôt ki·∫øn tr√∫c AI hybrid ti√™n ti·∫øn, k·∫øt h·ª£p:

1.1 CORE ALGORITHMS:
- Q-Learning (Reinforcement Learning) - Thu·∫≠t to√°n ch√≠nh
- Minimax + Alpha-Beta Pruning - Thu·∫≠t to√°n fallback
- Decision Tree Classifier - Ph√¢n t√≠ch ng·ªØ c·∫£nh game
- Entropy Analysis - ƒê√°nh gi√° ƒëa d·∫°ng move

1.2 SPECIALIZED SYSTEMS:
- 6 AI Personality Styles v·ªõi training ri√™ng bi·ªát
- Self-Play Training Pipeline t·ª± ƒë·ªông
- Real-time Performance Testing Framework
- Dynamic Algorithm Switching

1.3 TECHNICAL FOUNDATION:
- State Space: 13,118+ unique game states
- Action Space: Movement + Wall placement combinations
- Training Method: Self-play v·ªõi 1000 episodes
- Learning Rate: Œ± = 0.1, Discount Factor: Œ≥ = 0.99

================================================================================
                    2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
================================================================================

2.1 GI·ªöI THI·ªÜU Q-LEARNING:
Q-Learning l√† thu·∫≠t to√°n Reinforcement Learning kh√¥ng c·∫ßn model, h·ªçc th√¥ng qua
t∆∞∆°ng t√°c v·ªõi m√¥i tr∆∞·ªùng ƒë·ªÉ t·ªëi ∆∞u h√≥a policy. Trong Quoridor, AI h·ªçc c√°ch
ch·ªçn moves t·ªëi ∆∞u d·ª±a tr√™n kinh nghi·ªám t·ª´ h√†ng ng√†n game.

2.2 C√îNG TH·ª®C TO√ÅN H·ªåC:
Q(s,a) = Q(s,a) + Œ±[r + Œ≥*max(Q(s',a')) - Q(s,a)]

Trong ƒë√≥:
- Q(s,a): Q-value c·ªßa state s v·ªõi action a
- Œ±: Learning rate (0.1) - t·ªëc ƒë·ªô h·ªçc
- r: Immediate reward t·ª´ action
- Œ≥: Discount factor (0.99) - t·∫ßm quan tr·ªçng future rewards
- s': Next state sau khi th·ª±c hi·ªán action
- max(Q(s',a')): Maximum Q-value c·ªßa next state

2.3 IMPLEMENTATION CHI TI·∫æT:

```csharp
public class QLearningAgent
{
    // Core Q-Table: Dictionary l∆∞u tr·ªØ Q-values
    private Dictionary<string, Dictionary<string, float>> qTable;
    
    // Hyperparameters
    private float learningRate = 0.1f;      // T·ªëc ƒë·ªô h·ªçc
    private float discountFactor = 0.99f;   // Discount future rewards
    private float epsilon = 0.995f;         // Exploration rate
    
    // Q-Learning Update Implementation
    public void UpdateQValue(string state, string action, float reward, string nextState)
    {
        // L·∫•y Q-value hi·ªán t·∫°i
        float currentQ = GetQValue(state, action);
        
        // T√¨m max Q-value c·ªßa next state
        float maxNextQ = GetMaxQValue(nextState);
        
        // √Åp d·ª•ng c√¥ng th·ª©c Q-Learning
        float newQ = currentQ + learningRate * (reward + discountFactor * maxNextQ - currentQ);
        
        // C·∫≠p nh·∫≠t Q-table
        SetQValue(state, action, newQ);
    }
}
```

2.4 STATE ENCODING:
Game state ƒë∆∞·ª£c encode th√†nh string duy nh·∫•t:
"aiPos_x,aiPos_y-humanPos_x,humanPos_y-aiWalls-humanWalls-wallsList"

V√≠ d·ª•: "4,8-4,0-9-10-7,4,False|5,6,True"
- AI ·ªü (4,8), Human ·ªü (4,0)
- AI c√≥ 9 walls, Human c√≥ 10 walls
- Walls ƒë√£ ƒë·∫∑t: (7,4,False), (5,6,True)

2.5 ACTION ENCODING:
Actions ƒë∆∞·ª£c encode th√†nh string:
- Movement: "M:x,y" (v√≠ d·ª•: "M:4,7")
- Wall: "W:x,y,horizontal" (v√≠ d·ª•: "W:3,5,True")

2.6 EPSILON-GREEDY STRATEGY:
```csharp
public AIMove ChooseAction(GameState state, List<AIMove> possibleMoves)
{
    // Exploration vs Exploitation trade-off
    if (Random.Range(0f, 1f) < epsilon)
    {
        // EXPLORATION: Ch·ªçn random ƒë·ªÉ kh√°m ph√°
        return possibleMoves[Random.Range(0, possibleMoves.Count)];
    }
    else
    {
        // EXPLOITATION: Ch·ªçn action c√≥ Q-value cao nh·∫•t
        return GetBestAction(state, possibleMoves);
    }
}
```

2.7 EPSILON DECAY:
Epsilon gi·∫£m d·∫ßn t·ª´ 0.995 ‚Üí 0.1 qua training:
- Ban ƒë·∫ßu: High exploration (99.5%)
- Cu·ªëi training: High exploitation (90%)

2.8 REWARD SYSTEM:
- Win game: +1000
- Lose game: -1000
- Move toward goal: +10
- Block opponent: +50
- Waste move: -5
- Strategic position: +15

================================================================================
                3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
================================================================================

3.1 GI·ªöI THI·ªÜU MINIMAX:
Minimax l√† thu·∫≠t to√°n game theory c·ªï ƒëi·ªÉn, t√¨m ki·∫øm move t·ªëi ∆∞u b·∫±ng c√°ch
d·ª± ƒëo√°n tr∆∞·ªõc nhi·ªÅu moves. Alpha-Beta pruning t·ªëi ∆∞u hi·ªáu su·∫•t b·∫±ng c√°ch
lo·∫°i b·ªè c√°c nh√°nh kh√¥ng c·∫ßn thi·∫øt.

3.2 NGUY√äN L√ù HO·∫†T ƒê·ªòNG:
- Maximizing Player (AI): T·ªëi ƒëa h√≥a score
- Minimizing Player (Human): T·ªëi thi·ªÉu h√≥a score AI
- Search depth: 4 levels (configurable)
- Pruning: Lo·∫°i b·ªè nh√°nh kh√¥ng promising

3.3 IMPLEMENTATION:

```csharp
float Minimax(GameState state, int depth, bool isMaximizing, float alpha, float beta)
{
    // Base case: Depth = 0 ho·∫∑c game over
    if (depth == 0 || IsGameOver(state))
        return EvaluatePosition(state);
    
    if (isMaximizing) // AI turn
    {
        float maxEval = float.NegativeInfinity;
        foreach (var move in GeneratePossibleMoves(state, true))
        {
            GameState newState = ApplyMove(state, move, true);
            float eval = Minimax(newState, depth - 1, false, alpha, beta);
            maxEval = Mathf.Max(maxEval, eval);
            alpha = Mathf.Max(alpha, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return maxEval;
    }
    else // Human turn
    {
        float minEval = float.PositiveInfinity;
        foreach (var move in GeneratePossibleMoves(state, false))
        {
            GameState newState = ApplyMove(state, move, false);
            float eval = Minimax(newState, depth - 1, true, alpha, beta);
            minEval = Mathf.Min(minEval, eval);
            beta = Mathf.Min(beta, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return minEval;
    }
}
```

3.4 EVALUATION FUNCTION:
Position evaluation d·ª±a tr√™n multiple factors:

```csharp
float EvaluatePosition(GameState state)
{
    float score = 0f;
    
    // Distance to goal (c√†ng g·∫ßn c√†ng t·ªët)
    int aiDistance = GetShortestPath(state.aiPosition, goalRow);
    int humanDistance = GetShortestPath(state.humanPosition, goalRow);
    score += (humanDistance - aiDistance) * 100;
    
    // Wall advantage
    score += (state.aiWallsLeft - state.humanWallsLeft) * 50;
    
    // Mobility (s·ªë moves c√≥ th·ªÉ)
    int aiMobility = GetValidMoves(state.aiPosition).Count;
    int humanMobility = GetValidMoves(state.humanPosition).Count;
    score += (aiMobility - humanMobility) * 20;
    
    // Center control
    score += GetCenterControlScore(state) * 30;
    
    return score;
}
```

3.5 ALPHA-BETA PRUNING HI·ªÜU QU·∫¢:
- Reduces search space t·ª´ O(b^d) xu·ªëng O(b^(d/2))
- Best case: 50% reduction in nodes evaluated
- Ordering moves by promising gi√∫p pruning hi·ªáu qu·∫£ h∆°n

================================================================================
                        4. SPECIALIZED AI STYLES SYSTEM
================================================================================

4.1 CONCEPT:
6 AI personalities v·ªõi training v√† behavior ri√™ng bi·ªát, m·ªói style c√≥:
- Ri√™ng Q-table file
- Custom reward structure
- Specific move prioritization
- Unique decision making logic

4.2 AI STYLES CHI TI·∫æT:

üî• AGGRESSIVE STYLE:
- M·ª•c ti√™u: T·∫•n c√¥ng v√† ch·∫∑n ƒë·ªëi th·ªß
- Wall Usage: 70% chance
- Reward Structure:
  * blockingOpponentReward = 50 * intensity
  * wallPlacementReward = 30 * intensity
  * defensiveWallPenalty = -20 * intensity
- Q-table: qtable_aggressive.json

üõ°Ô∏è DEFENSIVE STYLE:
- M·ª•c ti√™u: Ph√≤ng th·ªß v√† di chuy·ªÉn an to√†n
- Wall Usage: 50% chance
- Reward Structure:
  * defensiveWallReward = 35 * intensity
  * movementReward = 20 * intensity
  * aggressiveMovePenalty = -15 * intensity
- Q-table: qtable_defensive.json

üß± WALL MASTER STYLE:
- M·ª•c ti√™u: Chuy√™n gia ƒë·∫∑t t∆∞·ªùng v√† t·∫°o m√™ cung
- Wall Usage: 90% chance
- Reward Structure:
  * wallPlacementReward = 60 * intensity
  * strategicWallReward = 40 * intensity
  * wallComboReward = 50 * intensity
  * movementWithoutWallPenalty = -25 * intensity
- Q-table: qtable_wallmaster.json

‚ö° SPEED RUNNER STYLE:
- M·ª•c ti√™u: ƒê·∫°t ƒë√≠ch nhanh nh·∫•t
- Wall Usage: 10% chance
- Reward Structure:
  * movementReward = 40 * intensity
  * progressToGoalReward = 50 * intensity
  * quickWinBonus = 100 * intensity
  * wallPlacementPenalty = -20 * intensity
- Q-table: qtable_speedrunner.json

üß† TACTICAL STYLE:
- M·ª•c ti√™u: L·ª£i th·∫ø v·ªã tr√≠ v√† k·∫ø ho·∫°ch d√†i h·∫°n
- Wall Usage: Variable (strategic)
- Reward Structure:
  * tacticalMoveReward = 45 * intensity
  * positionAdvantageReward = 35 * intensity
  * mobilityReward = 25 * intensity
  * patternRecognitionBonus = 40 * intensity
- Q-table: qtable_tactical.json

‚öñÔ∏è BALANCED STYLE:
- M·ª•c ti√™u: C√¢n b·∫±ng t·∫•t c·∫£ aspects
- Wall Usage: 30% chance
- Reward Structure: Default balanced values
- Q-table: qtable.json

4.3 STYLE SWITCHING MECHANISM:

```csharp
public void SwitchToStyle(PlayStyle newStyle)
{
    // Save current Q-table
    if (allowQTableSaving) qAgent.SaveQTable(GetStyleSpecificQTablePath());
    
    // Load new style Q-table
    specializedStyle = newStyle;
    qAgent.LoadQTable(GetStyleSpecificQTablePath());
    
    // Reconfigure parameters
    ConfigureStyleSpecificTraining();
}
```

4.4 STYLE INTENSITY:
Parameter t·ª´ 0.0 ‚Üí 1.0 control m·ª©c ƒë·ªô chuy√™n h√≥a:
- 0.0: Balanced behavior
- 0.5: Moderate specialization
- 1.0: Extreme specialization

================================================================================
                        5. DECISION TREE CLASSIFIER
================================================================================

5.1 PURPOSE:
Decision Tree ph√¢n t√≠ch game context ƒë·ªÉ ch·ªçn strategy ph√π h·ª£p, d·ª±a tr√™n:
- Game phase (Early/Mid/End)
- Position advantage
- Resource advantage
- Mobility situation

5.2 FEATURE EXTRACTION:

```csharp
public struct DecisionFeatures
{
    public int aiDistanceToGoal;      // AI distance to goal line
    public int humanDistanceToGoal;   // Human distance to goal line
    public int aiWallsLeft;           // AI walls remaining
    public int humanWallsLeft;        // Human walls remaining
    public int aiMobility;            // AI possible moves
    public int humanMobility;         // Human possible moves
    public float distanceAdvantage;   // Human - AI distance
    public float wallAdvantage;       // AI - Human walls
    public bool isEarlyGame;          // > 15 moves estimated remaining
    public bool isMidGame;            // 5-15 moves remaining
    public bool isEndGame;            // < 5 moves remaining
}
```

5.3 DECISION LOGIC:

```csharp
public AIStrategy Predict(DecisionFeatures features)
{
    // Early Game: Play conservatively
    if (features.isEarlyGame)
        return AIStrategy.Balanced;
    
    // AI has significant lead: Focus on blocking
    if (features.distanceAdvantage > 2)
        return AIStrategy.Blocking;
    
    // AI is behind: Be aggressive
    if (features.distanceAdvantage < -1)
        return AIStrategy.Aggressive;
    
    // End game with wall advantage: Play defensively
    if (features.isEndGame && features.wallAdvantage > 2)
        return AIStrategy.Defensive;
    
    // Close game: Balanced approach
    return AIStrategy.Balanced;
}
```

5.4 STRATEGY OUTCOMES:
- Balanced: Equal weight to movement v√† walls
- Blocking: Prioritize moves that increase opponent path
- Aggressive: Focus on attacking positions
- Defensive: Protect own position and path

================================================================================
                        6. ENTROPY ANALYSIS & SOFTMAX
================================================================================

6.1 ENTROPY CONCEPT:
Entropy ƒëo ƒë·ªô ƒëa d·∫°ng/uncertainty c·ªßa move scores. High entropy = nhi·ªÅu moves
c√≥ scores t∆∞∆°ng ƒë∆∞∆°ng, low entropy = m·ªôt move r√µ r√†ng t·ªët nh·∫•t.

6.2 ENTROPY CALCULATION:

```csharp
float CalculateEntropy(List<float> moveScores)
{
    // Normalize th√†nh probabilities
    float total = moveScores.Sum();
    var probabilities = moveScores.Select(s => s / total);
    
    // Calculate Shannon entropy
    float entropy = 0f;
    foreach (float p in probabilities)
    {
        if (p > 0f)
            entropy -= p * Mathf.Log(p, 2f);
    }
    
    return entropy; // Range: 0 (deterministic) to log2(n) (uniform)
}
```

6.3 SOFTMAX DISTRIBUTION:

```csharp
float[] CalculateSoftmax(List<float> scores, float temperature)
{
    float[] probabilities = new float[scores.Count];
    float sum = scores.Sum(s => Mathf.Exp(s / temperature));
    
    for (int i = 0; i < scores.Count; i++)
    {
        probabilities[i] = Mathf.Exp(scores[i] / temperature) / sum;
    }
    
    return probabilities;
}
```

6.4 TEMPERATURE PARAMETER:
- Low temperature (0.1): Sharp distribution, focus on best moves
- High temperature (2.0): Flat distribution, more exploration
- Default temperature (1.0): Balanced exploitation/exploration

6.5 ADAPTIVE MOVE SELECTION:

```csharp
public AIMove ChooseActionWithEntropy(GameState state, List<AIMove> possibleMoves)
{
    var moveScores = EvaluateAllMoves(state, possibleMoves);
    float entropy = CalculateEntropy(moveScores);
    
    if (entropy > entropyThreshold) // High diversity
    {
        // Use softmax for probabilistic selection
        var probabilities = CalculateSoftmax(moveScores, softmaxTemperature);
        return SelectMoveByProbability(possibleMoves, probabilities);
    }
    else // Clear best move
    {
        // Use deterministic best move
        return GetBestScoredMove(possibleMoves, moveScores);
    }
}
```

================================================================================
                        7. TRAINING SYSTEM & SELF-PLAY
================================================================================

7.1 SELF-PLAY CONCEPT:
AI h·ªçc b·∫±ng c√°ch ch∆°i v·ªõi ch√≠nh n√≥ h√†ng ng√†n l·∫ßn, gradually improve performance
th√¥ng qua trial v√† error.

7.2 TRAINING PIPELINE:

```csharp
IEnumerator RunSelfPlayTraining()
{
    for (int episode = 1; episode <= maxEpisodes; episode++)
    {
        // Reset game state
        ResetGameForTraining();
        
        while (!IsGameOver())
        {
            // Get current state
            GameState currentState = GetCurrentGameState();
            List<AIMove> possibleMoves = GetPossibleMoves();
            
            // Choose action using Œµ-greedy
            AIMove chosenMove = qLearningAgent.ChooseAction(currentState, possibleMoves);
            
            // Execute move
            ExecuteMove(chosenMove);
            
            // Calculate reward
            float reward = CalculateReward(currentState, chosenMove);
            
            // Update Q-table
            GameState newState = GetCurrentGameState();
            qLearningAgent.UpdateQValue(
                EncodeState(currentState),
                EncodeAction(chosenMove),
                reward,
                EncodeState(newState)
            );
            
            yield return new WaitForEndOfFrame();
        }
        
        // Epsilon decay
        qLearningAgent.DecayEpsilon();
        
        // Save progress periodically
        if (episode % 100 == 0)
        {
            qLearningAgent.SaveQTable();
        }
    }
}
```

7.3 REWARD ENGINEERING:
Critical cho Q-Learning success:

```csharp
float CalculateReward(GameState state, AIMove move)
{
    float reward = 0f;
    
    // Game outcome rewards
    if (IsWin(state)) reward += 1000f;
    if (IsLoss(state)) reward -= 1000f;
    
    // Progress rewards
    if (move.moveType == MoveType.Movement)
    {
        if (IsMoveTowardGoal(move)) reward += 20f;
        else reward -= 5f;
    }
    
    // Strategic rewards
    if (move.moveType == MoveType.WallPlacement)
    {
        if (IsWallBlockingOpponent(move)) reward += 50f;
        if (IsWallStrategic(move)) reward += 30f;
        if (IsWallWasted(move)) reward -= 25f;
    }
    
    // Style-specific rewards
    reward += GetStyleSpecificReward(state, move);
    
    return reward;
}
```

7.4 TRAINING METRICS:
- Episodes: 1000 standard
- Duration: ~15 minutes
- Convergence: Usually around episode 800
- Q-table size: 13,118+ states at completion
- Final epsilon: 0.1 (90% exploitation)

================================================================================
                        8. AI TESTING FRAMEWORK
================================================================================

8.1 TESTING PHILOSOPHY:
Comprehensive testing ensures AI quality v√† reliability across different
game scenarios v√† edge cases.

8.2 TEST CATEGORIES:

A) DECISION TREE TESTS:
```csharp
void TestDecisionTreeStrategies()
{
    // Test Early Game
    var earlyGameFeatures = new DecisionFeatures
    {
        aiDistanceToGoal = 8, humanDistanceToGoal = 8,
        isEarlyGame = true
    };
    var strategy = decisionTree.Predict(earlyGameFeatures);
    Assert.AreEqual(AIStrategy.Balanced, strategy);
    
    // Test AI Leading
    var leadingFeatures = new DecisionFeatures
    {
        distanceAdvantage = 3, // AI leading
        isMidGame = true
    };
    strategy = decisionTree.Predict(leadingFeatures);
    Assert.AreEqual(AIStrategy.Blocking, strategy);
}
```

B) ENTROPY ANALYSIS TESTS:
```csharp
void TestEntropyAnalysis()
{
    // High entropy case (diverse scores)
    var highEntropyScores = new List<float> { 100f, 98f, 97f, 95f, 94f };
    float entropy = CalculateTestEntropy(highEntropyScores);
    Assert.IsTrue(entropy > 0.5f);
    
    // Low entropy case (clear winner)
    var lowEntropyScores = new List<float> { 100f, 50f, 50f, 50f, 50f };
    entropy = CalculateTestEntropy(lowEntropyScores);
    Assert.IsTrue(entropy < 0.5f);
}
```

C) SOFTMAX DISTRIBUTION TESTS:
```csharp
void TestSoftmaxDistribution()
{
    var testScores = new List<float> { 100f, 98f, 95f, 90f };
    float temperature = 1.0f;
    
    var probabilities = CalculateSoftmax(testScores, temperature);
    
    // Verify probabilities sum to 1
    float sum = probabilities.Sum();
    Assert.AreEqual(1.0f, sum, 0.001f);
    
    // Verify highest score gets highest probability
    int maxIndex = Array.IndexOf(probabilities, probabilities.Max());
    Assert.AreEqual(0, maxIndex);
}
```

D) PERFORMANCE BENCHMARKS:
- Move generation time: <100ms
- Q-Learning decision: <50ms
- Minimax decision (depth 4): <200ms
- Memory usage: <256MB

8.3 AUTOMATED TESTING:
Tests run automatically ƒë·ªÉ ensure code quality:
- Unit tests cho individual algorithms
- Integration tests cho AI system interactions
- Performance tests cho response times
- Regression tests ƒë·ªÉ prevent quality degradation

================================================================================
                        9. HYBRID AI ARCHITECTURE
================================================================================

9.1 CONCEPT:
Hybrid system k·∫øt h·ª£p strengths c·ªßa multiple algorithms:
- Q-Learning: Data-driven, learns from experience
- Minimax: Logical, guaranteed optimal given evaluation
- Decision Tree: Fast, context-aware switching
- Entropy: Adaptive, handles uncertainty

9.2 ALGORITHM SELECTION LOGIC:

```csharp
public AIMove GetBestMove()
{
    UpdateGameState();
    List<AIMove> possibleMoves = GeneratePossibleMoves();
    
    // Primary: Use Q-Learning if trained model available
    if (useQLearning && qAgent.IsTrainedModel())
    {
        Debug.Log($"üß† Using {specializedStyle} Q-Learning");
        return qAgent.ChooseAction(currentGameState, possibleMoves);
    }
    
    // Fallback: Use Minimax for guaranteed reasonable play
    else
    {
        Debug.Log($"üîç Using {specializedStyle} Minimax");
        return FindBestMoveWithMinimax();
    }
}
```

9.3 SEAMLESS SWITCHING:
- No interruption in gameplay
- Maintains consistent API
- Preserves game state integrity
- Logs algorithm used for debugging

9.4 BENEFITS:
- Robustness: Fallback ensures AI never fails
- Performance: Q-Learning for trained scenarios, Minimax for edge cases
- Flexibility: Can switch algorithms based on situation
- Development: Easy to test individual components

================================================================================
                        10. PERFORMANCE OPTIMIZATION
================================================================================

10.1 Q-TABLE OPTIMIZATION:
- Sparse representation: Only store visited states
- Compression: String interning for state keys
- Lazy loading: Load Q-table on demand
- Periodic saving: Every 100 training episodes

10.2 MINIMAX OPTIMIZATION:
- Alpha-Beta pruning: 50% search reduction
- Move ordering: Promising moves first
- Iterative deepening: Gradual depth increase
- Transposition tables: Cache evaluated positions

10.3 MEMORY MANAGEMENT:
- Object pooling cho GameState objects
- String caching cho repeated state/action encodings
- Garbage collection optimization
- Memory profiling v√† monitoring

10.4 MULTITHREADING CONSIDERATIONS:
- Q-Learning training: Background thread
- Move calculation: Main thread (Unity requirement)
- File I/O: Async operations where possible
- Thread-safe collections cho shared data

10.5 CACHING STRATEGIES:
- Path finding results
- Move generation results
- Evaluation function results
- Feature extraction results

================================================================================
                            K·∫æT LU·∫¨N
================================================================================

D·ª± √°n AI Quoridor Project th·ªÉ hi·ªán m·ªôt implementation comprehensive c·ªßa modern
game AI techniques:

TECHNICAL INNOVATIONS:
‚úÖ Hybrid Q-Learning + Minimax architecture
‚úÖ 6 specialized AI personalities v·ªõi separate training
‚úÖ Advanced decision making v·ªõi entropy analysis
‚úÖ Comprehensive testing v√† validation framework
‚úÖ Professional development tools v√† debugging

EDUCATIONAL VALUE:
‚úÖ Practical machine learning implementation
‚úÖ Traditional game AI algorithms
‚úÖ Modern software engineering practices
‚úÖ Performance optimization techniques
‚úÖ Automated testing methodologies

FUTURE ENHANCEMENTS:
üöÄ Deep Q-Learning (DQN) integration
üöÄ Multi-agent reinforcement learning
üöÄ Neural network-based evaluation functions
üöÄ Online learning v√† adaptation
üöÄ Transfer learning between game variants

D·ª± √°n n√†y demonstrating successful integration c·ªßa classical AI methods v·ªõi
modern machine learning, t·∫°o ra m·ªôt intelligent, adaptive, v√† highly
interactive gaming experience.

================================================================================
                            APPENDIX
================================================================================

FILES LI√äN QUAN:
- Assets/scripts/QLearningAgent.cs - Q-Learning implementation
- Assets/scripts/QuoridorAI.cs - Main AI controller
- Assets/scripts/DecisionTreeClassifier.cs - Decision tree logic
- Assets/scripts/QuoridorSelfPlayTrainer.cs - Training system
- Assets/scripts/AITestScript.cs - Testing framework
- Assets/scripts/Editor/ForceRetrainAI.cs - Development tools

Q-TABLE FILES:
- StreamingAssets/qtable.json - Balanced style
- StreamingAssets/qtable_aggressive.json - Aggressive style
- StreamingAssets/qtable_defensive.json - Defensive style
- StreamingAssets/qtable_wallmaster.json - Wall Master style
- StreamingAssets/qtable_speedrunner.json - Speed Runner style
- StreamingAssets/qtable_tactical.json - Tactical style

DOCUMENTATION:
- AI_ALGORITHMS_DETAILED_DESCRIPTION.txt - This file
- README.md - Project overview
- API_INTEGRATION_GUIDE.md - Voice/Chat integration
- TRAINING_GUIDE.md - AI training instructions

================================================================================
================================================================================
                    AI QUORIDOR GAME PROJECT - THU·∫¨T TO√ÅN AI CHI TI·∫æT
================================================================================

D·ª± √°n: AI Quoridor Game v·ªõi Machine Learning v√† Multi-Platform Integration
Ng√†y t·∫°o: 25/07/2025
T√°c gi·∫£: [Student Name]
Platform: Unity 3D
Language: C#

================================================================================
                            M·ª§C L·ª§C
================================================================================

1. T·ªîNG QUAN KI·∫æN TR√öC AI
2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
4. SPECIALIZED AI STYLES SYSTEM
5. DECISION TREE CLASSIFIER
6. ENTROPY ANALYSIS & SOFTMAX
7. TRAINING SYSTEM & SELF-PLAY
8. AI TESTING FRAMEWORK
9. HYBRID AI ARCHITECTURE
10. PERFORMANCE OPTIMIZATION

================================================================================
                        1. T·ªîNG QUAN KI·∫æN TR√öC AI
================================================================================

D·ª± √°n AI Quoridor s·ª≠ d·ª•ng m·ªôt ki·∫øn tr√∫c AI hybrid ti√™n ti·∫øn, k·∫øt h·ª£p:

1.1 CORE ALGORITHMS:
- Q-Learning (Reinforcement Learning) - Thu·∫≠t to√°n ch√≠nh
- Minimax + Alpha-Beta Pruning - Thu·∫≠t to√°n fallback
- Decision Tree Classifier - Ph√¢n t√≠ch ng·ªØ c·∫£nh game
- Entropy Analysis - ƒê√°nh gi√° ƒëa d·∫°ng move

1.2 SPECIALIZED SYSTEMS:
- 6 AI Personality Styles v·ªõi training ri√™ng bi·ªát
- Self-Play Training Pipeline t·ª± ƒë·ªông
- Real-time Performance Testing Framework
- Dynamic Algorithm Switching

1.3 TECHNICAL FOUNDATION:
- State Space: 13,118+ unique game states
- Action Space: Movement + Wall placement combinations
- Training Method: Self-play v·ªõi 1000 episodes
- Learning Rate: Œ± = 0.1, Discount Factor: Œ≥ = 0.99

================================================================================
                    2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
================================================================================

2.1 GI·ªöI THI·ªÜU Q-LEARNING:
Q-Learning l√† thu·∫≠t to√°n Reinforcement Learning kh√¥ng c·∫ßn model, h·ªçc th√¥ng qua
t∆∞∆°ng t√°c v·ªõi m√¥i tr∆∞·ªùng ƒë·ªÉ t·ªëi ∆∞u h√≥a policy. Trong Quoridor, AI h·ªçc c√°ch
ch·ªçn moves t·ªëi ∆∞u d·ª±a tr√™n kinh nghi·ªám t·ª´ h√†ng ng√†n game.

2.2 C√îNG TH·ª®C TO√ÅN H·ªåC:
Q(s,a) = Q(s,a) + Œ±[r + Œ≥*max(Q(s',a')) - Q(s,a)]

Trong ƒë√≥:
- Q(s,a): Q-value c·ªßa state s v·ªõi action a
- Œ±: Learning rate (0.1) - t·ªëc ƒë·ªô h·ªçc
- r: Immediate reward t·ª´ action
- Œ≥: Discount factor (0.99) - t·∫ßm quan tr·ªçng future rewards
- s': Next state sau khi th·ª±c hi·ªán action
- max(Q(s',a')): Maximum Q-value c·ªßa next state

2.3 IMPLEMENTATION CHI TI·∫æT:

```csharp
public class QLearningAgent
{
    // Core Q-Table: Dictionary l∆∞u tr·ªØ Q-values
    private Dictionary<string, Dictionary<string, float>> qTable;
    
    // Hyperparameters
    private float learningRate = 0.1f;      // T·ªëc ƒë·ªô h·ªçc
    private float discountFactor = 0.99f;   // Discount future rewards
    private float epsilon = 0.995f;         // Exploration rate
    
    // Q-Learning Update Implementation
    public void UpdateQValue(string state, string action, float reward, string nextState)
    {
        // L·∫•y Q-value hi·ªán t·∫°i
        float currentQ = GetQValue(state, action);
        
        // T√¨m max Q-value c·ªßa next state
        float maxNextQ = GetMaxQValue(nextState);
        
        // √Åp d·ª•ng c√¥ng th·ª©c Q-Learning
        float newQ = currentQ + learningRate * (reward + discountFactor * maxNextQ - currentQ);
        
        // C·∫≠p nh·∫≠t Q-table
        SetQValue(state, action, newQ);
    }
}
```

2.4 STATE ENCODING:
Game state ƒë∆∞·ª£c encode th√†nh string duy nh·∫•t:
"aiPos_x,aiPos_y-humanPos_x,humanPos_y-aiWalls-humanWalls-wallsList"

V√≠ d·ª•: "4,8-4,0-9-10-7,4,False|5,6,True"
- AI ·ªü (4,8), Human ·ªü (4,0)
- AI c√≥ 9 walls, Human c√≥ 10 walls
- Walls ƒë√£ ƒë·∫∑t: (7,4,False), (5,6,True)

2.5 ACTION ENCODING:
Actions ƒë∆∞·ª£c encode th√†nh string:
- Movement: "M:x,y" (v√≠ d·ª•: "M:4,7")
- Wall: "W:x,y,horizontal" (v√≠ d·ª•: "W:3,5,True")

2.6 EPSILON-GREEDY STRATEGY:
```csharp
public AIMove ChooseAction(GameState state, List<AIMove> possibleMoves)
{
    // Exploration vs Exploitation trade-off
    if (Random.Range(0f, 1f) < epsilon)
    {
        // EXPLORATION: Ch·ªçn random ƒë·ªÉ kh√°m ph√°
        return possibleMoves[Random.Range(0, possibleMoves.Count)];
    }
    else
    {
        // EXPLOITATION: Ch·ªçn action c√≥ Q-value cao nh·∫•t
        return GetBestAction(state, possibleMoves);
    }
}
```

2.7 EPSILON DECAY:
Epsilon gi·∫£m d·∫ßn t·ª´ 0.995 ‚Üí 0.1 qua training:
- Ban ƒë·∫ßu: High exploration (99.5%)
- Cu·ªëi training: High exploitation (90%)

2.8 REWARD SYSTEM:
- Win game: +1000
- Lose game: -1000
- Move toward goal: +10
- Block opponent: +50
- Waste move: -5
- Strategic position: +15

================================================================================
                3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
================================================================================

3.1 GI·ªöI THI·ªÜU MINIMAX:
Minimax l√† thu·∫≠t to√°n game theory c·ªï ƒëi·ªÉn, t√¨m ki·∫øm move t·ªëi ∆∞u b·∫±ng c√°ch
d·ª± ƒëo√°n tr∆∞·ªõc nhi·ªÅu moves. Alpha-Beta pruning t·ªëi ∆∞u hi·ªáu su·∫•t b·∫±ng c√°ch
lo·∫°i b·ªè c√°c nh√°nh kh√¥ng c·∫ßn thi·∫øt.

3.2 NGUY√äN L√ù HO·∫†T ƒê·ªòNG:
- Maximizing Player (AI): T·ªëi ƒëa h√≥a score
- Minimizing Player (Human): T·ªëi thi·ªÉu h√≥a score AI
- Search depth: 4 levels (configurable)
- Pruning: Lo·∫°i b·ªè nh√°nh kh√¥ng promising

3.3 IMPLEMENTATION:

```csharp
float Minimax(GameState state, int depth, bool isMaximizing, float alpha, float beta)
{
    // Base case: Depth = 0 ho·∫∑c game over
    if (depth == 0 || IsGameOver(state))
        return EvaluatePosition(state);
    
    if (isMaximizing) // AI turn
    {
        float maxEval = float.NegativeInfinity;
        foreach (var move in GeneratePossibleMoves(state, true))
        {
            GameState newState = ApplyMove(state, move, true);
            float eval = Minimax(newState, depth - 1, false, alpha, beta);
            maxEval = Mathf.Max(maxEval, eval);
            alpha = Mathf.Max(alpha, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return maxEval;
    }
    else // Human turn
    {
        float minEval = float.PositiveInfinity;
        foreach (var move in GeneratePossibleMoves(state, false))
        {
            GameState newState = ApplyMove(state, move, false);
            float eval = Minimax(newState, depth - 1, true, alpha, beta);
            minEval = Mathf.Min(minEval, eval);
            beta = Mathf.Min(beta, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return minEval;
    }
}
```

3.4 EVALUATION FUNCTION:
Position evaluation d·ª±a tr√™n multiple factors:

```csharp
float EvaluatePosition(GameState state)
{
    float score = 0f;
    
    // Distance to goal (c√†ng g·∫ßn c√†ng t·ªët)
    int aiDistance = GetShortestPath(state.aiPosition, goalRow);
    int humanDistance = GetShortestPath(state.humanPosition, goalRow);
    score += (humanDistance - aiDistance) * 100;
    
    // Wall advantage
    score += (state.aiWallsLeft - state.humanWallsLeft) * 50;
    
    // Mobility (s·ªë moves c√≥ th·ªÉ)
    int aiMobility = GetValidMoves(state.aiPosition).Count;
    int humanMobility = GetValidMoves(state.humanPosition).Count;
    score += (aiMobility - humanMobility) * 20;
    
    // Center control
    score += GetCenterControlScore(state) * 30;
    
    return score;
}
```

3.5 ALPHA-BETA PRUNING HI·ªÜU QU·∫¢:
- Reduces search space t·ª´ O(b^d) xu·ªëng O(b^(d/2))
- Best case: 50% reduction in nodes evaluated
- Ordering moves by promising gi√∫p pruning hi·ªáu qu·∫£ h∆°n

================================================================================
                        4. SPECIALIZED AI STYLES SYSTEM
================================================================================

4.1 CONCEPT:
6 AI personalities v·ªõi training v√† behavior ri√™ng bi·ªát, m·ªói style c√≥:
- Ri√™ng Q-table file
- Custom reward structure
- Specific move prioritization
- Unique decision making logic

4.2 AI STYLES CHI TI·∫æT:

üî• AGGRESSIVE STYLE:
- M·ª•c ti√™u: T·∫•n c√¥ng v√† ch·∫∑n ƒë·ªëi th·ªß
- Wall Usage: 70% chance
- Reward Structure:
  * blockingOpponentReward = 50 * intensity
  * wallPlacementReward = 30 * intensity
  * defensiveWallPenalty = -20 * intensity
- Q-table: qtable_aggressive.json

üõ°Ô∏è DEFENSIVE STYLE:
- M·ª•c ti√™u: Ph√≤ng th·ªß v√† di chuy·ªÉn an to√†n
- Wall Usage: 50% chance
- Reward Structure:
  * defensiveWallReward = 35 * intensity
  * movementReward = 20 * intensity
  * aggressiveMovePenalty = -15 * intensity
- Q-table: qtable_defensive.json

üß± WALL MASTER STYLE:
- M·ª•c ti√™u: Chuy√™n gia ƒë·∫∑t t∆∞·ªùng v√† t·∫°o m√™ cung
- Wall Usage: 90% chance
- Reward Structure:
  * wallPlacementReward = 60 * intensity
  * strategicWallReward = 40 * intensity
  * wallComboReward = 50 * intensity
  * movementWithoutWallPenalty = -25 * intensity
- Q-table: qtable_wallmaster.json

‚ö° SPEED RUNNER STYLE:
- M·ª•c ti√™u: ƒê·∫°t ƒë√≠ch nhanh nh·∫•t
- Wall Usage: 10% chance
- Reward Structure:
  * movementReward = 40 * intensity
  * progressToGoalReward = 50 * intensity
  * quickWinBonus = 100 * intensity
  * wallPlacementPenalty = -20 * intensity
- Q-table: qtable_speedrunner.json

üß† TACTICAL STYLE:
- M·ª•c ti√™u: L·ª£i th·∫ø v·ªã tr√≠ v√† k·∫ø ho·∫°ch d√†i h·∫°n
- Wall Usage: Variable (strategic)
- Reward Structure:
  * tacticalMoveReward = 45 * intensity
  * positionAdvantageReward = 35 * intensity
  * mobilityReward = 25 * intensity
  * patternRecognitionBonus = 40 * intensity
- Q-table: qtable_tactical.json

‚öñÔ∏è BALANCED STYLE:
- M·ª•c ti√™u: C√¢n b·∫±ng t·∫•t c·∫£ aspects
- Wall Usage: 30% chance
- Reward Structure: Default balanced values
- Q-table: qtable.json

4.3 STYLE SWITCHING MECHANISM:

```csharp
public void SwitchToStyle(PlayStyle newStyle)
{
    // Save current Q-table
    if (allowQTableSaving) qAgent.SaveQTable(GetStyleSpecificQTablePath());
    
    // Load new style Q-table
    specializedStyle = newStyle;
    qAgent.LoadQTable(GetStyleSpecificQTablePath());
    
    // Reconfigure parameters
    ConfigureStyleSpecificTraining();
}
```

4.4 STYLE INTENSITY:
Parameter t·ª´ 0.0 ‚Üí 1.0 control m·ª©c ƒë·ªô chuy√™n h√≥a:
- 0.0: Balanced behavior
- 0.5: Moderate specialization
- 1.0: Extreme specialization

================================================================================
                        5. DECISION TREE CLASSIFIER
================================================================================

5.1 PURPOSE:
Decision Tree ph√¢n t√≠ch game context ƒë·ªÉ ch·ªçn strategy ph√π h·ª£p, d·ª±a tr√™n:
- Game phase (Early/Mid/End)
- Position advantage
- Resource advantage
- Mobility situation

5.2 FEATURE EXTRACTION:

```csharp
public struct DecisionFeatures
{
    public int aiDistanceToGoal;      // AI distance to goal line
    public int humanDistanceToGoal;   // Human distance to goal line
    public int aiWallsLeft;           // AI walls remaining
    public int humanWallsLeft;        // Human walls remaining
    public int aiMobility;            // AI possible moves
    public int humanMobility;         // Human possible moves
    public float distanceAdvantage;   // Human - AI distance
    public float wallAdvantage;       // AI - Human walls
    public bool isEarlyGame;          // > 15 moves estimated remaining
    public bool isMidGame;            // 5-15 moves remaining
    public bool isEndGame;            // < 5 moves remaining
}
```

5.3 DECISION LOGIC:

```csharp
public AIStrategy Predict(DecisionFeatures features)
{
    // Early Game: Play conservatively
    if (features.isEarlyGame)
        return AIStrategy.Balanced;
    
    // AI has significant lead: Focus on blocking
    if (features.distanceAdvantage > 2)
        return AIStrategy.Blocking;
    
    // AI is behind: Be aggressive
    if (features.distanceAdvantage < -1)
        return AIStrategy.Aggressive;
    
    // End game with wall advantage: Play defensively
    if (features.isEndGame && features.wallAdvantage > 2)
        return AIStrategy.Defensive;
    
    // Close game: Balanced approach
    return AIStrategy.Balanced;
}
```

5.4 STRATEGY OUTCOMES:
- Balanced: Equal weight to movement v√† walls
- Blocking: Prioritize moves that increase opponent path
- Aggressive: Focus on attacking positions
- Defensive: Protect own position and path

================================================================================
                        6. ENTROPY ANALYSIS & SOFTMAX
================================================================================

6.1 ENTROPY CONCEPT:
Entropy ƒëo ƒë·ªô ƒëa d·∫°ng/uncertainty c·ªßa move scores. High entropy = nhi·ªÅu moves
c√≥ scores t∆∞∆°ng ƒë∆∞∆°ng, low entropy = m·ªôt move r√µ r√†ng t·ªët nh·∫•t.

6.2 ENTROPY CALCULATION:

```csharp
float CalculateEntropy(List<float> moveScores)
{
    // Normalize th√†nh probabilities
    float total = moveScores.Sum();
    var probabilities = moveScores.Select(s => s / total);
    
    // Calculate Shannon entropy
    float entropy = 0f;
    foreach (float p in probabilities)
    {
        if (p > 0f)
            entropy -= p * Mathf.Log(p, 2f);
    }
    
    return entropy; // Range: 0 (deterministic) to log2(n) (uniform)
}
```

6.3 SOFTMAX DISTRIBUTION:

```csharp
float[] CalculateSoftmax(List<float> scores, float temperature)
{
    float[] probabilities = new float[scores.Count];
    float sum = scores.Sum(s => Mathf.Exp(s / temperature));
    
    for (int i = 0; i < scores.Count; i++)
    {
        probabilities[i] = Mathf.Exp(scores[i] / temperature) / sum;
    }
    
    return probabilities;
}
```

6.4 TEMPERATURE PARAMETER:
- Low temperature (0.1): Sharp distribution, focus on best moves
- High temperature (2.0): Flat distribution, more exploration
- Default temperature (1.0): Balanced exploitation/exploration

6.5 ADAPTIVE MOVE SELECTION:

```csharp
public AIMove ChooseActionWithEntropy(GameState state, List<AIMove> possibleMoves)
{
    var moveScores = EvaluateAllMoves(state, possibleMoves);
    float entropy = CalculateEntropy(moveScores);
    
    if (entropy > entropyThreshold) // High diversity
    {
        // Use softmax for probabilistic selection
        var probabilities = CalculateSoftmax(moveScores, softmaxTemperature);
        return SelectMoveByProbability(possibleMoves, probabilities);
    }
    else // Clear best move
    {
        // Use deterministic best move
        return GetBestScoredMove(possibleMoves, moveScores);
    }
}
```

================================================================================
                        7. TRAINING SYSTEM & SELF-PLAY
================================================================================

7.1 SELF-PLAY CONCEPT:
AI h·ªçc b·∫±ng c√°ch ch∆°i v·ªõi ch√≠nh n√≥ h√†ng ng√†n l·∫ßn, gradually improve performance
th√¥ng qua trial v√† error.

7.2 TRAINING PIPELINE:

```csharp
IEnumerator RunSelfPlayTraining()
{
    for (int episode = 1; episode <= maxEpisodes; episode++)
    {
        // Reset game state
        ResetGameForTraining();
        
        while (!IsGameOver())
        {
            // Get current state
            GameState currentState = GetCurrentGameState();
            List<AIMove> possibleMoves = GetPossibleMoves();
            
            // Choose action using Œµ-greedy
            AIMove chosenMove = qLearningAgent.ChooseAction(currentState, possibleMoves);
            
            // Execute move
            ExecuteMove(chosenMove);
            
            // Calculate reward
            float reward = CalculateReward(currentState, chosenMove);
            
            // Update Q-table
            GameState newState = GetCurrentGameState();
            qLearningAgent.UpdateQValue(
                EncodeState(currentState),
                EncodeAction(chosenMove),
                reward,
                EncodeState(newState)
            );
            
            yield return new WaitForEndOfFrame();
        }
        
        // Epsilon decay
        qLearningAgent.DecayEpsilon();
        
        // Save progress periodically
        if (episode % 100 == 0)
        {
            qLearningAgent.SaveQTable();
        }
    }
}
```

7.3 REWARD ENGINEERING:
Critical cho Q-Learning success:

```csharp
float CalculateReward(GameState state, AIMove move)
{
    float reward = 0f;
    
    // Game outcome rewards
    if (IsWin(state)) reward += 1000f;
    if (IsLoss(state)) reward -= 1000f;
    
    // Progress rewards
    if (move.moveType == MoveType.Movement)
    {
        if (IsMoveTowardGoal(move)) reward += 20f;
        else reward -= 5f;
    }
    
    // Strategic rewards
    if (move.moveType == MoveType.WallPlacement)
    {
        if (IsWallBlockingOpponent(move)) reward += 50f;
        if (IsWallStrategic(move)) reward += 30f;
        if (IsWallWasted(move)) reward -= 25f;
    }
    
    // Style-specific rewards
    reward += GetStyleSpecificReward(state, move);
    
    return reward;
}
```

7.4 TRAINING METRICS:
- Episodes: 1000 standard
- Duration: ~15 minutes
- Convergence: Usually around episode 800
- Q-table size: 13,118+ states at completion
- Final epsilon: 0.1 (90% exploitation)

================================================================================
                        8. AI TESTING FRAMEWORK
================================================================================

8.1 TESTING PHILOSOPHY:
Comprehensive testing ensures AI quality v√† reliability across different
game scenarios v√† edge cases.

8.2 TEST CATEGORIES:

A) DECISION TREE TESTS:
```csharp
void TestDecisionTreeStrategies()
{
    // Test Early Game
    var earlyGameFeatures = new DecisionFeatures
    {
        aiDistanceToGoal = 8, humanDistanceToGoal = 8,
        isEarlyGame = true
    };
    var strategy = decisionTree.Predict(earlyGameFeatures);
    Assert.AreEqual(AIStrategy.Balanced, strategy);
    
    // Test AI Leading
    var leadingFeatures = new DecisionFeatures
    {
        distanceAdvantage = 3, // AI leading
        isMidGame = true
    };
    strategy = decisionTree.Predict(leadingFeatures);
    Assert.AreEqual(AIStrategy.Blocking, strategy);
}
```

B) ENTROPY ANALYSIS TESTS:
```csharp
void TestEntropyAnalysis()
{
    // High entropy case (diverse scores)
    var highEntropyScores = new List<float> { 100f, 98f, 97f, 95f, 94f };
    float entropy = CalculateTestEntropy(highEntropyScores);
    Assert.IsTrue(entropy > 0.5f);
    
    // Low entropy case (clear winner)
    var lowEntropyScores = new List<float> { 100f, 50f, 50f, 50f, 50f };
    entropy = CalculateTestEntropy(lowEntropyScores);
    Assert.IsTrue(entropy < 0.5f);
}
```

C) SOFTMAX DISTRIBUTION TESTS:
```csharp
void TestSoftmaxDistribution()
{
    var testScores = new List<float> { 100f, 98f, 95f, 90f };
    float temperature = 1.0f;
    
    var probabilities = CalculateSoftmax(testScores, temperature);
    
    // Verify probabilities sum to 1
    float sum = probabilities.Sum();
    Assert.AreEqual(1.0f, sum, 0.001f);
    
    // Verify highest score gets highest probability
    int maxIndex = Array.IndexOf(probabilities, probabilities.Max());
    Assert.AreEqual(0, maxIndex);
}
```

D) PERFORMANCE BENCHMARKS:
- Move generation time: <100ms
- Q-Learning decision: <50ms
- Minimax decision (depth 4): <200ms
- Memory usage: <256MB

8.3 AUTOMATED TESTING:
Tests run automatically ƒë·ªÉ ensure code quality:
- Unit tests cho individual algorithms
- Integration tests cho AI system interactions
- Performance tests cho response times
- Regression tests ƒë·ªÉ prevent quality degradation

================================================================================
                        9. HYBRID AI ARCHITECTURE
================================================================================

9.1 CONCEPT:
Hybrid system k·∫øt h·ª£p strengths c·ªßa multiple algorithms:
- Q-Learning: Data-driven, learns from experience
- Minimax: Logical, guaranteed optimal given evaluation
- Decision Tree: Fast, context-aware switching
- Entropy: Adaptive, handles uncertainty

9.2 ALGORITHM SELECTION LOGIC:

```csharp
public AIMove GetBestMove()
{
    UpdateGameState();
    List<AIMove> possibleMoves = GeneratePossibleMoves();
    
    // Primary: Use Q-Learning if trained model available
    if (useQLearning && qAgent.IsTrainedModel())
    {
        Debug.Log($"üß† Using {specializedStyle} Q-Learning");
        return qAgent.ChooseAction(currentGameState, possibleMoves);
    }
    
    // Fallback: Use Minimax for guaranteed reasonable play
    else
    {
        Debug.Log($"üîç Using {specializedStyle} Minimax");
        return FindBestMoveWithMinimax();
    }
}
```

9.3 SEAMLESS SWITCHING:
- No interruption in gameplay
- Maintains consistent API
- Preserves game state integrity
- Logs algorithm used for debugging

9.4 BENEFITS:
- Robustness: Fallback ensures AI never fails
- Performance: Q-Learning for trained scenarios, Minimax for edge cases
- Flexibility: Can switch algorithms based on situation
- Development: Easy to test individual components

================================================================================
                        10. PERFORMANCE OPTIMIZATION
================================================================================

10.1 Q-TABLE OPTIMIZATION:
- Sparse representation: Only store visited states
- Compression: String interning for state keys
- Lazy loading: Load Q-table on demand
- Periodic saving: Every 100 training episodes

10.2 MINIMAX OPTIMIZATION:
- Alpha-Beta pruning: 50% search reduction
- Move ordering: Promising moves first
- Iterative deepening: Gradual depth increase
- Transposition tables: Cache evaluated positions

10.3 MEMORY MANAGEMENT:
- Object pooling cho GameState objects
- String caching cho repeated state/action encodings
- Garbage collection optimization
- Memory profiling v√† monitoring

10.4 MULTITHREADING CONSIDERATIONS:
- Q-Learning training: Background thread
- Move calculation: Main thread (Unity requirement)
- File I/O: Async operations where possible
- Thread-safe collections cho shared data

10.5 CACHING STRATEGIES:
- Path finding results
- Move generation results
- Evaluation function results
- Feature extraction results

================================================================================
                            K·∫æT LU·∫¨N
================================================================================

D·ª± √°n AI Quoridor Project th·ªÉ hi·ªán m·ªôt implementation comprehensive c·ªßa modern
game AI techniques:

TECHNICAL INNOVATIONS:
‚úÖ Hybrid Q-Learning + Minimax architecture
‚úÖ 6 specialized AI personalities v·ªõi separate training
‚úÖ Advanced decision making v·ªõi entropy analysis
‚úÖ Comprehensive testing v√† validation framework
‚úÖ Professional development tools v√† debugging

EDUCATIONAL VALUE:
‚úÖ Practical machine learning implementation
‚úÖ Traditional game AI algorithms
‚úÖ Modern software engineering practices
‚úÖ Performance optimization techniques
‚úÖ Automated testing methodologies

FUTURE ENHANCEMENTS:
üöÄ Deep Q-Learning (DQN) integration
üöÄ Multi-agent reinforcement learning
üöÄ Neural network-based evaluation functions
üöÄ Online learning v√† adaptation
üöÄ Transfer learning between game variants

D·ª± √°n n√†y demonstrating successful integration c·ªßa classical AI methods v·ªõi
modern machine learning, t·∫°o ra m·ªôt intelligent, adaptive, v√† highly
interactive gaming experience.

================================================================================
                            APPENDIX
================================================================================

FILES LI√äN QUAN:
- Assets/scripts/QLearningAgent.cs - Q-Learning implementation
- Assets/scripts/QuoridorAI.cs - Main AI controller
- Assets/scripts/DecisionTreeClassifier.cs - Decision tree logic
- Assets/scripts/QuoridorSelfPlayTrainer.cs - Training system
- Assets/scripts/AITestScript.cs - Testing framework
- Assets/scripts/Editor/ForceRetrainAI.cs - Development tools

Q-TABLE FILES:
- StreamingAssets/qtable.json - Balanced style
- StreamingAssets/qtable_aggressive.json - Aggressive style
- StreamingAssets/qtable_defensive.json - Defensive style
- StreamingAssets/qtable_wallmaster.json - Wall Master style
- StreamingAssets/qtable_speedrunner.json - Speed Runner style
- StreamingAssets/qtable_tactical.json - Tactical style

DOCUMENTATION:
- AI_ALGORITHMS_DETAILED_DESCRIPTION.txt - This file
- README.md - Project overview
- API_INTEGRATION_GUIDE.md - Voice/Chat integration
- TRAINING_GUIDE.md - AI training instructions

================================================================================
================================================================================
                    AI QUORIDOR GAME PROJECT - THU·∫¨T TO√ÅN AI CHI TI·∫æT
================================================================================

D·ª± √°n: AI Quoridor Game v·ªõi Machine Learning v√† Multi-Platform Integration
Ng√†y t·∫°o: 25/07/2025
T√°c gi·∫£: [Student Name]
Platform: Unity 3D
Language: C#

================================================================================
                            M·ª§C L·ª§C
================================================================================

1. T·ªîNG QUAN KI·∫æN TR√öC AI
2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
4. SPECIALIZED AI STYLES SYSTEM
5. DECISION TREE CLASSIFIER
6. ENTROPY ANALYSIS & SOFTMAX
7. TRAINING SYSTEM & SELF-PLAY
8. AI TESTING FRAMEWORK
9. HYBRID AI ARCHITECTURE
10. PERFORMANCE OPTIMIZATION

================================================================================
                        1. T·ªîNG QUAN KI·∫æN TR√öC AI
================================================================================

D·ª± √°n AI Quoridor s·ª≠ d·ª•ng m·ªôt ki·∫øn tr√∫c AI hybrid ti√™n ti·∫øn, k·∫øt h·ª£p:

1.1 CORE ALGORITHMS:
- Q-Learning (Reinforcement Learning) - Thu·∫≠t to√°n ch√≠nh
- Minimax + Alpha-Beta Pruning - Thu·∫≠t to√°n fallback
- Decision Tree Classifier - Ph√¢n t√≠ch ng·ªØ c·∫£nh game
- Entropy Analysis - ƒê√°nh gi√° ƒëa d·∫°ng move

1.2 SPECIALIZED SYSTEMS:
- 6 AI Personality Styles v·ªõi training ri√™ng bi·ªát
- Self-Play Training Pipeline t·ª± ƒë·ªông
- Real-time Performance Testing Framework
- Dynamic Algorithm Switching

1.3 TECHNICAL FOUNDATION:
- State Space: 13,118+ unique game states
- Action Space: Movement + Wall placement combinations
- Training Method: Self-play v·ªõi 1000 episodes
- Learning Rate: Œ± = 0.1, Discount Factor: Œ≥ = 0.99

================================================================================
                    2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
================================================================================

2.1 GI·ªöI THI·ªÜU Q-LEARNING:
Q-Learning l√† thu·∫≠t to√°n Reinforcement Learning kh√¥ng c·∫ßn model, h·ªçc th√¥ng qua
t∆∞∆°ng t√°c v·ªõi m√¥i tr∆∞·ªùng ƒë·ªÉ t·ªëi ∆∞u h√≥a policy. Trong Quoridor, AI h·ªçc c√°ch
ch·ªçn moves t·ªëi ∆∞u d·ª±a tr√™n kinh nghi·ªám t·ª´ h√†ng ng√†n game.

2.2 C√îNG TH·ª®C TO√ÅN H·ªåC:
Q(s,a) = Q(s,a) + Œ±[r + Œ≥*max(Q(s',a')) - Q(s,a)]

Trong ƒë√≥:
- Q(s,a): Q-value c·ªßa state s v·ªõi action a
- Œ±: Learning rate (0.1) - t·ªëc ƒë·ªô h·ªçc
- r: Immediate reward t·ª´ action
- Œ≥: Discount factor (0.99) - t·∫ßm quan tr·ªçng future rewards
- s': Next state sau khi th·ª±c hi·ªán action
- max(Q(s',a')): Maximum Q-value c·ªßa next state

2.3 IMPLEMENTATION CHI TI·∫æT:

```csharp
public class QLearningAgent
{
    // Core Q-Table: Dictionary l∆∞u tr·ªØ Q-values
    private Dictionary<string, Dictionary<string, float>> qTable;
    
    // Hyperparameters
    private float learningRate = 0.1f;      // T·ªëc ƒë·ªô h·ªçc
    private float discountFactor = 0.99f;   // Discount future rewards
    private float epsilon = 0.995f;         // Exploration rate
    
    // Q-Learning Update Implementation
    public void UpdateQValue(string state, string action, float reward, string nextState)
    {
        // L·∫•y Q-value hi·ªán t·∫°i
        float currentQ = GetQValue(state, action);
        
        // T√¨m max Q-value c·ªßa next state
        float maxNextQ = GetMaxQValue(nextState);
        
        // √Åp d·ª•ng c√¥ng th·ª©c Q-Learning
        float newQ = currentQ + learningRate * (reward + discountFactor * maxNextQ - currentQ);
        
        // C·∫≠p nh·∫≠t Q-table
        SetQValue(state, action, newQ);
    }
}
```

2.4 STATE ENCODING:
Game state ƒë∆∞·ª£c encode th√†nh string duy nh·∫•t:
"aiPos_x,aiPos_y-humanPos_x,humanPos_y-aiWalls-humanWalls-wallsList"

V√≠ d·ª•: "4,8-4,0-9-10-7,4,False|5,6,True"
- AI ·ªü (4,8), Human ·ªü (4,0)
- AI c√≥ 9 walls, Human c√≥ 10 walls
- Walls ƒë√£ ƒë·∫∑t: (7,4,False), (5,6,True)

2.5 ACTION ENCODING:
Actions ƒë∆∞·ª£c encode th√†nh string:
- Movement: "M:x,y" (v√≠ d·ª•: "M:4,7")
- Wall: "W:x,y,horizontal" (v√≠ d·ª•: "W:3,5,True")

2.6 EPSILON-GREEDY STRATEGY:
```csharp
public AIMove ChooseAction(GameState state, List<AIMove> possibleMoves)
{
    // Exploration vs Exploitation trade-off
    if (Random.Range(0f, 1f) < epsilon)
    {
        // EXPLORATION: Ch·ªçn random ƒë·ªÉ kh√°m ph√°
        return possibleMoves[Random.Range(0, possibleMoves.Count)];
    }
    else
    {
        // EXPLOITATION: Ch·ªçn action c√≥ Q-value cao nh·∫•t
        return GetBestAction(state, possibleMoves);
    }
}
```

2.7 EPSILON DECAY:
Epsilon gi·∫£m d·∫ßn t·ª´ 0.995 ‚Üí 0.1 qua training:
- Ban ƒë·∫ßu: High exploration (99.5%)
- Cu·ªëi training: High exploitation (90%)

2.8 REWARD SYSTEM:
- Win game: +1000
- Lose game: -1000
- Move toward goal: +10
- Block opponent: +50
- Waste move: -5
- Strategic position: +15

================================================================================
                3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
================================================================================

3.1 GI·ªöI THI·ªÜU MINIMAX:
Minimax l√† thu·∫≠t to√°n game theory c·ªï ƒëi·ªÉn, t√¨m ki·∫øm move t·ªëi ∆∞u b·∫±ng c√°ch
d·ª± ƒëo√°n tr∆∞·ªõc nhi·ªÅu moves. Alpha-Beta pruning t·ªëi ∆∞u hi·ªáu su·∫•t b·∫±ng c√°ch
lo·∫°i b·ªè c√°c nh√°nh kh√¥ng c·∫ßn thi·∫øt.

3.2 NGUY√äN L√ù HO·∫†T ƒê·ªòNG:
- Maximizing Player (AI): T·ªëi ƒëa h√≥a score
- Minimizing Player (Human): T·ªëi thi·ªÉu h√≥a score AI
- Search depth: 4 levels (configurable)
- Pruning: Lo·∫°i b·ªè nh√°nh kh√¥ng promising

3.3 IMPLEMENTATION:

```csharp
float Minimax(GameState state, int depth, bool isMaximizing, float alpha, float beta)
{
    // Base case: Depth = 0 ho·∫∑c game over
    if (depth == 0 || IsGameOver(state))
        return EvaluatePosition(state);
    
    if (isMaximizing) // AI turn
    {
        float maxEval = float.NegativeInfinity;
        foreach (var move in GeneratePossibleMoves(state, true))
        {
            GameState newState = ApplyMove(state, move, true);
            float eval = Minimax(newState, depth - 1, false, alpha, beta);
            maxEval = Mathf.Max(maxEval, eval);
            alpha = Mathf.Max(alpha, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return maxEval;
    }
    else // Human turn
    {
        float minEval = float.PositiveInfinity;
        foreach (var move in GeneratePossibleMoves(state, false))
        {
            GameState newState = ApplyMove(state, move, false);
            float eval = Minimax(newState, depth - 1, true, alpha, beta);
            minEval = Mathf.Min(minEval, eval);
            beta = Mathf.Min(beta, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return minEval;
    }
}
```

3.4 EVALUATION FUNCTION:
Position evaluation d·ª±a tr√™n multiple factors:

```csharp
float EvaluatePosition(GameState state)
{
    float score = 0f;
    
    // Distance to goal (c√†ng g·∫ßn c√†ng t·ªët)
    int aiDistance = GetShortestPath(state.aiPosition, goalRow);
    int humanDistance = GetShortestPath(state.humanPosition, goalRow);
    score += (humanDistance - aiDistance) * 100;
    
    // Wall advantage
    score += (state.aiWallsLeft - state.humanWallsLeft) * 50;
    
    // Mobility (s·ªë moves c√≥ th·ªÉ)
    int aiMobility = GetValidMoves(state.aiPosition).Count;
    int humanMobility = GetValidMoves(state.humanPosition).Count;
    score += (aiMobility - humanMobility) * 20;
    
    // Center control
    score += GetCenterControlScore(state) * 30;
    
    return score;
}
```

3.5 ALPHA-BETA PRUNING HI·ªÜU QU·∫¢:
- Reduces search space t·ª´ O(b^d) xu·ªëng O(b^(d/2))
- Best case: 50% reduction in nodes evaluated
- Ordering moves by promising gi√∫p pruning hi·ªáu qu·∫£ h∆°n

================================================================================
                        4. SPECIALIZED AI STYLES SYSTEM
================================================================================

4.1 CONCEPT:
6 AI personalities v·ªõi training v√† behavior ri√™ng bi·ªát, m·ªói style c√≥:
- Ri√™ng Q-table file
- Custom reward structure
- Specific move prioritization
- Unique decision making logic

4.2 AI STYLES CHI TI·∫æT:

üî• AGGRESSIVE STYLE:
- M·ª•c ti√™u: T·∫•n c√¥ng v√† ch·∫∑n ƒë·ªëi th·ªß
- Wall Usage: 70% chance
- Reward Structure:
  * blockingOpponentReward = 50 * intensity
  * wallPlacementReward = 30 * intensity
  * defensiveWallPenalty = -20 * intensity
- Q-table: qtable_aggressive.json

üõ°Ô∏è DEFENSIVE STYLE:
- M·ª•c ti√™u: Ph√≤ng th·ªß v√† di chuy·ªÉn an to√†n
- Wall Usage: 50% chance
- Reward Structure:
  * defensiveWallReward = 35 * intensity
  * movementReward = 20 * intensity
  * aggressiveMovePenalty = -15 * intensity
- Q-table: qtable_defensive.json

üß± WALL MASTER STYLE:
- M·ª•c ti√™u: Chuy√™n gia ƒë·∫∑t t∆∞·ªùng v√† t·∫°o m√™ cung
- Wall Usage: 90% chance
- Reward Structure:
  * wallPlacementReward = 60 * intensity
  * strategicWallReward = 40 * intensity
  * wallComboReward = 50 * intensity
  * movementWithoutWallPenalty = -25 * intensity
- Q-table: qtable_wallmaster.json

‚ö° SPEED RUNNER STYLE:
- M·ª•c ti√™u: ƒê·∫°t ƒë√≠ch nhanh nh·∫•t
- Wall Usage: 10% chance
- Reward Structure:
  * movementReward = 40 * intensity
  * progressToGoalReward = 50 * intensity
  * quickWinBonus = 100 * intensity
  * wallPlacementPenalty = -20 * intensity
- Q-table: qtable_speedrunner.json

üß† TACTICAL STYLE:
- M·ª•c ti√™u: L·ª£i th·∫ø v·ªã tr√≠ v√† k·∫ø ho·∫°ch d√†i h·∫°n
- Wall Usage: Variable (strategic)
- Reward Structure:
  * tacticalMoveReward = 45 * intensity
  * positionAdvantageReward = 35 * intensity
  * mobilityReward = 25 * intensity
  * patternRecognitionBonus = 40 * intensity
- Q-table: qtable_tactical.json

‚öñÔ∏è BALANCED STYLE:
- M·ª•c ti√™u: C√¢n b·∫±ng t·∫•t c·∫£ aspects
- Wall Usage: 30% chance
- Reward Structure: Default balanced values
- Q-table: qtable.json

4.3 STYLE SWITCHING MECHANISM:

```csharp
public void SwitchToStyle(PlayStyle newStyle)
{
    // Save current Q-table
    if (allowQTableSaving) qAgent.SaveQTable(GetStyleSpecificQTablePath());
    
    // Load new style Q-table
    specializedStyle = newStyle;
    qAgent.LoadQTable(GetStyleSpecificQTablePath());
    
    // Reconfigure parameters
    ConfigureStyleSpecificTraining();
}
```

4.4 STYLE INTENSITY:
Parameter t·ª´ 0.0 ‚Üí 1.0 control m·ª©c ƒë·ªô chuy√™n h√≥a:
- 0.0: Balanced behavior
- 0.5: Moderate specialization
- 1.0: Extreme specialization

================================================================================
                        5. DECISION TREE CLASSIFIER
================================================================================

5.1 PURPOSE:
Decision Tree ph√¢n t√≠ch game context ƒë·ªÉ ch·ªçn strategy ph√π h·ª£p, d·ª±a tr√™n:
- Game phase (Early/Mid/End)
- Position advantage
- Resource advantage
- Mobility situation

5.2 FEATURE EXTRACTION:

```csharp
public struct DecisionFeatures
{
    public int aiDistanceToGoal;      // AI distance to goal line
    public int humanDistanceToGoal;   // Human distance to goal line
    public int aiWallsLeft;           // AI walls remaining
    public int humanWallsLeft;        // Human walls remaining
    public int aiMobility;            // AI possible moves
    public int humanMobility;         // Human possible moves
    public float distanceAdvantage;   // Human - AI distance
    public float wallAdvantage;       // AI - Human walls
    public bool isEarlyGame;          // > 15 moves estimated remaining
    public bool isMidGame;            // 5-15 moves remaining
    public bool isEndGame;            // < 5 moves remaining
}
```

5.3 DECISION LOGIC:

```csharp
public AIStrategy Predict(DecisionFeatures features)
{
    // Early Game: Play conservatively
    if (features.isEarlyGame)
        return AIStrategy.Balanced;
    
    // AI has significant lead: Focus on blocking
    if (features.distanceAdvantage > 2)
        return AIStrategy.Blocking;
    
    // AI is behind: Be aggressive
    if (features.distanceAdvantage < -1)
        return AIStrategy.Aggressive;
    
    // End game with wall advantage: Play defensively
    if (features.isEndGame && features.wallAdvantage > 2)
        return AIStrategy.Defensive;
    
    // Close game: Balanced approach
    return AIStrategy.Balanced;
}
```

5.4 STRATEGY OUTCOMES:
- Balanced: Equal weight to movement v√† walls
- Blocking: Prioritize moves that increase opponent path
- Aggressive: Focus on attacking positions
- Defensive: Protect own position and path

================================================================================
                        6. ENTROPY ANALYSIS & SOFTMAX
================================================================================

6.1 ENTROPY CONCEPT:
Entropy ƒëo ƒë·ªô ƒëa d·∫°ng/uncertainty c·ªßa move scores. High entropy = nhi·ªÅu moves
c√≥ scores t∆∞∆°ng ƒë∆∞∆°ng, low entropy = m·ªôt move r√µ r√†ng t·ªët nh·∫•t.

6.2 ENTROPY CALCULATION:

```csharp
float CalculateEntropy(List<float> moveScores)
{
    // Normalize th√†nh probabilities
    float total = moveScores.Sum();
    var probabilities = moveScores.Select(s => s / total);
    
    // Calculate Shannon entropy
    float entropy = 0f;
    foreach (float p in probabilities)
    {
        if (p > 0f)
            entropy -= p * Mathf.Log(p, 2f);
    }
    
    return entropy; // Range: 0 (deterministic) to log2(n) (uniform)
}
```

6.3 SOFTMAX DISTRIBUTION:

```csharp
float[] CalculateSoftmax(List<float> scores, float temperature)
{
    float[] probabilities = new float[scores.Count];
    float sum = scores.Sum(s => Mathf.Exp(s / temperature));
    
    for (int i = 0; i < scores.Count; i++)
    {
        probabilities[i] = Mathf.Exp(scores[i] / temperature) / sum;
    }
    
    return probabilities;
}
```

6.4 TEMPERATURE PARAMETER:
- Low temperature (0.1): Sharp distribution, focus on best moves
- High temperature (2.0): Flat distribution, more exploration
- Default temperature (1.0): Balanced exploitation/exploration

6.5 ADAPTIVE MOVE SELECTION:

```csharp
public AIMove ChooseActionWithEntropy(GameState state, List<AIMove> possibleMoves)
{
    var moveScores = EvaluateAllMoves(state, possibleMoves);
    float entropy = CalculateEntropy(moveScores);
    
    if (entropy > entropyThreshold) // High diversity
    {
        // Use softmax for probabilistic selection
        var probabilities = CalculateSoftmax(moveScores, softmaxTemperature);
        return SelectMoveByProbability(possibleMoves, probabilities);
    }
    else // Clear best move
    {
        // Use deterministic best move
        return GetBestScoredMove(possibleMoves, moveScores);
    }
}
```

================================================================================
                        7. TRAINING SYSTEM & SELF-PLAY
================================================================================

7.1 SELF-PLAY CONCEPT:
AI h·ªçc b·∫±ng c√°ch ch∆°i v·ªõi ch√≠nh n√≥ h√†ng ng√†n l·∫ßn, gradually improve performance
th√¥ng qua trial v√† error.

7.2 TRAINING PIPELINE:

```csharp
IEnumerator RunSelfPlayTraining()
{
    for (int episode = 1; episode <= maxEpisodes; episode++)
    {
        // Reset game state
        ResetGameForTraining();
        
        while (!IsGameOver())
        {
            // Get current state
            GameState currentState = GetCurrentGameState();
            List<AIMove> possibleMoves = GetPossibleMoves();
            
            // Choose action using Œµ-greedy
            AIMove chosenMove = qLearningAgent.ChooseAction(currentState, possibleMoves);
            
            // Execute move
            ExecuteMove(chosenMove);
            
            // Calculate reward
            float reward = CalculateReward(currentState, chosenMove);
            
            // Update Q-table
            GameState newState = GetCurrentGameState();
            qLearningAgent.UpdateQValue(
                EncodeState(currentState),
                EncodeAction(chosenMove),
                reward,
                EncodeState(newState)
            );
            
            yield return new WaitForEndOfFrame();
        }
        
        // Epsilon decay
        qLearningAgent.DecayEpsilon();
        
        // Save progress periodically
        if (episode % 100 == 0)
        {
            qLearningAgent.SaveQTable();
        }
    }
}
```

7.3 REWARD ENGINEERING:
Critical cho Q-Learning success:

```csharp
float CalculateReward(GameState state, AIMove move)
{
    float reward = 0f;
    
    // Game outcome rewards
    if (IsWin(state)) reward += 1000f;
    if (IsLoss(state)) reward -= 1000f;
    
    // Progress rewards
    if (move.moveType == MoveType.Movement)
    {
        if (IsMoveTowardGoal(move)) reward += 20f;
        else reward -= 5f;
    }
    
    // Strategic rewards
    if (move.moveType == MoveType.WallPlacement)
    {
        if (IsWallBlockingOpponent(move)) reward += 50f;
        if (IsWallStrategic(move)) reward += 30f;
        if (IsWallWasted(move)) reward -= 25f;
    }
    
    // Style-specific rewards
    reward += GetStyleSpecificReward(state, move);
    
    return reward;
}
```

7.4 TRAINING METRICS:
- Episodes: 1000 standard
- Duration: ~15 minutes
- Convergence: Usually around episode 800
- Q-table size: 13,118+ states at completion
- Final epsilon: 0.1 (90% exploitation)

================================================================================
                        8. AI TESTING FRAMEWORK
================================================================================

8.1 TESTING PHILOSOPHY:
Comprehensive testing ensures AI quality v√† reliability across different
game scenarios v√† edge cases.

8.2 TEST CATEGORIES:

A) DECISION TREE TESTS:
```csharp
void TestDecisionTreeStrategies()
{
    // Test Early Game
    var earlyGameFeatures = new DecisionFeatures
    {
        aiDistanceToGoal = 8, humanDistanceToGoal = 8,
        isEarlyGame = true
    };
    var strategy = decisionTree.Predict(earlyGameFeatures);
    Assert.AreEqual(AIStrategy.Balanced, strategy);
    
    // Test AI Leading
    var leadingFeatures = new DecisionFeatures
    {
        distanceAdvantage = 3, // AI leading
        isMidGame = true
    };
    strategy = decisionTree.Predict(leadingFeatures);
    Assert.AreEqual(AIStrategy.Blocking, strategy);
}
```

B) ENTROPY ANALYSIS TESTS:
```csharp
void TestEntropyAnalysis()
{
    // High entropy case (diverse scores)
    var highEntropyScores = new List<float> { 100f, 98f, 97f, 95f, 94f };
    float entropy = CalculateTestEntropy(highEntropyScores);
    Assert.IsTrue(entropy > 0.5f);
    
    // Low entropy case (clear winner)
    var lowEntropyScores = new List<float> { 100f, 50f, 50f, 50f, 50f };
    entropy = CalculateTestEntropy(lowEntropyScores);
    Assert.IsTrue(entropy < 0.5f);
}
```

C) SOFTMAX DISTRIBUTION TESTS:
```csharp
void TestSoftmaxDistribution()
{
    var testScores = new List<float> { 100f, 98f, 95f, 90f };
    float temperature = 1.0f;
    
    var probabilities = CalculateSoftmax(testScores, temperature);
    
    // Verify probabilities sum to 1
    float sum = probabilities.Sum();
    Assert.AreEqual(1.0f, sum, 0.001f);
    
    // Verify highest score gets highest probability
    int maxIndex = Array.IndexOf(probabilities, probabilities.Max());
    Assert.AreEqual(0, maxIndex);
}
```

D) PERFORMANCE BENCHMARKS:
- Move generation time: <100ms
- Q-Learning decision: <50ms
- Minimax decision (depth 4): <200ms
- Memory usage: <256MB

8.3 AUTOMATED TESTING:
Tests run automatically ƒë·ªÉ ensure code quality:
- Unit tests cho individual algorithms
- Integration tests cho AI system interactions
- Performance tests cho response times
- Regression tests ƒë·ªÉ prevent quality degradation

================================================================================
                        9. HYBRID AI ARCHITECTURE
================================================================================

9.1 CONCEPT:
Hybrid system k·∫øt h·ª£p strengths c·ªßa multiple algorithms:
- Q-Learning: Data-driven, learns from experience
- Minimax: Logical, guaranteed optimal given evaluation
- Decision Tree: Fast, context-aware switching
- Entropy: Adaptive, handles uncertainty

9.2 ALGORITHM SELECTION LOGIC:

```csharp
public AIMove GetBestMove()
{
    UpdateGameState();
    List<AIMove> possibleMoves = GeneratePossibleMoves();
    
    // Primary: Use Q-Learning if trained model available
    if (useQLearning && qAgent.IsTrainedModel())
    {
        Debug.Log($"üß† Using {specializedStyle} Q-Learning");
        return qAgent.ChooseAction(currentGameState, possibleMoves);
    }
    
    // Fallback: Use Minimax for guaranteed reasonable play
    else
    {
        Debug.Log($"üîç Using {specializedStyle} Minimax");
        return FindBestMoveWithMinimax();
    }
}
```

9.3 SEAMLESS SWITCHING:
- No interruption in gameplay
- Maintains consistent API
- Preserves game state integrity
- Logs algorithm used for debugging

9.4 BENEFITS:
- Robustness: Fallback ensures AI never fails
- Performance: Q-Learning for trained scenarios, Minimax for edge cases
- Flexibility: Can switch algorithms based on situation
- Development: Easy to test individual components

================================================================================
                        10. PERFORMANCE OPTIMIZATION
================================================================================

10.1 Q-TABLE OPTIMIZATION:
- Sparse representation: Only store visited states
- Compression: String interning for state keys
- Lazy loading: Load Q-table on demand
- Periodic saving: Every 100 training episodes

10.2 MINIMAX OPTIMIZATION:
- Alpha-Beta pruning: 50% search reduction
- Move ordering: Promising moves first
- Iterative deepening: Gradual depth increase
- Transposition tables: Cache evaluated positions

10.3 MEMORY MANAGEMENT:
- Object pooling cho GameState objects
- String caching cho repeated state/action encodings
- Garbage collection optimization
- Memory profiling v√† monitoring

10.4 MULTITHREADING CONSIDERATIONS:
- Q-Learning training: Background thread
- Move calculation: Main thread (Unity requirement)
- File I/O: Async operations where possible
- Thread-safe collections cho shared data

10.5 CACHING STRATEGIES:
- Path finding results
- Move generation results
- Evaluation function results
- Feature extraction results

================================================================================
                            K·∫æT LU·∫¨N
================================================================================

D·ª± √°n AI Quoridor Project th·ªÉ hi·ªán m·ªôt implementation comprehensive c·ªßa modern
game AI techniques:

TECHNICAL INNOVATIONS:
‚úÖ Hybrid Q-Learning + Minimax architecture
‚úÖ 6 specialized AI personalities v·ªõi separate training
‚úÖ Advanced decision making v·ªõi entropy analysis
‚úÖ Comprehensive testing v√† validation framework
‚úÖ Professional development tools v√† debugging

EDUCATIONAL VALUE:
‚úÖ Practical machine learning implementation
‚úÖ Traditional game AI algorithms
‚úÖ Modern software engineering practices
‚úÖ Performance optimization techniques
‚úÖ Automated testing methodologies

FUTURE ENHANCEMENTS:
üöÄ Deep Q-Learning (DQN) integration
üöÄ Multi-agent reinforcement learning
üöÄ Neural network-based evaluation functions
üöÄ Online learning v√† adaptation
üöÄ Transfer learning between game variants

D·ª± √°n n√†y demonstrating successful integration c·ªßa classical AI methods v·ªõi
modern machine learning, t·∫°o ra m·ªôt intelligent, adaptive, v√† highly
interactive gaming experience.

================================================================================
                            APPENDIX
================================================================================

FILES LI√äN QUAN:
- Assets/scripts/QLearningAgent.cs - Q-Learning implementation
- Assets/scripts/QuoridorAI.cs - Main AI controller
- Assets/scripts/DecisionTreeClassifier.cs - Decision tree logic
- Assets/scripts/QuoridorSelfPlayTrainer.cs - Training system
- Assets/scripts/AITestScript.cs - Testing framework
- Assets/scripts/Editor/ForceRetrainAI.cs - Development tools

Q-TABLE FILES:
- StreamingAssets/qtable.json - Balanced style
- StreamingAssets/qtable_aggressive.json - Aggressive style
- StreamingAssets/qtable_defensive.json - Defensive style
- StreamingAssets/qtable_wallmaster.json - Wall Master style
- StreamingAssets/qtable_speedrunner.json - Speed Runner style
- StreamingAssets/qtable_tactical.json - Tactical style

DOCUMENTATION:
- AI_ALGORITHMS_DETAILED_DESCRIPTION.txt - This file
- README.md - Project overview
- API_INTEGRATION_GUIDE.md - Voice/Chat integration
- TRAINING_GUIDE.md - AI training instructions

================================================================================
================================================================================
                    AI QUORIDOR GAME PROJECT - THU·∫¨T TO√ÅN AI CHI TI·∫æT
================================================================================

D·ª± √°n: AI Quoridor Game v·ªõi Machine Learning v√† Multi-Platform Integration
Ng√†y t·∫°o: 25/07/2025
T√°c gi·∫£: [Student Name]
Platform: Unity 3D
Language: C#

================================================================================
                            M·ª§C L·ª§C
================================================================================

1. T·ªîNG QUAN KI·∫æN TR√öC AI
2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
4. SPECIALIZED AI STYLES SYSTEM
5. DECISION TREE CLASSIFIER
6. ENTROPY ANALYSIS & SOFTMAX
7. TRAINING SYSTEM & SELF-PLAY
8. AI TESTING FRAMEWORK
9. HYBRID AI ARCHITECTURE
10. PERFORMANCE OPTIMIZATION

================================================================================
                        1. T·ªîNG QUAN KI·∫æN TR√öC AI
================================================================================

D·ª± √°n AI Quoridor s·ª≠ d·ª•ng m·ªôt ki·∫øn tr√∫c AI hybrid ti√™n ti·∫øn, k·∫øt h·ª£p:

1.1 CORE ALGORITHMS:
- Q-Learning (Reinforcement Learning) - Thu·∫≠t to√°n ch√≠nh
- Minimax + Alpha-Beta Pruning - Thu·∫≠t to√°n fallback
- Decision Tree Classifier - Ph√¢n t√≠ch ng·ªØ c·∫£nh game
- Entropy Analysis - ƒê√°nh gi√° ƒëa d·∫°ng move

1.2 SPECIALIZED SYSTEMS:
- 6 AI Personality Styles v·ªõi training ri√™ng bi·ªát
- Self-Play Training Pipeline t·ª± ƒë·ªông
- Real-time Performance Testing Framework
- Dynamic Algorithm Switching

1.3 TECHNICAL FOUNDATION:
- State Space: 13,118+ unique game states
- Action Space: Movement + Wall placement combinations
- Training Method: Self-play v·ªõi 1000 episodes
- Learning Rate: Œ± = 0.1, Discount Factor: Œ≥ = 0.99

================================================================================
                    2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
================================================================================

2.1 GI·ªöI THI·ªÜU Q-LEARNING:
Q-Learning l√† thu·∫≠t to√°n Reinforcement Learning kh√¥ng c·∫ßn model, h·ªçc th√¥ng qua
t∆∞∆°ng t√°c v·ªõi m√¥i tr∆∞·ªùng ƒë·ªÉ t·ªëi ∆∞u h√≥a policy. Trong Quoridor, AI h·ªçc c√°ch
ch·ªçn moves t·ªëi ∆∞u d·ª±a tr√™n kinh nghi·ªám t·ª´ h√†ng ng√†n game.

2.2 C√îNG TH·ª®C TO√ÅN H·ªåC:
Q(s,a) = Q(s,a) + Œ±[r + Œ≥*max(Q(s',a')) - Q(s,a)]

Trong ƒë√≥:
- Q(s,a): Q-value c·ªßa state s v·ªõi action a
- Œ±: Learning rate (0.1) - t·ªëc ƒë·ªô h·ªçc
- r: Immediate reward t·ª´ action
- Œ≥: Discount factor (0.99) - t·∫ßm quan tr·ªçng future rewards
- s': Next state sau khi th·ª±c hi·ªán action
- max(Q(s',a')): Maximum Q-value c·ªßa next state

2.3 IMPLEMENTATION CHI TI·∫æT:

```csharp
public class QLearningAgent
{
    // Core Q-Table: Dictionary l∆∞u tr·ªØ Q-values
    private Dictionary<string, Dictionary<string, float>> qTable;
    
    // Hyperparameters
    private float learningRate = 0.1f;      // T·ªëc ƒë·ªô h·ªçc
    private float discountFactor = 0.99f;   // Discount future rewards
    private float epsilon = 0.995f;         // Exploration rate
    
    // Q-Learning Update Implementation
    public void UpdateQValue(string state, string action, float reward, string nextState)
    {
        // L·∫•y Q-value hi·ªán t·∫°i
        float currentQ = GetQValue(state, action);
        
        // T√¨m max Q-value c·ªßa next state
        float maxNextQ = GetMaxQValue(nextState);
        
        // √Åp d·ª•ng c√¥ng th·ª©c Q-Learning
        float newQ = currentQ + learningRate * (reward + discountFactor * maxNextQ - currentQ);
        
        // C·∫≠p nh·∫≠t Q-table
        SetQValue(state, action, newQ);
    }
}
```

2.4 STATE ENCODING:
Game state ƒë∆∞·ª£c encode th√†nh string duy nh·∫•t:
"aiPos_x,aiPos_y-humanPos_x,humanPos_y-aiWalls-humanWalls-wallsList"

V√≠ d·ª•: "4,8-4,0-9-10-7,4,False|5,6,True"
- AI ·ªü (4,8), Human ·ªü (4,0)
- AI c√≥ 9 walls, Human c√≥ 10 walls
- Walls ƒë√£ ƒë·∫∑t: (7,4,False), (5,6,True)

2.5 ACTION ENCODING:
Actions ƒë∆∞·ª£c encode th√†nh string:
- Movement: "M:x,y" (v√≠ d·ª•: "M:4,7")
- Wall: "W:x,y,horizontal" (v√≠ d·ª•: "W:3,5,True")

2.6 EPSILON-GREEDY STRATEGY:
```csharp
public AIMove ChooseAction(GameState state, List<AIMove> possibleMoves)
{
    // Exploration vs Exploitation trade-off
    if (Random.Range(0f, 1f) < epsilon)
    {
        // EXPLORATION: Ch·ªçn random ƒë·ªÉ kh√°m ph√°
        return possibleMoves[Random.Range(0, possibleMoves.Count)];
    }
    else
    {
        // EXPLOITATION: Ch·ªçn action c√≥ Q-value cao nh·∫•t
        return GetBestAction(state, possibleMoves);
    }
}
```

2.7 EPSILON DECAY:
Epsilon gi·∫£m d·∫ßn t·ª´ 0.995 ‚Üí 0.1 qua training:
- Ban ƒë·∫ßu: High exploration (99.5%)
- Cu·ªëi training: High exploitation (90%)

2.8 REWARD SYSTEM:
- Win game: +1000
- Lose game: -1000
- Move toward goal: +10
- Block opponent: +50
- Waste move: -5
- Strategic position: +15

================================================================================
                3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
================================================================================

3.1 GI·ªöI THI·ªÜU MINIMAX:
Minimax l√† thu·∫≠t to√°n game theory c·ªï ƒëi·ªÉn, t√¨m ki·∫øm move t·ªëi ∆∞u b·∫±ng c√°ch
d·ª± ƒëo√°n tr∆∞·ªõc nhi·ªÅu moves. Alpha-Beta pruning t·ªëi ∆∞u hi·ªáu su·∫•t b·∫±ng c√°ch
lo·∫°i b·ªè c√°c nh√°nh kh√¥ng c·∫ßn thi·∫øt.

3.2 NGUY√äN L√ù HO·∫†T ƒê·ªòNG:
- Maximizing Player (AI): T·ªëi ƒëa h√≥a score
- Minimizing Player (Human): T·ªëi thi·ªÉu h√≥a score AI
- Search depth: 4 levels (configurable)
- Pruning: Lo·∫°i b·ªè nh√°nh kh√¥ng promising

3.3 IMPLEMENTATION:

```csharp
float Minimax(GameState state, int depth, bool isMaximizing, float alpha, float beta)
{
    // Base case: Depth = 0 ho·∫∑c game over
    if (depth == 0 || IsGameOver(state))
        return EvaluatePosition(state);
    
    if (isMaximizing) // AI turn
    {
        float maxEval = float.NegativeInfinity;
        foreach (var move in GeneratePossibleMoves(state, true))
        {
            GameState newState = ApplyMove(state, move, true);
            float eval = Minimax(newState, depth - 1, false, alpha, beta);
            maxEval = Mathf.Max(maxEval, eval);
            alpha = Mathf.Max(alpha, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return maxEval;
    }
    else // Human turn
    {
        float minEval = float.PositiveInfinity;
        foreach (var move in GeneratePossibleMoves(state, false))
        {
            GameState newState = ApplyMove(state, move, false);
            float eval = Minimax(newState, depth - 1, true, alpha, beta);
            minEval = Mathf.Min(minEval, eval);
            beta = Mathf.Min(beta, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return minEval;
    }
}
```

3.4 EVALUATION FUNCTION:
Position evaluation d·ª±a tr√™n multiple factors:

```csharp
float EvaluatePosition(GameState state)
{
    float score = 0f;
    
    // Distance to goal (c√†ng g·∫ßn c√†ng t·ªët)
    int aiDistance = GetShortestPath(state.aiPosition, goalRow);
    int humanDistance = GetShortestPath(state.humanPosition, goalRow);
    score += (humanDistance - aiDistance) * 100;
    
    // Wall advantage
    score += (state.aiWallsLeft - state.humanWallsLeft) * 50;
    
    // Mobility (s·ªë moves c√≥ th·ªÉ)
    int aiMobility = GetValidMoves(state.aiPosition).Count;
    int humanMobility = GetValidMoves(state.humanPosition).Count;
    score += (aiMobility - humanMobility) * 20;
    
    // Center control
    score += GetCenterControlScore(state) * 30;
    
    return score;
}
```

3.5 ALPHA-BETA PRUNING HI·ªÜU QU·∫¢:
- Reduces search space t·ª´ O(b^d) xu·ªëng O(b^(d/2))
- Best case: 50% reduction in nodes evaluated
- Ordering moves by promising gi√∫p pruning hi·ªáu qu·∫£ h∆°n

================================================================================
                        4. SPECIALIZED AI STYLES SYSTEM
================================================================================

4.1 CONCEPT:
6 AI personalities v·ªõi training v√† behavior ri√™ng bi·ªát, m·ªói style c√≥:
- Ri√™ng Q-table file
- Custom reward structure
- Specific move prioritization
- Unique decision making logic

4.2 AI STYLES CHI TI·∫æT:

üî• AGGRESSIVE STYLE:
- M·ª•c ti√™u: T·∫•n c√¥ng v√† ch·∫∑n ƒë·ªëi th·ªß
- Wall Usage: 70% chance
- Reward Structure:
  * blockingOpponentReward = 50 * intensity
  * wallPlacementReward = 30 * intensity
  * defensiveWallPenalty = -20 * intensity
- Q-table: qtable_aggressive.json

üõ°Ô∏è DEFENSIVE STYLE:
- M·ª•c ti√™u: Ph√≤ng th·ªß v√† di chuy·ªÉn an to√†n
- Wall Usage: 50% chance
- Reward Structure:
  * defensiveWallReward = 35 * intensity
  * movementReward = 20 * intensity
  * aggressiveMovePenalty = -15 * intensity
- Q-table: qtable_defensive.json

üß± WALL MASTER STYLE:
- M·ª•c ti√™u: Chuy√™n gia ƒë·∫∑t t∆∞·ªùng v√† t·∫°o m√™ cung
- Wall Usage: 90% chance
- Reward Structure:
  * wallPlacementReward = 60 * intensity
  * strategicWallReward = 40 * intensity
  * wallComboReward = 50 * intensity
  * movementWithoutWallPenalty = -25 * intensity
- Q-table: qtable_wallmaster.json

‚ö° SPEED RUNNER STYLE:
- M·ª•c ti√™u: ƒê·∫°t ƒë√≠ch nhanh nh·∫•t
- Wall Usage: 10% chance
- Reward Structure:
  * movementReward = 40 * intensity
  * progressToGoalReward = 50 * intensity
  * quickWinBonus = 100 * intensity
  * wallPlacementPenalty = -20 * intensity
- Q-table: qtable_speedrunner.json

üß† TACTICAL STYLE:
- M·ª•c ti√™u: L·ª£i th·∫ø v·ªã tr√≠ v√† k·∫ø ho·∫°ch d√†i h·∫°n
- Wall Usage: Variable (strategic)
- Reward Structure:
  * tacticalMoveReward = 45 * intensity
  * positionAdvantageReward = 35 * intensity
  * mobilityReward = 25 * intensity
  * patternRecognitionBonus = 40 * intensity
- Q-table: qtable_tactical.json

‚öñÔ∏è BALANCED STYLE:
- M·ª•c ti√™u: C√¢n b·∫±ng t·∫•t c·∫£ aspects
- Wall Usage: 30% chance
- Reward Structure: Default balanced values
- Q-table: qtable.json

4.3 STYLE SWITCHING MECHANISM:

```csharp
public void SwitchToStyle(PlayStyle newStyle)
{
    // Save current Q-table
    if (allowQTableSaving) qAgent.SaveQTable(GetStyleSpecificQTablePath());
    
    // Load new style Q-table
    specializedStyle = newStyle;
    qAgent.LoadQTable(GetStyleSpecificQTablePath());
    
    // Reconfigure parameters
    ConfigureStyleSpecificTraining();
}
```

4.4 STYLE INTENSITY:
Parameter t·ª´ 0.0 ‚Üí 1.0 control m·ª©c ƒë·ªô chuy√™n h√≥a:
- 0.0: Balanced behavior
- 0.5: Moderate specialization
- 1.0: Extreme specialization

================================================================================
                        5. DECISION TREE CLASSIFIER
================================================================================

5.1 PURPOSE:
Decision Tree ph√¢n t√≠ch game context ƒë·ªÉ ch·ªçn strategy ph√π h·ª£p, d·ª±a tr√™n:
- Game phase (Early/Mid/End)
- Position advantage
- Resource advantage
- Mobility situation

5.2 FEATURE EXTRACTION:

```csharp
public struct DecisionFeatures
{
    public int aiDistanceToGoal;      // AI distance to goal line
    public int humanDistanceToGoal;   // Human distance to goal line
    public int aiWallsLeft;           // AI walls remaining
    public int humanWallsLeft;        // Human walls remaining
    public int aiMobility;            // AI possible moves
    public int humanMobility;         // Human possible moves
    public float distanceAdvantage;   // Human - AI distance
    public float wallAdvantage;       // AI - Human walls
    public bool isEarlyGame;          // > 15 moves estimated remaining
    public bool isMidGame;            // 5-15 moves remaining
    public bool isEndGame;            // < 5 moves remaining
}
```

5.3 DECISION LOGIC:

```csharp
public AIStrategy Predict(DecisionFeatures features)
{
    // Early Game: Play conservatively
    if (features.isEarlyGame)
        return AIStrategy.Balanced;
    
    // AI has significant lead: Focus on blocking
    if (features.distanceAdvantage > 2)
        return AIStrategy.Blocking;
    
    // AI is behind: Be aggressive
    if (features.distanceAdvantage < -1)
        return AIStrategy.Aggressive;
    
    // End game with wall advantage: Play defensively
    if (features.isEndGame && features.wallAdvantage > 2)
        return AIStrategy.Defensive;
    
    // Close game: Balanced approach
    return AIStrategy.Balanced;
}
```

5.4 STRATEGY OUTCOMES:
- Balanced: Equal weight to movement v√† walls
- Blocking: Prioritize moves that increase opponent path
- Aggressive: Focus on attacking positions
- Defensive: Protect own position and path

================================================================================
                        6. ENTROPY ANALYSIS & SOFTMAX
================================================================================

6.1 ENTROPY CONCEPT:
Entropy ƒëo ƒë·ªô ƒëa d·∫°ng/uncertainty c·ªßa move scores. High entropy = nhi·ªÅu moves
c√≥ scores t∆∞∆°ng ƒë∆∞∆°ng, low entropy = m·ªôt move r√µ r√†ng t·ªët nh·∫•t.

6.2 ENTROPY CALCULATION:

```csharp
float CalculateEntropy(List<float> moveScores)
{
    // Normalize th√†nh probabilities
    float total = moveScores.Sum();
    var probabilities = moveScores.Select(s => s / total);
    
    // Calculate Shannon entropy
    float entropy = 0f;
    foreach (float p in probabilities)
    {
        if (p > 0f)
            entropy -= p * Mathf.Log(p, 2f);
    }
    
    return entropy; // Range: 0 (deterministic) to log2(n) (uniform)
}
```

6.3 SOFTMAX DISTRIBUTION:

```csharp
float[] CalculateSoftmax(List<float> scores, float temperature)
{
    float[] probabilities = new float[scores.Count];
    float sum = scores.Sum(s => Mathf.Exp(s / temperature));
    
    for (int i = 0; i < scores.Count; i++)
    {
        probabilities[i] = Mathf.Exp(scores[i] / temperature) / sum;
    }
    
    return probabilities;
}
```

6.4 TEMPERATURE PARAMETER:
- Low temperature (0.1): Sharp distribution, focus on best moves
- High temperature (2.0): Flat distribution, more exploration
- Default temperature (1.0): Balanced exploitation/exploration

6.5 ADAPTIVE MOVE SELECTION:

```csharp
public AIMove ChooseActionWithEntropy(GameState state, List<AIMove> possibleMoves)
{
    var moveScores = EvaluateAllMoves(state, possibleMoves);
    float entropy = CalculateEntropy(moveScores);
    
    if (entropy > entropyThreshold) // High diversity
    {
        // Use softmax for probabilistic selection
        var probabilities = CalculateSoftmax(moveScores, softmaxTemperature);
        return SelectMoveByProbability(possibleMoves, probabilities);
    }
    else // Clear best move
    {
        // Use deterministic best move
        return GetBestScoredMove(possibleMoves, moveScores);
    }
}
```

================================================================================
                        7. TRAINING SYSTEM & SELF-PLAY
================================================================================

7.1 SELF-PLAY CONCEPT:
AI h·ªçc b·∫±ng c√°ch ch∆°i v·ªõi ch√≠nh n√≥ h√†ng ng√†n l·∫ßn, gradually improve performance
th√¥ng qua trial v√† error.

7.2 TRAINING PIPELINE:

```csharp
IEnumerator RunSelfPlayTraining()
{
    for (int episode = 1; episode <= maxEpisodes; episode++)
    {
        // Reset game state
        ResetGameForTraining();
        
        while (!IsGameOver())
        {
            // Get current state
            GameState currentState = GetCurrentGameState();
            List<AIMove> possibleMoves = GetPossibleMoves();
            
            // Choose action using Œµ-greedy
            AIMove chosenMove = qLearningAgent.ChooseAction(currentState, possibleMoves);
            
            // Execute move
            ExecuteMove(chosenMove);
            
            // Calculate reward
            float reward = CalculateReward(currentState, chosenMove);
            
            // Update Q-table
            GameState newState = GetCurrentGameState();
            qLearningAgent.UpdateQValue(
                EncodeState(currentState),
                EncodeAction(chosenMove),
                reward,
                EncodeState(newState)
            );
            
            yield return new WaitForEndOfFrame();
        }
        
        // Epsilon decay
        qLearningAgent.DecayEpsilon();
        
        // Save progress periodically
        if (episode % 100 == 0)
        {
            qLearningAgent.SaveQTable();
        }
    }
}
```

7.3 REWARD ENGINEERING:
Critical cho Q-Learning success:

```csharp
float CalculateReward(GameState state, AIMove move)
{
    float reward = 0f;
    
    // Game outcome rewards
    if (IsWin(state)) reward += 1000f;
    if (IsLoss(state)) reward -= 1000f;
    
    // Progress rewards
    if (move.moveType == MoveType.Movement)
    {
        if (IsMoveTowardGoal(move)) reward += 20f;
        else reward -= 5f;
    }
    
    // Strategic rewards
    if (move.moveType == MoveType.WallPlacement)
    {
        if (IsWallBlockingOpponent(move)) reward += 50f;
        if (IsWallStrategic(move)) reward += 30f;
        if (IsWallWasted(move)) reward -= 25f;
    }
    
    // Style-specific rewards
    reward += GetStyleSpecificReward(state, move);
    
    return reward;
}
```

7.4 TRAINING METRICS:
- Episodes: 1000 standard
- Duration: ~15 minutes
- Convergence: Usually around episode 800
- Q-table size: 13,118+ states at completion
- Final epsilon: 0.1 (90% exploitation)

================================================================================
                        8. AI TESTING FRAMEWORK
================================================================================

8.1 TESTING PHILOSOPHY:
Comprehensive testing ensures AI quality v√† reliability across different
game scenarios v√† edge cases.

8.2 TEST CATEGORIES:

A) DECISION TREE TESTS:
```csharp
void TestDecisionTreeStrategies()
{
    // Test Early Game
    var earlyGameFeatures = new DecisionFeatures
    {
        aiDistanceToGoal = 8, humanDistanceToGoal = 8,
        isEarlyGame = true
    };
    var strategy = decisionTree.Predict(earlyGameFeatures);
    Assert.AreEqual(AIStrategy.Balanced, strategy);
    
    // Test AI Leading
    var leadingFeatures = new DecisionFeatures
    {
        distanceAdvantage = 3, // AI leading
        isMidGame = true
    };
    strategy = decisionTree.Predict(leadingFeatures);
    Assert.AreEqual(AIStrategy.Blocking, strategy);
}
```

B) ENTROPY ANALYSIS TESTS:
```csharp
void TestEntropyAnalysis()
{
    // High entropy case (diverse scores)
    var highEntropyScores = new List<float> { 100f, 98f, 97f, 95f, 94f };
    float entropy = CalculateTestEntropy(highEntropyScores);
    Assert.IsTrue(entropy > 0.5f);
    
    // Low entropy case (clear winner)
    var lowEntropyScores = new List<float> { 100f, 50f, 50f, 50f, 50f };
    entropy = CalculateTestEntropy(lowEntropyScores);
    Assert.IsTrue(entropy < 0.5f);
}
```

C) SOFTMAX DISTRIBUTION TESTS:
```csharp
void TestSoftmaxDistribution()
{
    var testScores = new List<float> { 100f, 98f, 95f, 90f };
    float temperature = 1.0f;
    
    var probabilities = CalculateSoftmax(testScores, temperature);
    
    // Verify probabilities sum to 1
    float sum = probabilities.Sum();
    Assert.AreEqual(1.0f, sum, 0.001f);
    
    // Verify highest score gets highest probability
    int maxIndex = Array.IndexOf(probabilities, probabilities.Max());
    Assert.AreEqual(0, maxIndex);
}
```

D) PERFORMANCE BENCHMARKS:
- Move generation time: <100ms
- Q-Learning decision: <50ms
- Minimax decision (depth 4): <200ms
- Memory usage: <256MB

8.3 AUTOMATED TESTING:
Tests run automatically ƒë·ªÉ ensure code quality:
- Unit tests cho individual algorithms
- Integration tests cho AI system interactions
- Performance tests cho response times
- Regression tests ƒë·ªÉ prevent quality degradation

================================================================================
                        9. HYBRID AI ARCHITECTURE
================================================================================

9.1 CONCEPT:
Hybrid system k·∫øt h·ª£p strengths c·ªßa multiple algorithms:
- Q-Learning: Data-driven, learns from experience
- Minimax: Logical, guaranteed optimal given evaluation
- Decision Tree: Fast, context-aware switching
- Entropy: Adaptive, handles uncertainty

9.2 ALGORITHM SELECTION LOGIC:

```csharp
public AIMove GetBestMove()
{
    UpdateGameState();
    List<AIMove> possibleMoves = GeneratePossibleMoves();
    
    // Primary: Use Q-Learning if trained model available
    if (useQLearning && qAgent.IsTrainedModel())
    {
        Debug.Log($"üß† Using {specializedStyle} Q-Learning");
        return qAgent.ChooseAction(currentGameState, possibleMoves);
    }
    
    // Fallback: Use Minimax for guaranteed reasonable play
    else
    {
        Debug.Log($"üîç Using {specializedStyle} Minimax");
        return FindBestMoveWithMinimax();
    }
}
```

9.3 SEAMLESS SWITCHING:
- No interruption in gameplay
- Maintains consistent API
- Preserves game state integrity
- Logs algorithm used for debugging

9.4 BENEFITS:
- Robustness: Fallback ensures AI never fails
- Performance: Q-Learning for trained scenarios, Minimax for edge cases
- Flexibility: Can switch algorithms based on situation
- Development: Easy to test individual components

================================================================================
                        10. PERFORMANCE OPTIMIZATION
================================================================================

10.1 Q-TABLE OPTIMIZATION:
- Sparse representation: Only store visited states
- Compression: String interning for state keys
- Lazy loading: Load Q-table on demand
- Periodic saving: Every 100 training episodes

10.2 MINIMAX OPTIMIZATION:
- Alpha-Beta pruning: 50% search reduction
- Move ordering: Promising moves first
- Iterative deepening: Gradual depth increase
- Transposition tables: Cache evaluated positions

10.3 MEMORY MANAGEMENT:
- Object pooling cho GameState objects
- String caching cho repeated state/action encodings
- Garbage collection optimization
- Memory profiling v√† monitoring

10.4 MULTITHREADING CONSIDERATIONS:
- Q-Learning training: Background thread
- Move calculation: Main thread (Unity requirement)
- File I/O: Async operations where possible
- Thread-safe collections cho shared data

10.5 CACHING STRATEGIES:
- Path finding results
- Move generation results
- Evaluation function results
- Feature extraction results

================================================================================
                            K·∫æT LU·∫¨N
================================================================================

D·ª± √°n AI Quoridor Project th·ªÉ hi·ªán m·ªôt implementation comprehensive c·ªßa modern
game AI techniques:

TECHNICAL INNOVATIONS:
‚úÖ Hybrid Q-Learning + Minimax architecture
‚úÖ 6 specialized AI personalities v·ªõi separate training
‚úÖ Advanced decision making v·ªõi entropy analysis
‚úÖ Comprehensive testing v√† validation framework
‚úÖ Professional development tools v√† debugging

EDUCATIONAL VALUE:
‚úÖ Practical machine learning implementation
‚úÖ Traditional game AI algorithms
‚úÖ Modern software engineering practices
‚úÖ Performance optimization techniques
‚úÖ Automated testing methodologies

FUTURE ENHANCEMENTS:
üöÄ Deep Q-Learning (DQN) integration
üöÄ Multi-agent reinforcement learning
üöÄ Neural network-based evaluation functions
üöÄ Online learning v√† adaptation
üöÄ Transfer learning between game variants

D·ª± √°n n√†y demonstrating successful integration c·ªßa classical AI methods v·ªõi
modern machine learning, t·∫°o ra m·ªôt intelligent, adaptive, v√† highly
interactive gaming experience.

================================================================================
                            APPENDIX
================================================================================

FILES LI√äN QUAN:
- Assets/scripts/QLearningAgent.cs - Q-Learning implementation
- Assets/scripts/QuoridorAI.cs - Main AI controller
- Assets/scripts/DecisionTreeClassifier.cs - Decision tree logic
- Assets/scripts/QuoridorSelfPlayTrainer.cs - Training system
- Assets/scripts/AITestScript.cs - Testing framework
- Assets/scripts/Editor/ForceRetrainAI.cs - Development tools

Q-TABLE FILES:
- StreamingAssets/qtable.json - Balanced style
- StreamingAssets/qtable_aggressive.json - Aggressive style
- StreamingAssets/qtable_defensive.json - Defensive style
- StreamingAssets/qtable_wallmaster.json - Wall Master style
- StreamingAssets/qtable_speedrunner.json - Speed Runner style
- StreamingAssets/qtable_tactical.json - Tactical style

DOCUMENTATION:
- AI_ALGORITHMS_DETAILED_DESCRIPTION.txt - This file
- README.md - Project overview
- API_INTEGRATION_GUIDE.md - Voice/Chat integration
- TRAINING_GUIDE.md - AI training instructions

================================================================================
================================================================================
                    AI QUORIDOR GAME PROJECT - THU·∫¨T TO√ÅN AI CHI TI·∫æT
================================================================================

D·ª± √°n: AI Quoridor Game v·ªõi Machine Learning v√† Multi-Platform Integration
Ng√†y t·∫°o: 25/07/2025
T√°c gi·∫£: [Student Name]
Platform: Unity 3D
Language: C#

================================================================================
                            M·ª§C L·ª§C
================================================================================

1. T·ªîNG QUAN KI·∫æN TR√öC AI
2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
4. SPECIALIZED AI STYLES SYSTEM
5. DECISION TREE CLASSIFIER
6. ENTROPY ANALYSIS & SOFTMAX
7. TRAINING SYSTEM & SELF-PLAY
8. AI TESTING FRAMEWORK
9. HYBRID AI ARCHITECTURE
10. PERFORMANCE OPTIMIZATION

================================================================================
                        1. T·ªîNG QUAN KI·∫æN TR√öC AI
================================================================================

D·ª± √°n AI Quoridor s·ª≠ d·ª•ng m·ªôt ki·∫øn tr√∫c AI hybrid ti√™n ti·∫øn, k·∫øt h·ª£p:

1.1 CORE ALGORITHMS:
- Q-Learning (Reinforcement Learning) - Thu·∫≠t to√°n ch√≠nh
- Minimax + Alpha-Beta Pruning - Thu·∫≠t to√°n fallback
- Decision Tree Classifier - Ph√¢n t√≠ch ng·ªØ c·∫£nh game
- Entropy Analysis - ƒê√°nh gi√° ƒëa d·∫°ng move

1.2 SPECIALIZED SYSTEMS:
- 6 AI Personality Styles v·ªõi training ri√™ng bi·ªát
- Self-Play Training Pipeline t·ª± ƒë·ªông
- Real-time Performance Testing Framework
- Dynamic Algorithm Switching

1.3 TECHNICAL FOUNDATION:
- State Space: 13,118+ unique game states
- Action Space: Movement + Wall placement combinations
- Training Method: Self-play v·ªõi 1000 episodes
- Learning Rate: Œ± = 0.1, Discount Factor: Œ≥ = 0.99

================================================================================
                    2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
================================================================================

2.1 GI·ªöI THI·ªÜU Q-LEARNING:
Q-Learning l√† thu·∫≠t to√°n Reinforcement Learning kh√¥ng c·∫ßn model, h·ªçc th√¥ng qua
t∆∞∆°ng t√°c v·ªõi m√¥i tr∆∞·ªùng ƒë·ªÉ t·ªëi ∆∞u h√≥a policy. Trong Quoridor, AI h·ªçc c√°ch
ch·ªçn moves t·ªëi ∆∞u d·ª±a tr√™n kinh nghi·ªám t·ª´ h√†ng ng√†n game.

2.2 C√îNG TH·ª®C TO√ÅN H·ªåC:
Q(s,a) = Q(s,a) + Œ±[r + Œ≥*max(Q(s',a')) - Q(s,a)]

Trong ƒë√≥:
- Q(s,a): Q-value c·ªßa state s v·ªõi action a
- Œ±: Learning rate (0.1) - t·ªëc ƒë·ªô h·ªçc
- r: Immediate reward t·ª´ action
- Œ≥: Discount factor (0.99) - t·∫ßm quan tr·ªçng future rewards
- s': Next state sau khi th·ª±c hi·ªán action
- max(Q(s',a')): Maximum Q-value c·ªßa next state

2.3 IMPLEMENTATION CHI TI·∫æT:

```csharp
public class QLearningAgent
{
    // Core Q-Table: Dictionary l∆∞u tr·ªØ Q-values
    private Dictionary<string, Dictionary<string, float>> qTable;
    
    // Hyperparameters
    private float learningRate = 0.1f;      // T·ªëc ƒë·ªô h·ªçc
    private float discountFactor = 0.99f;   // Discount future rewards
    private float epsilon = 0.995f;         // Exploration rate
    
    // Q-Learning Update Implementation
    public void UpdateQValue(string state, string action, float reward, string nextState)
    {
        // L·∫•y Q-value hi·ªán t·∫°i
        float currentQ = GetQValue(state, action);
        
        // T√¨m max Q-value c·ªßa next state
        float maxNextQ = GetMaxQValue(nextState);
        
        // √Åp d·ª•ng c√¥ng th·ª©c Q-Learning
        float newQ = currentQ + learningRate * (reward + discountFactor * maxNextQ - currentQ);
        
        // C·∫≠p nh·∫≠t Q-table
        SetQValue(state, action, newQ);
    }
}
```

2.4 STATE ENCODING:
Game state ƒë∆∞·ª£c encode th√†nh string duy nh·∫•t:
"aiPos_x,aiPos_y-humanPos_x,humanPos_y-aiWalls-humanWalls-wallsList"

V√≠ d·ª•: "4,8-4,0-9-10-7,4,False|5,6,True"
- AI ·ªü (4,8), Human ·ªü (4,0)
- AI c√≥ 9 walls, Human c√≥ 10 walls
- Walls ƒë√£ ƒë·∫∑t: (7,4,False), (5,6,True)

2.5 ACTION ENCODING:
Actions ƒë∆∞·ª£c encode th√†nh string:
- Movement: "M:x,y" (v√≠ d·ª•: "M:4,7")
- Wall: "W:x,y,horizontal" (v√≠ d·ª•: "W:3,5,True")

2.6 EPSILON-GREEDY STRATEGY:
```csharp
public AIMove ChooseAction(GameState state, List<AIMove> possibleMoves)
{
    // Exploration vs Exploitation trade-off
    if (Random.Range(0f, 1f) < epsilon)
    {
        // EXPLORATION: Ch·ªçn random ƒë·ªÉ kh√°m ph√°
        return possibleMoves[Random.Range(0, possibleMoves.Count)];
    }
    else
    {
        // EXPLOITATION: Ch·ªçn action c√≥ Q-value cao nh·∫•t
        return GetBestAction(state, possibleMoves);
    }
}
```

2.7 EPSILON DECAY:
Epsilon gi·∫£m d·∫ßn t·ª´ 0.995 ‚Üí 0.1 qua training:
- Ban ƒë·∫ßu: High exploration (99.5%)
- Cu·ªëi training: High exploitation (90%)

2.8 REWARD SYSTEM:
- Win game: +1000
- Lose game: -1000
- Move toward goal: +10
- Block opponent: +50
- Waste move: -5
- Strategic position: +15

================================================================================
                3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
================================================================================

3.1 GI·ªöI THI·ªÜU MINIMAX:
Minimax l√† thu·∫≠t to√°n game theory c·ªï ƒëi·ªÉn, t√¨m ki·∫øm move t·ªëi ∆∞u b·∫±ng c√°ch
d·ª± ƒëo√°n tr∆∞·ªõc nhi·ªÅu moves. Alpha-Beta pruning t·ªëi ∆∞u hi·ªáu su·∫•t b·∫±ng c√°ch
lo·∫°i b·ªè c√°c nh√°nh kh√¥ng c·∫ßn thi·∫øt.

3.2 NGUY√äN L√ù HO·∫†T ƒê·ªòNG:
- Maximizing Player (AI): T·ªëi ƒëa h√≥a score
- Minimizing Player (Human): T·ªëi thi·ªÉu h√≥a score AI
- Search depth: 4 levels (configurable)
- Pruning: Lo·∫°i b·ªè nh√°nh kh√¥ng promising

3.3 IMPLEMENTATION:

```csharp
float Minimax(GameState state, int depth, bool isMaximizing, float alpha, float beta)
{
    // Base case: Depth = 0 ho·∫∑c game over
    if (depth == 0 || IsGameOver(state))
        return EvaluatePosition(state);
    
    if (isMaximizing) // AI turn
    {
        float maxEval = float.NegativeInfinity;
        foreach (var move in GeneratePossibleMoves(state, true))
        {
            GameState newState = ApplyMove(state, move, true);
            float eval = Minimax(newState, depth - 1, false, alpha, beta);
            maxEval = Mathf.Max(maxEval, eval);
            alpha = Mathf.Max(alpha, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return maxEval;
    }
    else // Human turn
    {
        float minEval = float.PositiveInfinity;
        foreach (var move in GeneratePossibleMoves(state, false))
        {
            GameState newState = ApplyMove(state, move, false);
            float eval = Minimax(newState, depth - 1, true, alpha, beta);
            minEval = Mathf.Min(minEval, eval);
            beta = Mathf.Min(beta, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return minEval;
    }
}
```

3.4 EVALUATION FUNCTION:
Position evaluation d·ª±a tr√™n multiple factors:

```csharp
float EvaluatePosition(GameState state)
{
    float score = 0f;
    
    // Distance to goal (c√†ng g·∫ßn c√†ng t·ªët)
    int aiDistance = GetShortestPath(state.aiPosition, goalRow);
    int humanDistance = GetShortestPath(state.humanPosition, goalRow);
    score += (humanDistance - aiDistance) * 100;
    
    // Wall advantage
    score += (state.aiWallsLeft - state.humanWallsLeft) * 50;
    
    // Mobility (s·ªë moves c√≥ th·ªÉ)
    int aiMobility = GetValidMoves(state.aiPosition).Count;
    int humanMobility = GetValidMoves(state.humanPosition).Count;
    score += (aiMobility - humanMobility) * 20;
    
    // Center control
    score += GetCenterControlScore(state) * 30;
    
    return score;
}
```

3.5 ALPHA-BETA PRUNING HI·ªÜU QU·∫¢:
- Reduces search space t·ª´ O(b^d) xu·ªëng O(b^(d/2))
- Best case: 50% reduction in nodes evaluated
- Ordering moves by promising gi√∫p pruning hi·ªáu qu·∫£ h∆°n

================================================================================
                        4. SPECIALIZED AI STYLES SYSTEM
================================================================================

4.1 CONCEPT:
6 AI personalities v·ªõi training v√† behavior ri√™ng bi·ªát, m·ªói style c√≥:
- Ri√™ng Q-table file
- Custom reward structure
- Specific move prioritization
- Unique decision making logic

4.2 AI STYLES CHI TI·∫æT:

üî• AGGRESSIVE STYLE:
- M·ª•c ti√™u: T·∫•n c√¥ng v√† ch·∫∑n ƒë·ªëi th·ªß
- Wall Usage: 70% chance
- Reward Structure:
  * blockingOpponentReward = 50 * intensity
  * wallPlacementReward = 30 * intensity
  * defensiveWallPenalty = -20 * intensity
- Q-table: qtable_aggressive.json

üõ°Ô∏è DEFENSIVE STYLE:
- M·ª•c ti√™u: Ph√≤ng th·ªß v√† di chuy·ªÉn an to√†n
- Wall Usage: 50% chance
- Reward Structure:
  * defensiveWallReward = 35 * intensity
  * movementReward = 20 * intensity
  * aggressiveMovePenalty = -15 * intensity
- Q-table: qtable_defensive.json

üß± WALL MASTER STYLE:
- M·ª•c ti√™u: Chuy√™n gia ƒë·∫∑t t∆∞·ªùng v√† t·∫°o m√™ cung
- Wall Usage: 90% chance
- Reward Structure:
  * wallPlacementReward = 60 * intensity
  * strategicWallReward = 40 * intensity
  * wallComboReward = 50 * intensity
  * movementWithoutWallPenalty = -25 * intensity
- Q-table: qtable_wallmaster.json

‚ö° SPEED RUNNER STYLE:
- M·ª•c ti√™u: ƒê·∫°t ƒë√≠ch nhanh nh·∫•t
- Wall Usage: 10% chance
- Reward Structure:
  * movementReward = 40 * intensity
  * progressToGoalReward = 50 * intensity
  * quickWinBonus = 100 * intensity
  * wallPlacementPenalty = -20 * intensity
- Q-table: qtable_speedrunner.json

üß† TACTICAL STYLE:
- M·ª•c ti√™u: L·ª£i th·∫ø v·ªã tr√≠ v√† k·∫ø ho·∫°ch d√†i h·∫°n
- Wall Usage: Variable (strategic)
- Reward Structure:
  * tacticalMoveReward = 45 * intensity
  * positionAdvantageReward = 35 * intensity
  * mobilityReward = 25 * intensity
  * patternRecognitionBonus = 40 * intensity
- Q-table: qtable_tactical.json

‚öñÔ∏è BALANCED STYLE:
- M·ª•c ti√™u: C√¢n b·∫±ng t·∫•t c·∫£ aspects
- Wall Usage: 30% chance
- Reward Structure: Default balanced values
- Q-table: qtable.json

4.3 STYLE SWITCHING MECHANISM:

```csharp
public void SwitchToStyle(PlayStyle newStyle)
{
    // Save current Q-table
    if (allowQTableSaving) qAgent.SaveQTable(GetStyleSpecificQTablePath());
    
    // Load new style Q-table
    specializedStyle = newStyle;
    qAgent.LoadQTable(GetStyleSpecificQTablePath());
    
    // Reconfigure parameters
    ConfigureStyleSpecificTraining();
}
```

4.4 STYLE INTENSITY:
Parameter t·ª´ 0.0 ‚Üí 1.0 control m·ª©c ƒë·ªô chuy√™n h√≥a:
- 0.0: Balanced behavior
- 0.5: Moderate specialization
- 1.0: Extreme specialization

================================================================================
                        5. DECISION TREE CLASSIFIER
================================================================================

5.1 PURPOSE:
Decision Tree ph√¢n t√≠ch game context ƒë·ªÉ ch·ªçn strategy ph√π h·ª£p, d·ª±a tr√™n:
- Game phase (Early/Mid/End)
- Position advantage
- Resource advantage
- Mobility situation

5.2 FEATURE EXTRACTION:

```csharp
public struct DecisionFeatures
{
    public int aiDistanceToGoal;      // AI distance to goal line
    public int humanDistanceToGoal;   // Human distance to goal line
    public int aiWallsLeft;           // AI walls remaining
    public int humanWallsLeft;        // Human walls remaining
    public int aiMobility;            // AI possible moves
    public int humanMobility;         // Human possible moves
    public float distanceAdvantage;   // Human - AI distance
    public float wallAdvantage;       // AI - Human walls
    public bool isEarlyGame;          // > 15 moves estimated remaining
    public bool isMidGame;            // 5-15 moves remaining
    public bool isEndGame;            // < 5 moves remaining
}
```

5.3 DECISION LOGIC:

```csharp
public AIStrategy Predict(DecisionFeatures features)
{
    // Early Game: Play conservatively
    if (features.isEarlyGame)
        return AIStrategy.Balanced;
    
    // AI has significant lead: Focus on blocking
    if (features.distanceAdvantage > 2)
        return AIStrategy.Blocking;
    
    // AI is behind: Be aggressive
    if (features.distanceAdvantage < -1)
        return AIStrategy.Aggressive;
    
    // End game with wall advantage: Play defensively
    if (features.isEndGame && features.wallAdvantage > 2)
        return AIStrategy.Defensive;
    
    // Close game: Balanced approach
    return AIStrategy.Balanced;
}
```

5.4 STRATEGY OUTCOMES:
- Balanced: Equal weight to movement v√† walls
- Blocking: Prioritize moves that increase opponent path
- Aggressive: Focus on attacking positions
- Defensive: Protect own position and path

================================================================================
                        6. ENTROPY ANALYSIS & SOFTMAX
================================================================================

6.1 ENTROPY CONCEPT:
Entropy ƒëo ƒë·ªô ƒëa d·∫°ng/uncertainty c·ªßa move scores. High entropy = nhi·ªÅu moves
c√≥ scores t∆∞∆°ng ƒë∆∞∆°ng, low entropy = m·ªôt move r√µ r√†ng t·ªët nh·∫•t.

6.2 ENTROPY CALCULATION:

```csharp
float CalculateEntropy(List<float> moveScores)
{
    // Normalize th√†nh probabilities
    float total = moveScores.Sum();
    var probabilities = moveScores.Select(s => s / total);
    
    // Calculate Shannon entropy
    float entropy = 0f;
    foreach (float p in probabilities)
    {
        if (p > 0f)
            entropy -= p * Mathf.Log(p, 2f);
    }
    
    return entropy; // Range: 0 (deterministic) to log2(n) (uniform)
}
```

6.3 SOFTMAX DISTRIBUTION:

```csharp
float[] CalculateSoftmax(List<float> scores, float temperature)
{
    float[] probabilities = new float[scores.Count];
    float sum = scores.Sum(s => Mathf.Exp(s / temperature));
    
    for (int i = 0; i < scores.Count; i++)
    {
        probabilities[i] = Mathf.Exp(scores[i] / temperature) / sum;
    }
    
    return probabilities;
}
```

6.4 TEMPERATURE PARAMETER:
- Low temperature (0.1): Sharp distribution, focus on best moves
- High temperature (2.0): Flat distribution, more exploration
- Default temperature (1.0): Balanced exploitation/exploration

6.5 ADAPTIVE MOVE SELECTION:

```csharp
public AIMove ChooseActionWithEntropy(GameState state, List<AIMove> possibleMoves)
{
    var moveScores = EvaluateAllMoves(state, possibleMoves);
    float entropy = CalculateEntropy(moveScores);
    
    if (entropy > entropyThreshold) // High diversity
    {
        // Use softmax for probabilistic selection
        var probabilities = CalculateSoftmax(moveScores, softmaxTemperature);
        return SelectMoveByProbability(possibleMoves, probabilities);
    }
    else // Clear best move
    {
        // Use deterministic best move
        return GetBestScoredMove(possibleMoves, moveScores);
    }
}
```

================================================================================
                        7. TRAINING SYSTEM & SELF-PLAY
================================================================================

7.1 SELF-PLAY CONCEPT:
AI h·ªçc b·∫±ng c√°ch ch∆°i v·ªõi ch√≠nh n√≥ h√†ng ng√†n l·∫ßn, gradually improve performance
th√¥ng qua trial v√† error.

7.2 TRAINING PIPELINE:

```csharp
IEnumerator RunSelfPlayTraining()
{
    for (int episode = 1; episode <= maxEpisodes; episode++)
    {
        // Reset game state
        ResetGameForTraining();
        
        while (!IsGameOver())
        {
            // Get current state
            GameState currentState = GetCurrentGameState();
            List<AIMove> possibleMoves = GetPossibleMoves();
            
            // Choose action using Œµ-greedy
            AIMove chosenMove = qLearningAgent.ChooseAction(currentState, possibleMoves);
            
            // Execute move
            ExecuteMove(chosenMove);
            
            // Calculate reward
            float reward = CalculateReward(currentState, chosenMove);
            
            // Update Q-table
            GameState newState = GetCurrentGameState();
            qLearningAgent.UpdateQValue(
                EncodeState(currentState),
                EncodeAction(chosenMove),
                reward,
                EncodeState(newState)
            );
            
            yield return new WaitForEndOfFrame();
        }
        
        // Epsilon decay
        qLearningAgent.DecayEpsilon();
        
        // Save progress periodically
        if (episode % 100 == 0)
        {
            qLearningAgent.SaveQTable();
        }
    }
}
```

7.3 REWARD ENGINEERING:
Critical cho Q-Learning success:

```csharp
float CalculateReward(GameState state, AIMove move)
{
    float reward = 0f;
    
    // Game outcome rewards
    if (IsWin(state)) reward += 1000f;
    if (IsLoss(state)) reward -= 1000f;
    
    // Progress rewards
    if (move.moveType == MoveType.Movement)
    {
        if (IsMoveTowardGoal(move)) reward += 20f;
        else reward -= 5f;
    }
    
    // Strategic rewards
    if (move.moveType == MoveType.WallPlacement)
    {
        if (IsWallBlockingOpponent(move)) reward += 50f;
        if (IsWallStrategic(move)) reward += 30f;
        if (IsWallWasted(move)) reward -= 25f;
    }
    
    // Style-specific rewards
    reward += GetStyleSpecificReward(state, move);
    
    return reward;
}
```

7.4 TRAINING METRICS:
- Episodes: 1000 standard
- Duration: ~15 minutes
- Convergence: Usually around episode 800
- Q-table size: 13,118+ states at completion
- Final epsilon: 0.1 (90% exploitation)

================================================================================
                        8. AI TESTING FRAMEWORK
================================================================================

8.1 TESTING PHILOSOPHY:
Comprehensive testing ensures AI quality v√† reliability across different
game scenarios v√† edge cases.

8.2 TEST CATEGORIES:

A) DECISION TREE TESTS:
```csharp
void TestDecisionTreeStrategies()
{
    // Test Early Game
    var earlyGameFeatures = new DecisionFeatures
    {
        aiDistanceToGoal = 8, humanDistanceToGoal = 8,
        isEarlyGame = true
    };
    var strategy = decisionTree.Predict(earlyGameFeatures);
    Assert.AreEqual(AIStrategy.Balanced, strategy);
    
    // Test AI Leading
    var leadingFeatures = new DecisionFeatures
    {
        distanceAdvantage = 3, // AI leading
        isMidGame = true
    };
    strategy = decisionTree.Predict(leadingFeatures);
    Assert.AreEqual(AIStrategy.Blocking, strategy);
}
```

B) ENTROPY ANALYSIS TESTS:
```csharp
void TestEntropyAnalysis()
{
    // High entropy case (diverse scores)
    var highEntropyScores = new List<float> { 100f, 98f, 97f, 95f, 94f };
    float entropy = CalculateTestEntropy(highEntropyScores);
    Assert.IsTrue(entropy > 0.5f);
    
    // Low entropy case (clear winner)
    var lowEntropyScores = new List<float> { 100f, 50f, 50f, 50f, 50f };
    entropy = CalculateTestEntropy(lowEntropyScores);
    Assert.IsTrue(entropy < 0.5f);
}
```

C) SOFTMAX DISTRIBUTION TESTS:
```csharp
void TestSoftmaxDistribution()
{
    var testScores = new List<float> { 100f, 98f, 95f, 90f };
    float temperature = 1.0f;
    
    var probabilities = CalculateSoftmax(testScores, temperature);
    
    // Verify probabilities sum to 1
    float sum = probabilities.Sum();
    Assert.AreEqual(1.0f, sum, 0.001f);
    
    // Verify highest score gets highest probability
    int maxIndex = Array.IndexOf(probabilities, probabilities.Max());
    Assert.AreEqual(0, maxIndex);
}
```

D) PERFORMANCE BENCHMARKS:
- Move generation time: <100ms
- Q-Learning decision: <50ms
- Minimax decision (depth 4): <200ms
- Memory usage: <256MB

8.3 AUTOMATED TESTING:
Tests run automatically ƒë·ªÉ ensure code quality:
- Unit tests cho individual algorithms
- Integration tests cho AI system interactions
- Performance tests cho response times
- Regression tests ƒë·ªÉ prevent quality degradation

================================================================================
                        9. HYBRID AI ARCHITECTURE
================================================================================

9.1 CONCEPT:
Hybrid system k·∫øt h·ª£p strengths c·ªßa multiple algorithms:
- Q-Learning: Data-driven, learns from experience
- Minimax: Logical, guaranteed optimal given evaluation
- Decision Tree: Fast, context-aware switching
- Entropy: Adaptive, handles uncertainty

9.2 ALGORITHM SELECTION LOGIC:

```csharp
public AIMove GetBestMove()
{
    UpdateGameState();
    List<AIMove> possibleMoves = GeneratePossibleMoves();
    
    // Primary: Use Q-Learning if trained model available
    if (useQLearning && qAgent.IsTrainedModel())
    {
        Debug.Log($"üß† Using {specializedStyle} Q-Learning");
        return qAgent.ChooseAction(currentGameState, possibleMoves);
    }
    
    // Fallback: Use Minimax for guaranteed reasonable play
    else
    {
        Debug.Log($"üîç Using {specializedStyle} Minimax");
        return FindBestMoveWithMinimax();
    }
}
```

9.3 SEAMLESS SWITCHING:
- No interruption in gameplay
- Maintains consistent API
- Preserves game state integrity
- Logs algorithm used for debugging

9.4 BENEFITS:
- Robustness: Fallback ensures AI never fails
- Performance: Q-Learning for trained scenarios, Minimax for edge cases
- Flexibility: Can switch algorithms based on situation
- Development: Easy to test individual components

================================================================================
                        10. PERFORMANCE OPTIMIZATION
================================================================================

10.1 Q-TABLE OPTIMIZATION:
- Sparse representation: Only store visited states
- Compression: String interning for state keys
- Lazy loading: Load Q-table on demand
- Periodic saving: Every 100 training episodes

10.2 MINIMAX OPTIMIZATION:
- Alpha-Beta pruning: 50% search reduction
- Move ordering: Promising moves first
- Iterative deepening: Gradual depth increase
- Transposition tables: Cache evaluated positions

10.3 MEMORY MANAGEMENT:
- Object pooling cho GameState objects
- String caching cho repeated state/action encodings
- Garbage collection optimization
- Memory profiling v√† monitoring

10.4 MULTITHREADING CONSIDERATIONS:
- Q-Learning training: Background thread
- Move calculation: Main thread (Unity requirement)
- File I/O: Async operations where possible
- Thread-safe collections cho shared data

10.5 CACHING STRATEGIES:
- Path finding results
- Move generation results
- Evaluation function results
- Feature extraction results

================================================================================
                            K·∫æT LU·∫¨N
================================================================================

D·ª± √°n AI Quoridor Project th·ªÉ hi·ªán m·ªôt implementation comprehensive c·ªßa modern
game AI techniques:

TECHNICAL INNOVATIONS:
‚úÖ Hybrid Q-Learning + Minimax architecture
‚úÖ 6 specialized AI personalities v·ªõi separate training
‚úÖ Advanced decision making v·ªõi entropy analysis
‚úÖ Comprehensive testing v√† validation framework
‚úÖ Professional development tools v√† debugging

EDUCATIONAL VALUE:
‚úÖ Practical machine learning implementation
‚úÖ Traditional game AI algorithms
‚úÖ Modern software engineering practices
‚úÖ Performance optimization techniques
‚úÖ Automated testing methodologies

FUTURE ENHANCEMENTS:
üöÄ Deep Q-Learning (DQN) integration
üöÄ Multi-agent reinforcement learning
üöÄ Neural network-based evaluation functions
üöÄ Online learning v√† adaptation
üöÄ Transfer learning between game variants

D·ª± √°n n√†y demonstrating successful integration c·ªßa classical AI methods v·ªõi
modern machine learning, t·∫°o ra m·ªôt intelligent, adaptive, v√† highly
interactive gaming experience.

================================================================================
                            APPENDIX
================================================================================

FILES LI√äN QUAN:
- Assets/scripts/QLearningAgent.cs - Q-Learning implementation
- Assets/scripts/QuoridorAI.cs - Main AI controller
- Assets/scripts/DecisionTreeClassifier.cs - Decision tree logic
- Assets/scripts/QuoridorSelfPlayTrainer.cs - Training system
- Assets/scripts/AITestScript.cs - Testing framework
- Assets/scripts/Editor/ForceRetrainAI.cs - Development tools

Q-TABLE FILES:
- StreamingAssets/qtable.json - Balanced style
- StreamingAssets/qtable_aggressive.json - Aggressive style
- StreamingAssets/qtable_defensive.json - Defensive style
- StreamingAssets/qtable_wallmaster.json - Wall Master style
- StreamingAssets/qtable_speedrunner.json - Speed Runner style
- StreamingAssets/qtable_tactical.json - Tactical style

DOCUMENTATION:
- AI_ALGORITHMS_DETAILED_DESCRIPTION.txt - This file
- README.md - Project overview
- API_INTEGRATION_GUIDE.md - Voice/Chat integration
- TRAINING_GUIDE.md - AI training instructions

================================================================================
================================================================================
                    AI QUORIDOR GAME PROJECT - THU·∫¨T TO√ÅN AI CHI TI·∫æT
================================================================================

D·ª± √°n: AI Quoridor Game v·ªõi Machine Learning v√† Multi-Platform Integration
Ng√†y t·∫°o: 25/07/2025
T√°c gi·∫£: [Student Name]
Platform: Unity 3D
Language: C#

================================================================================
                            M·ª§C L·ª§C
================================================================================

1. T·ªîNG QUAN KI·∫æN TR√öC AI
2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
4. SPECIALIZED AI STYLES SYSTEM
5. DECISION TREE CLASSIFIER
6. ENTROPY ANALYSIS & SOFTMAX
7. TRAINING SYSTEM & SELF-PLAY
8. AI TESTING FRAMEWORK
9. HYBRID AI ARCHITECTURE
10. PERFORMANCE OPTIMIZATION

================================================================================
                        1. T·ªîNG QUAN KI·∫æN TR√öC AI
================================================================================

D·ª± √°n AI Quoridor s·ª≠ d·ª•ng m·ªôt ki·∫øn tr√∫c AI hybrid ti√™n ti·∫øn, k·∫øt h·ª£p:

1.1 CORE ALGORITHMS:
- Q-Learning (Reinforcement Learning) - Thu·∫≠t to√°n ch√≠nh
- Minimax + Alpha-Beta Pruning - Thu·∫≠t to√°n fallback
- Decision Tree Classifier - Ph√¢n t√≠ch ng·ªØ c·∫£nh game
- Entropy Analysis - ƒê√°nh gi√° ƒëa d·∫°ng move

1.2 SPECIALIZED SYSTEMS:
- 6 AI Personality Styles v·ªõi training ri√™ng bi·ªát
- Self-Play Training Pipeline t·ª± ƒë·ªông
- Real-time Performance Testing Framework
- Dynamic Algorithm Switching

1.3 TECHNICAL FOUNDATION:
- State Space: 13,118+ unique game states
- Action Space: Movement + Wall placement combinations
- Training Method: Self-play v·ªõi 1000 episodes
- Learning Rate: Œ± = 0.1, Discount Factor: Œ≥ = 0.99

================================================================================
                    2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
================================================================================

2.1 GI·ªöI THI·ªÜU Q-LEARNING:
Q-Learning l√† thu·∫≠t to√°n Reinforcement Learning kh√¥ng c·∫ßn model, h·ªçc th√¥ng qua
t∆∞∆°ng t√°c v·ªõi m√¥i tr∆∞·ªùng ƒë·ªÉ t·ªëi ∆∞u h√≥a policy. Trong Quoridor, AI h·ªçc c√°ch
ch·ªçn moves t·ªëi ∆∞u d·ª±a tr√™n kinh nghi·ªám t·ª´ h√†ng ng√†n game.

2.2 C√îNG TH·ª®C TO√ÅN H·ªåC:
Q(s,a) = Q(s,a) + Œ±[r + Œ≥*max(Q(s',a')) - Q(s,a)]

Trong ƒë√≥:
- Q(s,a): Q-value c·ªßa state s v·ªõi action a
- Œ±: Learning rate (0.1) - t·ªëc ƒë·ªô h·ªçc
- r: Immediate reward t·ª´ action
- Œ≥: Discount factor (0.99) - t·∫ßm quan tr·ªçng future rewards
- s': Next state sau khi th·ª±c hi·ªán action
- max(Q(s',a')): Maximum Q-value c·ªßa next state

2.3 IMPLEMENTATION CHI TI·∫æT:

```csharp
public class QLearningAgent
{
    // Core Q-Table: Dictionary l∆∞u tr·ªØ Q-values
    private Dictionary<string, Dictionary<string, float>> qTable;
    
    // Hyperparameters
    private float learningRate = 0.1f;      // T·ªëc ƒë·ªô h·ªçc
    private float discountFactor = 0.99f;   // Discount future rewards
    private float epsilon = 0.995f;         // Exploration rate
    
    // Q-Learning Update Implementation
    public void UpdateQValue(string state, string action, float reward, string nextState)
    {
        // L·∫•y Q-value hi·ªán t·∫°i
        float currentQ = GetQValue(state, action);
        
        // T√¨m max Q-value c·ªßa next state
        float maxNextQ = GetMaxQValue(nextState);
        
        // √Åp d·ª•ng c√¥ng th·ª©c Q-Learning
        float newQ = currentQ + learningRate * (reward + discountFactor * maxNextQ - currentQ);
        
        // C·∫≠p nh·∫≠t Q-table
        SetQValue(state, action, newQ);
    }
}
```

2.4 STATE ENCODING:
Game state ƒë∆∞·ª£c encode th√†nh string duy nh·∫•t:
"aiPos_x,aiPos_y-humanPos_x,humanPos_y-aiWalls-humanWalls-wallsList"

V√≠ d·ª•: "4,8-4,0-9-10-7,4,False|5,6,True"
- AI ·ªü (4,8), Human ·ªü (4,0)
- AI c√≥ 9 walls, Human c√≥ 10 walls
- Walls ƒë√£ ƒë·∫∑t: (7,4,False), (5,6,True)

2.5 ACTION ENCODING:
Actions ƒë∆∞·ª£c encode th√†nh string:
- Movement: "M:x,y" (v√≠ d·ª•: "M:4,7")
- Wall: "W:x,y,horizontal" (v√≠ d·ª•: "W:3,5,True")

2.6 EPSILON-GREEDY STRATEGY:
```csharp
public AIMove ChooseAction(GameState state, List<AIMove> possibleMoves)
{
    // Exploration vs Exploitation trade-off
    if (Random.Range(0f, 1f) < epsilon)
    {
        // EXPLORATION: Ch·ªçn random ƒë·ªÉ kh√°m ph√°
        return possibleMoves[Random.Range(0, possibleMoves.Count)];
    }
    else
    {
        // EXPLOITATION: Ch·ªçn action c√≥ Q-value cao nh·∫•t
        return GetBestAction(state, possibleMoves);
    }
}
```

2.7 EPSILON DECAY:
Epsilon gi·∫£m d·∫ßn t·ª´ 0.995 ‚Üí 0.1 qua training:
- Ban ƒë·∫ßu: High exploration (99.5%)
- Cu·ªëi training: High exploitation (90%)

2.8 REWARD SYSTEM:
- Win game: +1000
- Lose game: -1000
- Move toward goal: +10
- Block opponent: +50
- Waste move: -5
- Strategic position: +15

================================================================================
                3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
================================================================================

3.1 GI·ªöI THI·ªÜU MINIMAX:
Minimax l√† thu·∫≠t to√°n game theory c·ªï ƒëi·ªÉn, t√¨m ki·∫øm move t·ªëi ∆∞u b·∫±ng c√°ch
d·ª± ƒëo√°n tr∆∞·ªõc nhi·ªÅu moves. Alpha-Beta pruning t·ªëi ∆∞u hi·ªáu su·∫•t b·∫±ng c√°ch
lo·∫°i b·ªè c√°c nh√°nh kh√¥ng c·∫ßn thi·∫øt.

3.2 NGUY√äN L√ù HO·∫†T ƒê·ªòNG:
- Maximizing Player (AI): T·ªëi ƒëa h√≥a score
- Minimizing Player (Human): T·ªëi thi·ªÉu h√≥a score AI
- Search depth: 4 levels (configurable)
- Pruning: Lo·∫°i b·ªè nh√°nh kh√¥ng promising

3.3 IMPLEMENTATION:

```csharp
float Minimax(GameState state, int depth, bool isMaximizing, float alpha, float beta)
{
    // Base case: Depth = 0 ho·∫∑c game over
    if (depth == 0 || IsGameOver(state))
        return EvaluatePosition(state);
    
    if (isMaximizing) // AI turn
    {
        float maxEval = float.NegativeInfinity;
        foreach (var move in GeneratePossibleMoves(state, true))
        {
            GameState newState = ApplyMove(state, move, true);
            float eval = Minimax(newState, depth - 1, false, alpha, beta);
            maxEval = Mathf.Max(maxEval, eval);
            alpha = Mathf.Max(alpha, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return maxEval;
    }
    else // Human turn
    {
        float minEval = float.PositiveInfinity;
        foreach (var move in GeneratePossibleMoves(state, false))
        {
            GameState newState = ApplyMove(state, move, false);
            float eval = Minimax(newState, depth - 1, true, alpha, beta);
            minEval = Mathf.Min(minEval, eval);
            beta = Mathf.Min(beta, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return minEval;
    }
}
```

3.4 EVALUATION FUNCTION:
Position evaluation d·ª±a tr√™n multiple factors:

```csharp
float EvaluatePosition(GameState state)
{
    float score = 0f;
    
    // Distance to goal (c√†ng g·∫ßn c√†ng t·ªët)
    int aiDistance = GetShortestPath(state.aiPosition, goalRow);
    int humanDistance = GetShortestPath(state.humanPosition, goalRow);
    score += (humanDistance - aiDistance) * 100;
    
    // Wall advantage
    score += (state.aiWallsLeft - state.humanWallsLeft) * 50;
    
    // Mobility (s·ªë moves c√≥ th·ªÉ)
    int aiMobility = GetValidMoves(state.aiPosition).Count;
    int humanMobility = GetValidMoves(state.humanPosition).Count;
    score += (aiMobility - humanMobility) * 20;
    
    // Center control
    score += GetCenterControlScore(state) * 30;
    
    return score;
}
```

3.5 ALPHA-BETA PRUNING HI·ªÜU QU·∫¢:
- Reduces search space t·ª´ O(b^d) xu·ªëng O(b^(d/2))
- Best case: 50% reduction in nodes evaluated
- Ordering moves by promising gi√∫p pruning hi·ªáu qu·∫£ h∆°n

================================================================================
                        4. SPECIALIZED AI STYLES SYSTEM
================================================================================

4.1 CONCEPT:
6 AI personalities v·ªõi training v√† behavior ri√™ng bi·ªát, m·ªói style c√≥:
- Ri√™ng Q-table file
- Custom reward structure
- Specific move prioritization
- Unique decision making logic

4.2 AI STYLES CHI TI·∫æT:

üî• AGGRESSIVE STYLE:
- M·ª•c ti√™u: T·∫•n c√¥ng v√† ch·∫∑n ƒë·ªëi th·ªß
- Wall Usage: 70% chance
- Reward Structure:
  * blockingOpponentReward = 50 * intensity
  * wallPlacementReward = 30 * intensity
  * defensiveWallPenalty = -20 * intensity
- Q-table: qtable_aggressive.json

üõ°Ô∏è DEFENSIVE STYLE:
- M·ª•c ti√™u: Ph√≤ng th·ªß v√† di chuy·ªÉn an to√†n
- Wall Usage: 50% chance
- Reward Structure:
  * defensiveWallReward = 35 * intensity
  * movementReward = 20 * intensity
  * aggressiveMovePenalty = -15 * intensity
- Q-table: qtable_defensive.json

üß± WALL MASTER STYLE:
- M·ª•c ti√™u: Chuy√™n gia ƒë·∫∑t t∆∞·ªùng v√† t·∫°o m√™ cung
- Wall Usage: 90% chance
- Reward Structure:
  * wallPlacementReward = 60 * intensity
  * strategicWallReward = 40 * intensity
  * wallComboReward = 50 * intensity
  * movementWithoutWallPenalty = -25 * intensity
- Q-table: qtable_wallmaster.json

‚ö° SPEED RUNNER STYLE:
- M·ª•c ti√™u: ƒê·∫°t ƒë√≠ch nhanh nh·∫•t
- Wall Usage: 10% chance
- Reward Structure:
  * movementReward = 40 * intensity
  * progressToGoalReward = 50 * intensity
  * quickWinBonus = 100 * intensity
  * wallPlacementPenalty = -20 * intensity
- Q-table: qtable_speedrunner.json

üß† TACTICAL STYLE:
- M·ª•c ti√™u: L·ª£i th·∫ø v·ªã tr√≠ v√† k·∫ø ho·∫°ch d√†i h·∫°n
- Wall Usage: Variable (strategic)
- Reward Structure:
  * tacticalMoveReward = 45 * intensity
  * positionAdvantageReward = 35 * intensity
  * mobilityReward = 25 * intensity
  * patternRecognitionBonus = 40 * intensity
- Q-table: qtable_tactical.json

‚öñÔ∏è BALANCED STYLE:
- M·ª•c ti√™u: C√¢n b·∫±ng t·∫•t c·∫£ aspects
- Wall Usage: 30% chance
- Reward Structure: Default balanced values
- Q-table: qtable.json

4.3 STYLE SWITCHING MECHANISM:

```csharp
public void SwitchToStyle(PlayStyle newStyle)
{
    // Save current Q-table
    if (allowQTableSaving) qAgent.SaveQTable(GetStyleSpecificQTablePath());
    
    // Load new style Q-table
    specializedStyle = newStyle;
    qAgent.LoadQTable(GetStyleSpecificQTablePath());
    
    // Reconfigure parameters
    ConfigureStyleSpecificTraining();
}
```

4.4 STYLE INTENSITY:
Parameter t·ª´ 0.0 ‚Üí 1.0 control m·ª©c ƒë·ªô chuy√™n h√≥a:
- 0.0: Balanced behavior
- 0.5: Moderate specialization
- 1.0: Extreme specialization

================================================================================
                        5. DECISION TREE CLASSIFIER
================================================================================

5.1 PURPOSE:
Decision Tree ph√¢n t√≠ch game context ƒë·ªÉ ch·ªçn strategy ph√π h·ª£p, d·ª±a tr√™n:
- Game phase (Early/Mid/End)
- Position advantage
- Resource advantage
- Mobility situation

5.2 FEATURE EXTRACTION:

```csharp
public struct DecisionFeatures
{
    public int aiDistanceToGoal;      // AI distance to goal line
    public int humanDistanceToGoal;   // Human distance to goal line
    public int aiWallsLeft;           // AI walls remaining
    public int humanWallsLeft;        // Human walls remaining
    public int aiMobility;            // AI possible moves
    public int humanMobility;         // Human possible moves
    public float distanceAdvantage;   // Human - AI distance
    public float wallAdvantage;       // AI - Human walls
    public bool isEarlyGame;          // > 15 moves estimated remaining
    public bool isMidGame;            // 5-15 moves remaining
    public bool isEndGame;            // < 5 moves remaining
}
```

5.3 DECISION LOGIC:

```csharp
public AIStrategy Predict(DecisionFeatures features)
{
    // Early Game: Play conservatively
    if (features.isEarlyGame)
        return AIStrategy.Balanced;
    
    // AI has significant lead: Focus on blocking
    if (features.distanceAdvantage > 2)
        return AIStrategy.Blocking;
    
    // AI is behind: Be aggressive
    if (features.distanceAdvantage < -1)
        return AIStrategy.Aggressive;
    
    // End game with wall advantage: Play defensively
    if (features.isEndGame && features.wallAdvantage > 2)
        return AIStrategy.Defensive;
    
    // Close game: Balanced approach
    return AIStrategy.Balanced;
}
```

5.4 STRATEGY OUTCOMES:
- Balanced: Equal weight to movement v√† walls
- Blocking: Prioritize moves that increase opponent path
- Aggressive: Focus on attacking positions
- Defensive: Protect own position and path

================================================================================
                        6. ENTROPY ANALYSIS & SOFTMAX
================================================================================

6.1 ENTROPY CONCEPT:
Entropy ƒëo ƒë·ªô ƒëa d·∫°ng/uncertainty c·ªßa move scores. High entropy = nhi·ªÅu moves
c√≥ scores t∆∞∆°ng ƒë∆∞∆°ng, low entropy = m·ªôt move r√µ r√†ng t·ªët nh·∫•t.

6.2 ENTROPY CALCULATION:

```csharp
float CalculateEntropy(List<float> moveScores)
{
    // Normalize th√†nh probabilities
    float total = moveScores.Sum();
    var probabilities = moveScores.Select(s => s / total);
    
    // Calculate Shannon entropy
    float entropy = 0f;
    foreach (float p in probabilities)
    {
        if (p > 0f)
            entropy -= p * Mathf.Log(p, 2f);
    }
    
    return entropy; // Range: 0 (deterministic) to log2(n) (uniform)
}
```

6.3 SOFTMAX DISTRIBUTION:

```csharp
float[] CalculateSoftmax(List<float> scores, float temperature)
{
    float[] probabilities = new float[scores.Count];
    float sum = scores.Sum(s => Mathf.Exp(s / temperature));
    
    for (int i = 0; i < scores.Count; i++)
    {
        probabilities[i] = Mathf.Exp(scores[i] / temperature) / sum;
    }
    
    return probabilities;
}
```

6.4 TEMPERATURE PARAMETER:
- Low temperature (0.1): Sharp distribution, focus on best moves
- High temperature (2.0): Flat distribution, more exploration
- Default temperature (1.0): Balanced exploitation/exploration

6.5 ADAPTIVE MOVE SELECTION:

```csharp
public AIMove ChooseActionWithEntropy(GameState state, List<AIMove> possibleMoves)
{
    var moveScores = EvaluateAllMoves(state, possibleMoves);
    float entropy = CalculateEntropy(moveScores);
    
    if (entropy > entropyThreshold) // High diversity
    {
        // Use softmax for probabilistic selection
        var probabilities = CalculateSoftmax(moveScores, softmaxTemperature);
        return SelectMoveByProbability(possibleMoves, probabilities);
    }
    else // Clear best move
    {
        // Use deterministic best move
        return GetBestScoredMove(possibleMoves, moveScores);
    }
}
```

================================================================================
                        7. TRAINING SYSTEM & SELF-PLAY
================================================================================

7.1 SELF-PLAY CONCEPT:
AI h·ªçc b·∫±ng c√°ch ch∆°i v·ªõi ch√≠nh n√≥ h√†ng ng√†n l·∫ßn, gradually improve performance
th√¥ng qua trial v√† error.

7.2 TRAINING PIPELINE:

```csharp
IEnumerator RunSelfPlayTraining()
{
    for (int episode = 1; episode <= maxEpisodes; episode++)
    {
        // Reset game state
        ResetGameForTraining();
        
        while (!IsGameOver())
        {
            // Get current state
            GameState currentState = GetCurrentGameState();
            List<AIMove> possibleMoves = GetPossibleMoves();
            
            // Choose action using Œµ-greedy
            AIMove chosenMove = qLearningAgent.ChooseAction(currentState, possibleMoves);
            
            // Execute move
            ExecuteMove(chosenMove);
            
            // Calculate reward
            float reward = CalculateReward(currentState, chosenMove);
            
            // Update Q-table
            GameState newState = GetCurrentGameState();
            qLearningAgent.UpdateQValue(
                EncodeState(currentState),
                EncodeAction(chosenMove),
                reward,
                EncodeState(newState)
            );
            
            yield return new WaitForEndOfFrame();
        }
        
        // Epsilon decay
        qLearningAgent.DecayEpsilon();
        
        // Save progress periodically
        if (episode % 100 == 0)
        {
            qLearningAgent.SaveQTable();
        }
    }
}
```

7.3 REWARD ENGINEERING:
Critical cho Q-Learning success:

```csharp
float CalculateReward(GameState state, AIMove move)
{
    float reward = 0f;
    
    // Game outcome rewards
    if (IsWin(state)) reward += 1000f;
    if (IsLoss(state)) reward -= 1000f;
    
    // Progress rewards
    if (move.moveType == MoveType.Movement)
    {
        if (IsMoveTowardGoal(move)) reward += 20f;
        else reward -= 5f;
    }
    
    // Strategic rewards
    if (move.moveType == MoveType.WallPlacement)
    {
        if (IsWallBlockingOpponent(move)) reward += 50f;
        if (IsWallStrategic(move)) reward += 30f;
        if (IsWallWasted(move)) reward -= 25f;
    }
    
    // Style-specific rewards
    reward += GetStyleSpecificReward(state, move);
    
    return reward;
}
```

7.4 TRAINING METRICS:
- Episodes: 1000 standard
- Duration: ~15 minutes
- Convergence: Usually around episode 800
- Q-table size: 13,118+ states at completion
- Final epsilon: 0.1 (90% exploitation)

================================================================================
                        8. AI TESTING FRAMEWORK
================================================================================

8.1 TESTING PHILOSOPHY:
Comprehensive testing ensures AI quality v√† reliability across different
game scenarios v√† edge cases.

8.2 TEST CATEGORIES:

A) DECISION TREE TESTS:
```csharp
void TestDecisionTreeStrategies()
{
    // Test Early Game
    var earlyGameFeatures = new DecisionFeatures
    {
        aiDistanceToGoal = 8, humanDistanceToGoal = 8,
        isEarlyGame = true
    };
    var strategy = decisionTree.Predict(earlyGameFeatures);
    Assert.AreEqual(AIStrategy.Balanced, strategy);
    
    // Test AI Leading
    var leadingFeatures = new DecisionFeatures
    {
        distanceAdvantage = 3, // AI leading
        isMidGame = true
    };
    strategy = decisionTree.Predict(leadingFeatures);
    Assert.AreEqual(AIStrategy.Blocking, strategy);
}
```

B) ENTROPY ANALYSIS TESTS:
```csharp
void TestEntropyAnalysis()
{
    // High entropy case (diverse scores)
    var highEntropyScores = new List<float> { 100f, 98f, 97f, 95f, 94f };
    float entropy = CalculateTestEntropy(highEntropyScores);
    Assert.IsTrue(entropy > 0.5f);
    
    // Low entropy case (clear winner)
    var lowEntropyScores = new List<float> { 100f, 50f, 50f, 50f, 50f };
    entropy = CalculateTestEntropy(lowEntropyScores);
    Assert.IsTrue(entropy < 0.5f);
}
```

C) SOFTMAX DISTRIBUTION TESTS:
```csharp
void TestSoftmaxDistribution()
{
    var testScores = new List<float> { 100f, 98f, 95f, 90f };
    float temperature = 1.0f;
    
    var probabilities = CalculateSoftmax(testScores, temperature);
    
    // Verify probabilities sum to 1
    float sum = probabilities.Sum();
    Assert.AreEqual(1.0f, sum, 0.001f);
    
    // Verify highest score gets highest probability
    int maxIndex = Array.IndexOf(probabilities, probabilities.Max());
    Assert.AreEqual(0, maxIndex);
}
```

D) PERFORMANCE BENCHMARKS:
- Move generation time: <100ms
- Q-Learning decision: <50ms
- Minimax decision (depth 4): <200ms
- Memory usage: <256MB

8.3 AUTOMATED TESTING:
Tests run automatically ƒë·ªÉ ensure code quality:
- Unit tests cho individual algorithms
- Integration tests cho AI system interactions
- Performance tests cho response times
- Regression tests ƒë·ªÉ prevent quality degradation

================================================================================
                        9. HYBRID AI ARCHITECTURE
================================================================================

9.1 CONCEPT:
Hybrid system k·∫øt h·ª£p strengths c·ªßa multiple algorithms:
- Q-Learning: Data-driven, learns from experience
- Minimax: Logical, guaranteed optimal given evaluation
- Decision Tree: Fast, context-aware switching
- Entropy: Adaptive, handles uncertainty

9.2 ALGORITHM SELECTION LOGIC:

```csharp
public AIMove GetBestMove()
{
    UpdateGameState();
    List<AIMove> possibleMoves = GeneratePossibleMoves();
    
    // Primary: Use Q-Learning if trained model available
    if (useQLearning && qAgent.IsTrainedModel())
    {
        Debug.Log($"üß† Using {specializedStyle} Q-Learning");
        return qAgent.ChooseAction(currentGameState, possibleMoves);
    }
    
    // Fallback: Use Minimax for guaranteed reasonable play
    else
    {
        Debug.Log($"üîç Using {specializedStyle} Minimax");
        return FindBestMoveWithMinimax();
    }
}
```

9.3 SEAMLESS SWITCHING:
- No interruption in gameplay
- Maintains consistent API
- Preserves game state integrity
- Logs algorithm used for debugging

9.4 BENEFITS:
- Robustness: Fallback ensures AI never fails
- Performance: Q-Learning for trained scenarios, Minimax for edge cases
- Flexibility: Can switch algorithms based on situation
- Development: Easy to test individual components

================================================================================
                        10. PERFORMANCE OPTIMIZATION
================================================================================

10.1 Q-TABLE OPTIMIZATION:
- Sparse representation: Only store visited states
- Compression: String interning for state keys
- Lazy loading: Load Q-table on demand
- Periodic saving: Every 100 training episodes

10.2 MINIMAX OPTIMIZATION:
- Alpha-Beta pruning: 50% search reduction
- Move ordering: Promising moves first
- Iterative deepening: Gradual depth increase
- Transposition tables: Cache evaluated positions

10.3 MEMORY MANAGEMENT:
- Object pooling cho GameState objects
- String caching cho repeated state/action encodings
- Garbage collection optimization
- Memory profiling v√† monitoring

10.4 MULTITHREADING CONSIDERATIONS:
- Q-Learning training: Background thread
- Move calculation: Main thread (Unity requirement)
- File I/O: Async operations where possible
- Thread-safe collections cho shared data

10.5 CACHING STRATEGIES:
- Path finding results
- Move generation results
- Evaluation function results
- Feature extraction results

================================================================================
                            K·∫æT LU·∫¨N
================================================================================

D·ª± √°n AI Quoridor Project th·ªÉ hi·ªán m·ªôt implementation comprehensive c·ªßa modern
game AI techniques:

TECHNICAL INNOVATIONS:
‚úÖ Hybrid Q-Learning + Minimax architecture
‚úÖ 6 specialized AI personalities v·ªõi separate training
‚úÖ Advanced decision making v·ªõi entropy analysis
‚úÖ Comprehensive testing v√† validation framework
‚úÖ Professional development tools v√† debugging

EDUCATIONAL VALUE:
‚úÖ Practical machine learning implementation
‚úÖ Traditional game AI algorithms
‚úÖ Modern software engineering practices
‚úÖ Performance optimization techniques
‚úÖ Automated testing methodologies

FUTURE ENHANCEMENTS:
üöÄ Deep Q-Learning (DQN) integration
üöÄ Multi-agent reinforcement learning
üöÄ Neural network-based evaluation functions
üöÄ Online learning v√† adaptation
üöÄ Transfer learning between game variants

D·ª± √°n n√†y demonstrating successful integration c·ªßa classical AI methods v·ªõi
modern machine learning, t·∫°o ra m·ªôt intelligent, adaptive, v√† highly
interactive gaming experience.

================================================================================
                            APPENDIX
================================================================================

FILES LI√äN QUAN:
- Assets/scripts/QLearningAgent.cs - Q-Learning implementation
- Assets/scripts/QuoridorAI.cs - Main AI controller
- Assets/scripts/DecisionTreeClassifier.cs - Decision tree logic
- Assets/scripts/QuoridorSelfPlayTrainer.cs - Training system
- Assets/scripts/AITestScript.cs - Testing framework
- Assets/scripts/Editor/ForceRetrainAI.cs - Development tools

Q-TABLE FILES:
- StreamingAssets/qtable.json - Balanced style
- StreamingAssets/qtable_aggressive.json - Aggressive style
- StreamingAssets/qtable_defensive.json - Defensive style
- StreamingAssets/qtable_wallmaster.json - Wall Master style
- StreamingAssets/qtable_speedrunner.json - Speed Runner style
- StreamingAssets/qtable_tactical.json - Tactical style

DOCUMENTATION:
- AI_ALGORITHMS_DETAILED_DESCRIPTION.txt - This file
- README.md - Project overview
- API_INTEGRATION_GUIDE.md - Voice/Chat integration
- TRAINING_GUIDE.md - AI training instructions

================================================================================
================================================================================
                    AI QUORIDOR GAME PROJECT - THU·∫¨T TO√ÅN AI CHI TI·∫æT
================================================================================

D·ª± √°n: AI Quoridor Game v·ªõi Machine Learning v√† Multi-Platform Integration
Ng√†y t·∫°o: 25/07/2025
T√°c gi·∫£: [Student Name]
Platform: Unity 3D
Language: C#

================================================================================
                            M·ª§C L·ª§C
================================================================================

1. T·ªîNG QUAN KI·∫æN TR√öC AI
2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
4. SPECIALIZED AI STYLES SYSTEM
5. DECISION TREE CLASSIFIER
6. ENTROPY ANALYSIS & SOFTMAX
7. TRAINING SYSTEM & SELF-PLAY
8. AI TESTING FRAMEWORK
9. HYBRID AI ARCHITECTURE
10. PERFORMANCE OPTIMIZATION

================================================================================
                        1. T·ªîNG QUAN KI·∫æN TR√öC AI
================================================================================

D·ª± √°n AI Quoridor s·ª≠ d·ª•ng m·ªôt ki·∫øn tr√∫c AI hybrid ti√™n ti·∫øn, k·∫øt h·ª£p:

1.1 CORE ALGORITHMS:
- Q-Learning (Reinforcement Learning) - Thu·∫≠t to√°n ch√≠nh
- Minimax + Alpha-Beta Pruning - Thu·∫≠t to√°n fallback
- Decision Tree Classifier - Ph√¢n t√≠ch ng·ªØ c·∫£nh game
- Entropy Analysis - ƒê√°nh gi√° ƒëa d·∫°ng move

1.2 SPECIALIZED SYSTEMS:
- 6 AI Personality Styles v·ªõi training ri√™ng bi·ªát
- Self-Play Training Pipeline t·ª± ƒë·ªông
- Real-time Performance Testing Framework
- Dynamic Algorithm Switching

1.3 TECHNICAL FOUNDATION:
- State Space: 13,118+ unique game states
- Action Space: Movement + Wall placement combinations
- Training Method: Self-play v·ªõi 1000 episodes
- Learning Rate: Œ± = 0.1, Discount Factor: Œ≥ = 0.99

================================================================================
                    2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
================================================================================

2.1 GI·ªöI THI·ªÜU Q-LEARNING:
Q-Learning l√† thu·∫≠t to√°n Reinforcement Learning kh√¥ng c·∫ßn model, h·ªçc th√¥ng qua
t∆∞∆°ng t√°c v·ªõi m√¥i tr∆∞·ªùng ƒë·ªÉ t·ªëi ∆∞u h√≥a policy. Trong Quoridor, AI h·ªçc c√°ch
ch·ªçn moves t·ªëi ∆∞u d·ª±a tr√™n kinh nghi·ªám t·ª´ h√†ng ng√†n game.

2.2 C√îNG TH·ª®C TO√ÅN H·ªåC:
Q(s,a) = Q(s,a) + Œ±[r + Œ≥*max(Q(s',a')) - Q(s,a)]

Trong ƒë√≥:
- Q(s,a): Q-value c·ªßa state s v·ªõi action a
- Œ±: Learning rate (0.1) - t·ªëc ƒë·ªô h·ªçc
- r: Immediate reward t·ª´ action
- Œ≥: Discount factor (0.99) - t·∫ßm quan tr·ªçng future rewards
- s': Next state sau khi th·ª±c hi·ªán action
- max(Q(s',a')): Maximum Q-value c·ªßa next state

2.3 IMPLEMENTATION CHI TI·∫æT:

```csharp
public class QLearningAgent
{
    // Core Q-Table: Dictionary l∆∞u tr·ªØ Q-values
    private Dictionary<string, Dictionary<string, float>> qTable;
    
    // Hyperparameters
    private float learningRate = 0.1f;      // T·ªëc ƒë·ªô h·ªçc
    private float discountFactor = 0.99f;   // Discount future rewards
    private float epsilon = 0.995f;         // Exploration rate
    
    // Q-Learning Update Implementation
    public void UpdateQValue(string state, string action, float reward, string nextState)
    {
        // L·∫•y Q-value hi·ªán t·∫°i
        float currentQ = GetQValue(state, action);
        
        // T√¨m max Q-value c·ªßa next state
        float maxNextQ = GetMaxQValue(nextState);
        
        // √Åp d·ª•ng c√¥ng th·ª©c Q-Learning
        float newQ = currentQ + learningRate * (reward + discountFactor * maxNextQ - currentQ);
        
        // C·∫≠p nh·∫≠t Q-table
        SetQValue(state, action, newQ);
    }
}
```

2.4 STATE ENCODING:
Game state ƒë∆∞·ª£c encode th√†nh string duy nh·∫•t:
"aiPos_x,aiPos_y-humanPos_x,humanPos_y-aiWalls-humanWalls-wallsList"

V√≠ d·ª•: "4,8-4,0-9-10-7,4,False|5,6,True"
- AI ·ªü (4,8), Human ·ªü (4,0)
- AI c√≥ 9 walls, Human c√≥ 10 walls
- Walls ƒë√£ ƒë·∫∑t: (7,4,False), (5,6,True)

2.5 ACTION ENCODING:
Actions ƒë∆∞·ª£c encode th√†nh string:
- Movement: "M:x,y" (v√≠ d·ª•: "M:4,7")
- Wall: "W:x,y,horizontal" (v√≠ d·ª•: "W:3,5,True")

2.6 EPSILON-GREEDY STRATEGY:
```csharp
public AIMove ChooseAction(GameState state, List<AIMove> possibleMoves)
{
    // Exploration vs Exploitation trade-off
    if (Random.Range(0f, 1f) < epsilon)
    {
        // EXPLORATION: Ch·ªçn random ƒë·ªÉ kh√°m ph√°
        return possibleMoves[Random.Range(0, possibleMoves.Count)];
    }
    else
    {
        // EXPLOITATION: Ch·ªçn action c√≥ Q-value cao nh·∫•t
        return GetBestAction(state, possibleMoves);
    }
}
```

2.7 EPSILON DECAY:
Epsilon gi·∫£m d·∫ßn t·ª´ 0.995 ‚Üí 0.1 qua training:
- Ban ƒë·∫ßu: High exploration (99.5%)
- Cu·ªëi training: High exploitation (90%)

2.8 REWARD SYSTEM:
- Win game: +1000
- Lose game: -1000
- Move toward goal: +10
- Block opponent: +50
- Waste move: -5
- Strategic position: +15

================================================================================
                3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
================================================================================

3.1 GI·ªöI THI·ªÜU MINIMAX:
Minimax l√† thu·∫≠t to√°n game theory c·ªï ƒëi·ªÉn, t√¨m ki·∫øm move t·ªëi ∆∞u b·∫±ng c√°ch
d·ª± ƒëo√°n tr∆∞·ªõc nhi·ªÅu moves. Alpha-Beta pruning t·ªëi ∆∞u hi·ªáu su·∫•t b·∫±ng c√°ch
lo·∫°i b·ªè c√°c nh√°nh kh√¥ng c·∫ßn thi·∫øt.

3.2 NGUY√äN L√ù HO·∫†T ƒê·ªòNG:
- Maximizing Player (AI): T·ªëi ƒëa h√≥a score
- Minimizing Player (Human): T·ªëi thi·ªÉu h√≥a score AI
- Search depth: 4 levels (configurable)
- Pruning: Lo·∫°i b·ªè nh√°nh kh√¥ng promising

3.3 IMPLEMENTATION:

```csharp
float Minimax(GameState state, int depth, bool isMaximizing, float alpha, float beta)
{
    // Base case: Depth = 0 ho·∫∑c game over
    if (depth == 0 || IsGameOver(state))
        return EvaluatePosition(state);
    
    if (isMaximizing) // AI turn
    {
        float maxEval = float.NegativeInfinity;
        foreach (var move in GeneratePossibleMoves(state, true))
        {
            GameState newState = ApplyMove(state, move, true);
            float eval = Minimax(newState, depth - 1, false, alpha, beta);
            maxEval = Mathf.Max(maxEval, eval);
            alpha = Mathf.Max(alpha, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return maxEval;
    }
    else // Human turn
    {
        float minEval = float.PositiveInfinity;
        foreach (var move in GeneratePossibleMoves(state, false))
        {
            GameState newState = ApplyMove(state, move, false);
            float eval = Minimax(newState, depth - 1, true, alpha, beta);
            minEval = Mathf.Min(minEval, eval);
            beta = Mathf.Min(beta, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return minEval;
    }
}
```

3.4 EVALUATION FUNCTION:
Position evaluation d·ª±a tr√™n multiple factors:

```csharp
float EvaluatePosition(GameState state)
{
    float score = 0f;
    
    // Distance to goal (c√†ng g·∫ßn c√†ng t·ªët)
    int aiDistance = GetShortestPath(state.aiPosition, goalRow);
    int humanDistance = GetShortestPath(state.humanPosition, goalRow);
    score += (humanDistance - aiDistance) * 100;
    
    // Wall advantage
    score += (state.aiWallsLeft - state.humanWallsLeft) * 50;
    
    // Mobility (s·ªë moves c√≥ th·ªÉ)
    int aiMobility = GetValidMoves(state.aiPosition).Count;
    int humanMobility = GetValidMoves(state.humanPosition).Count;
    score += (aiMobility - humanMobility) * 20;
    
    // Center control
    score += GetCenterControlScore(state) * 30;
    
    return score;
}
```

3.5 ALPHA-BETA PRUNING HI·ªÜU QU·∫¢:
- Reduces search space t·ª´ O(b^d) xu·ªëng O(b^(d/2))
- Best case: 50% reduction in nodes evaluated
- Ordering moves by promising gi√∫p pruning hi·ªáu qu·∫£ h∆°n

================================================================================
                        4. SPECIALIZED AI STYLES SYSTEM
================================================================================

4.1 CONCEPT:
6 AI personalities v·ªõi training v√† behavior ri√™ng bi·ªát, m·ªói style c√≥:
- Ri√™ng Q-table file
- Custom reward structure
- Specific move prioritization
- Unique decision making logic

4.2 AI STYLES CHI TI·∫æT:

üî• AGGRESSIVE STYLE:
- M·ª•c ti√™u: T·∫•n c√¥ng v√† ch·∫∑n ƒë·ªëi th·ªß
- Wall Usage: 70% chance
- Reward Structure:
  * blockingOpponentReward = 50 * intensity
  * wallPlacementReward = 30 * intensity
  * defensiveWallPenalty = -20 * intensity
- Q-table: qtable_aggressive.json

üõ°Ô∏è DEFENSIVE STYLE:
- M·ª•c ti√™u: Ph√≤ng th·ªß v√† di chuy·ªÉn an to√†n
- Wall Usage: 50% chance
- Reward Structure:
  * defensiveWallReward = 35 * intensity
  * movementReward = 20 * intensity
  * aggressiveMovePenalty = -15 * intensity
- Q-table: qtable_defensive.json

üß± WALL MASTER STYLE:
- M·ª•c ti√™u: Chuy√™n gia ƒë·∫∑t t∆∞·ªùng v√† t·∫°o m√™ cung
- Wall Usage: 90% chance
- Reward Structure:
  * wallPlacementReward = 60 * intensity
  * strategicWallReward = 40 * intensity
  * wallComboReward = 50 * intensity
  * movementWithoutWallPenalty = -25 * intensity
- Q-table: qtable_wallmaster.json

‚ö° SPEED RUNNER STYLE:
- M·ª•c ti√™u: ƒê·∫°t ƒë√≠ch nhanh nh·∫•t
- Wall Usage: 10% chance
- Reward Structure:
  * movementReward = 40 * intensity
  * progressToGoalReward = 50 * intensity
  * quickWinBonus = 100 * intensity
  * wallPlacementPenalty = -20 * intensity
- Q-table: qtable_speedrunner.json

üß† TACTICAL STYLE:
- M·ª•c ti√™u: L·ª£i th·∫ø v·ªã tr√≠ v√† k·∫ø ho·∫°ch d√†i h·∫°n
- Wall Usage: Variable (strategic)
- Reward Structure:
  * tacticalMoveReward = 45 * intensity
  * positionAdvantageReward = 35 * intensity
  * mobilityReward = 25 * intensity
  * patternRecognitionBonus = 40 * intensity
- Q-table: qtable_tactical.json

‚öñÔ∏è BALANCED STYLE:
- M·ª•c ti√™u: C√¢n b·∫±ng t·∫•t c·∫£ aspects
- Wall Usage: 30% chance
- Reward Structure: Default balanced values
- Q-table: qtable.json

4.3 STYLE SWITCHING MECHANISM:

```csharp
public void SwitchToStyle(PlayStyle newStyle)
{
    // Save current Q-table
    if (allowQTableSaving) qAgent.SaveQTable(GetStyleSpecificQTablePath());
    
    // Load new style Q-table
    specializedStyle = newStyle;
    qAgent.LoadQTable(GetStyleSpecificQTablePath());
    
    // Reconfigure parameters
    ConfigureStyleSpecificTraining();
}
```

4.4 STYLE INTENSITY:
Parameter t·ª´ 0.0 ‚Üí 1.0 control m·ª©c ƒë·ªô chuy√™n h√≥a:
- 0.0: Balanced behavior
- 0.5: Moderate specialization
- 1.0: Extreme specialization

================================================================================
                        5. DECISION TREE CLASSIFIER
================================================================================

5.1 PURPOSE:
Decision Tree ph√¢n t√≠ch game context ƒë·ªÉ ch·ªçn strategy ph√π h·ª£p, d·ª±a tr√™n:
- Game phase (Early/Mid/End)
- Position advantage
- Resource advantage
- Mobility situation

5.2 FEATURE EXTRACTION:

```csharp
public struct DecisionFeatures
{
    public int aiDistanceToGoal;      // AI distance to goal line
    public int humanDistanceToGoal;   // Human distance to goal line
    public int aiWallsLeft;           // AI walls remaining
    public int humanWallsLeft;        // Human walls remaining
    public int aiMobility;            // AI possible moves
    public int humanMobility;         // Human possible moves
    public float distanceAdvantage;   // Human - AI distance
    public float wallAdvantage;       // AI - Human walls
    public bool isEarlyGame;          // > 15 moves estimated remaining
    public bool isMidGame;            // 5-15 moves remaining
    public bool isEndGame;            // < 5 moves remaining
}
```

5.3 DECISION LOGIC:

```csharp
public AIStrategy Predict(DecisionFeatures features)
{
    // Early Game: Play conservatively
    if (features.isEarlyGame)
        return AIStrategy.Balanced;
    
    // AI has significant lead: Focus on blocking
    if (features.distanceAdvantage > 2)
        return AIStrategy.Blocking;
    
    // AI is behind: Be aggressive
    if (features.distanceAdvantage < -1)
        return AIStrategy.Aggressive;
    
    // End game with wall advantage: Play defensively
    if (features.isEndGame && features.wallAdvantage > 2)
        return AIStrategy.Defensive;
    
    // Close game: Balanced approach
    return AIStrategy.Balanced;
}
```

5.4 STRATEGY OUTCOMES:
- Balanced: Equal weight to movement v√† walls
- Blocking: Prioritize moves that increase opponent path
- Aggressive: Focus on attacking positions
- Defensive: Protect own position and path

================================================================================
                        6. ENTROPY ANALYSIS & SOFTMAX
================================================================================

6.1 ENTROPY CONCEPT:
Entropy ƒëo ƒë·ªô ƒëa d·∫°ng/uncertainty c·ªßa move scores. High entropy = nhi·ªÅu moves
c√≥ scores t∆∞∆°ng ƒë∆∞∆°ng, low entropy = m·ªôt move r√µ r√†ng t·ªët nh·∫•t.

6.2 ENTROPY CALCULATION:

```csharp
float CalculateEntropy(List<float> moveScores)
{
    // Normalize th√†nh probabilities
    float total = moveScores.Sum();
    var probabilities = moveScores.Select(s => s / total);
    
    // Calculate Shannon entropy
    float entropy = 0f;
    foreach (float p in probabilities)
    {
        if (p > 0f)
            entropy -= p * Mathf.Log(p, 2f);
    }
    
    return entropy; // Range: 0 (deterministic) to log2(n) (uniform)
}
```

6.3 SOFTMAX DISTRIBUTION:

```csharp
float[] CalculateSoftmax(List<float> scores, float temperature)
{
    float[] probabilities = new float[scores.Count];
    float sum = scores.Sum(s => Mathf.Exp(s / temperature));
    
    for (int i = 0; i < scores.Count; i++)
    {
        probabilities[i] = Mathf.Exp(scores[i] / temperature) / sum;
    }
    
    return probabilities;
}
```

6.4 TEMPERATURE PARAMETER:
- Low temperature (0.1): Sharp distribution, focus on best moves
- High temperature (2.0): Flat distribution, more exploration
- Default temperature (1.0): Balanced exploitation/exploration

6.5 ADAPTIVE MOVE SELECTION:

```csharp
public AIMove ChooseActionWithEntropy(GameState state, List<AIMove> possibleMoves)
{
    var moveScores = EvaluateAllMoves(state, possibleMoves);
    float entropy = CalculateEntropy(moveScores);
    
    if (entropy > entropyThreshold) // High diversity
    {
        // Use softmax for probabilistic selection
        var probabilities = CalculateSoftmax(moveScores, softmaxTemperature);
        return SelectMoveByProbability(possibleMoves, probabilities);
    }
    else // Clear best move
    {
        // Use deterministic best move
        return GetBestScoredMove(possibleMoves, moveScores);
    }
}
```

================================================================================
                        7. TRAINING SYSTEM & SELF-PLAY
================================================================================

7.1 SELF-PLAY CONCEPT:
AI h·ªçc b·∫±ng c√°ch ch∆°i v·ªõi ch√≠nh n√≥ h√†ng ng√†n l·∫ßn, gradually improve performance
th√¥ng qua trial v√† error.

7.2 TRAINING PIPELINE:

```csharp
IEnumerator RunSelfPlayTraining()
{
    for (int episode = 1; episode <= maxEpisodes; episode++)
    {
        // Reset game state
        ResetGameForTraining();
        
        while (!IsGameOver())
        {
            // Get current state
            GameState currentState = GetCurrentGameState();
            List<AIMove> possibleMoves = GetPossibleMoves();
            
            // Choose action using Œµ-greedy
            AIMove chosenMove = qLearningAgent.ChooseAction(currentState, possibleMoves);
            
            // Execute move
            ExecuteMove(chosenMove);
            
            // Calculate reward
            float reward = CalculateReward(currentState, chosenMove);
            
            // Update Q-table
            GameState newState = GetCurrentGameState();
            qLearningAgent.UpdateQValue(
                EncodeState(currentState),
                EncodeAction(chosenMove),
                reward,
                EncodeState(newState)
            );
            
            yield return new WaitForEndOfFrame();
        }
        
        // Epsilon decay
        qLearningAgent.DecayEpsilon();
        
        // Save progress periodically
        if (episode % 100 == 0)
        {
            qLearningAgent.SaveQTable();
        }
    }
}
```

7.3 REWARD ENGINEERING:
Critical cho Q-Learning success:

```csharp
float CalculateReward(GameState state, AIMove move)
{
    float reward = 0f;
    
    // Game outcome rewards
    if (IsWin(state)) reward += 1000f;
    if (IsLoss(state)) reward -= 1000f;
    
    // Progress rewards
    if (move.moveType == MoveType.Movement)
    {
        if (IsMoveTowardGoal(move)) reward += 20f;
        else reward -= 5f;
    }
    
    // Strategic rewards
    if (move.moveType == MoveType.WallPlacement)
    {
        if (IsWallBlockingOpponent(move)) reward += 50f;
        if (IsWallStrategic(move)) reward += 30f;
        if (IsWallWasted(move)) reward -= 25f;
    }
    
    // Style-specific rewards
    reward += GetStyleSpecificReward(state, move);
    
    return reward;
}
```

7.4 TRAINING METRICS:
- Episodes: 1000 standard
- Duration: ~15 minutes
- Convergence: Usually around episode 800
- Q-table size: 13,118+ states at completion
- Final epsilon: 0.1 (90% exploitation)

================================================================================
                        8. AI TESTING FRAMEWORK
================================================================================

8.1 TESTING PHILOSOPHY:
Comprehensive testing ensures AI quality v√† reliability across different
game scenarios v√† edge cases.

8.2 TEST CATEGORIES:

A) DECISION TREE TESTS:
```csharp
void TestDecisionTreeStrategies()
{
    // Test Early Game
    var earlyGameFeatures = new DecisionFeatures
    {
        aiDistanceToGoal = 8, humanDistanceToGoal = 8,
        isEarlyGame = true
    };
    var strategy = decisionTree.Predict(earlyGameFeatures);
    Assert.AreEqual(AIStrategy.Balanced, strategy);
    
    // Test AI Leading
    var leadingFeatures = new DecisionFeatures
    {
        distanceAdvantage = 3, // AI leading
        isMidGame = true
    };
    strategy = decisionTree.Predict(leadingFeatures);
    Assert.AreEqual(AIStrategy.Blocking, strategy);
}
```

B) ENTROPY ANALYSIS TESTS:
```csharp
void TestEntropyAnalysis()
{
    // High entropy case (diverse scores)
    var highEntropyScores = new List<float> { 100f, 98f, 97f, 95f, 94f };
    float entropy = CalculateTestEntropy(highEntropyScores);
    Assert.IsTrue(entropy > 0.5f);
    
    // Low entropy case (clear winner)
    var lowEntropyScores = new List<float> { 100f, 50f, 50f, 50f, 50f };
    entropy = CalculateTestEntropy(lowEntropyScores);
    Assert.IsTrue(entropy < 0.5f);
}
```

C) SOFTMAX DISTRIBUTION TESTS:
```csharp
void TestSoftmaxDistribution()
{
    var testScores = new List<float> { 100f, 98f, 95f, 90f };
    float temperature = 1.0f;
    
    var probabilities = CalculateSoftmax(testScores, temperature);
    
    // Verify probabilities sum to 1
    float sum = probabilities.Sum();
    Assert.AreEqual(1.0f, sum, 0.001f);
    
    // Verify highest score gets highest probability
    int maxIndex = Array.IndexOf(probabilities, probabilities.Max());
    Assert.AreEqual(0, maxIndex);
}
```

D) PERFORMANCE BENCHMARKS:
- Move generation time: <100ms
- Q-Learning decision: <50ms
- Minimax decision (depth 4): <200ms
- Memory usage: <256MB

8.3 AUTOMATED TESTING:
Tests run automatically ƒë·ªÉ ensure code quality:
- Unit tests cho individual algorithms
- Integration tests cho AI system interactions
- Performance tests cho response times
- Regression tests ƒë·ªÉ prevent quality degradation

================================================================================
                        9. HYBRID AI ARCHITECTURE
================================================================================

9.1 CONCEPT:
Hybrid system k·∫øt h·ª£p strengths c·ªßa multiple algorithms:
- Q-Learning: Data-driven, learns from experience
- Minimax: Logical, guaranteed optimal given evaluation
- Decision Tree: Fast, context-aware switching
- Entropy: Adaptive, handles uncertainty

9.2 ALGORITHM SELECTION LOGIC:

```csharp
public AIMove GetBestMove()
{
    UpdateGameState();
    List<AIMove> possibleMoves = GeneratePossibleMoves();
    
    // Primary: Use Q-Learning if trained model available
    if (useQLearning && qAgent.IsTrainedModel())
    {
        Debug.Log($"üß† Using {specializedStyle} Q-Learning");
        return qAgent.ChooseAction(currentGameState, possibleMoves);
    }
    
    // Fallback: Use Minimax for guaranteed reasonable play
    else
    {
        Debug.Log($"üîç Using {specializedStyle} Minimax");
        return FindBestMoveWithMinimax();
    }
}
```

9.3 SEAMLESS SWITCHING:
- No interruption in gameplay
- Maintains consistent API
- Preserves game state integrity
- Logs algorithm used for debugging

9.4 BENEFITS:
- Robustness: Fallback ensures AI never fails
- Performance: Q-Learning for trained scenarios, Minimax for edge cases
- Flexibility: Can switch algorithms based on situation
- Development: Easy to test individual components

================================================================================
                        10. PERFORMANCE OPTIMIZATION
================================================================================

10.1 Q-TABLE OPTIMIZATION:
- Sparse representation: Only store visited states
- Compression: String interning for state keys
- Lazy loading: Load Q-table on demand
- Periodic saving: Every 100 training episodes

10.2 MINIMAX OPTIMIZATION:
- Alpha-Beta pruning: 50% search reduction
- Move ordering: Promising moves first
- Iterative deepening: Gradual depth increase
- Transposition tables: Cache evaluated positions

10.3 MEMORY MANAGEMENT:
- Object pooling cho GameState objects
- String caching cho repeated state/action encodings
- Garbage collection optimization
- Memory profiling v√† monitoring

10.4 MULTITHREADING CONSIDERATIONS:
- Q-Learning training: Background thread
- Move calculation: Main thread (Unity requirement)
- File I/O: Async operations where possible
- Thread-safe collections cho shared data

10.5 CACHING STRATEGIES:
- Path finding results
- Move generation results
- Evaluation function results
- Feature extraction results

================================================================================
                            K·∫æT LU·∫¨N
================================================================================

D·ª± √°n AI Quoridor Project th·ªÉ hi·ªán m·ªôt implementation comprehensive c·ªßa modern
game AI techniques:

TECHNICAL INNOVATIONS:
‚úÖ Hybrid Q-Learning + Minimax architecture
‚úÖ 6 specialized AI personalities v·ªõi separate training
‚úÖ Advanced decision making v·ªõi entropy analysis
‚úÖ Comprehensive testing v√† validation framework
‚úÖ Professional development tools v√† debugging

EDUCATIONAL VALUE:
‚úÖ Practical machine learning implementation
‚úÖ Traditional game AI algorithms
‚úÖ Modern software engineering practices
‚úÖ Performance optimization techniques
‚úÖ Automated testing methodologies

FUTURE ENHANCEMENTS:
üöÄ Deep Q-Learning (DQN) integration
üöÄ Multi-agent reinforcement learning
üöÄ Neural network-based evaluation functions
üöÄ Online learning v√† adaptation
üöÄ Transfer learning between game variants

D·ª± √°n n√†y demonstrating successful integration c·ªßa classical AI methods v·ªõi
modern machine learning, t·∫°o ra m·ªôt intelligent, adaptive, v√† highly
interactive gaming experience.

================================================================================
                            APPENDIX
================================================================================

FILES LI√äN QUAN:
- Assets/scripts/QLearningAgent.cs - Q-Learning implementation
- Assets/scripts/QuoridorAI.cs - Main AI controller
- Assets/scripts/DecisionTreeClassifier.cs - Decision tree logic
- Assets/scripts/QuoridorSelfPlayTrainer.cs - Training system
- Assets/scripts/AITestScript.cs - Testing framework
- Assets/scripts/Editor/ForceRetrainAI.cs - Development tools

Q-TABLE FILES:
- StreamingAssets/qtable.json - Balanced style
- StreamingAssets/qtable_aggressive.json - Aggressive style
- StreamingAssets/qtable_defensive.json - Defensive style
- StreamingAssets/qtable_wallmaster.json - Wall Master style
- StreamingAssets/qtable_speedrunner.json - Speed Runner style
- StreamingAssets/qtable_tactical.json - Tactical style

DOCUMENTATION:
- AI_ALGORITHMS_DETAILED_DESCRIPTION.txt - This file
- README.md - Project overview
- API_INTEGRATION_GUIDE.md - Voice/Chat integration
- TRAINING_GUIDE.md - AI training instructions

================================================================================
================================================================================
                    AI QUORIDOR GAME PROJECT - THU·∫¨T TO√ÅN AI CHI TI·∫æT
================================================================================

D·ª± √°n: AI Quoridor Game v·ªõi Machine Learning v√† Multi-Platform Integration
Ng√†y t·∫°o: 25/07/2025
T√°c gi·∫£: [Student Name]
Platform: Unity 3D
Language: C#

================================================================================
                            M·ª§C L·ª§C
================================================================================

1. T·ªîNG QUAN KI·∫æN TR√öC AI
2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
4. SPECIALIZED AI STYLES SYSTEM
5. DECISION TREE CLASSIFIER
6. ENTROPY ANALYSIS & SOFTMAX
7. TRAINING SYSTEM & SELF-PLAY
8. AI TESTING FRAMEWORK
9. HYBRID AI ARCHITECTURE
10. PERFORMANCE OPTIMIZATION

================================================================================
                        1. T·ªîNG QUAN KI·∫æN TR√öC AI
================================================================================

D·ª± √°n AI Quoridor s·ª≠ d·ª•ng m·ªôt ki·∫øn tr√∫c AI hybrid ti√™n ti·∫øn, k·∫øt h·ª£p:

1.1 CORE ALGORITHMS:
- Q-Learning (Reinforcement Learning) - Thu·∫≠t to√°n ch√≠nh
- Minimax + Alpha-Beta Pruning - Thu·∫≠t to√°n fallback
- Decision Tree Classifier - Ph√¢n t√≠ch ng·ªØ c·∫£nh game
- Entropy Analysis - ƒê√°nh gi√° ƒëa d·∫°ng move

1.2 SPECIALIZED SYSTEMS:
- 6 AI Personality Styles v·ªõi training ri√™ng bi·ªát
- Self-Play Training Pipeline t·ª± ƒë·ªông
- Real-time Performance Testing Framework
- Dynamic Algorithm Switching

1.3 TECHNICAL FOUNDATION:
- State Space: 13,118+ unique game states
- Action Space: Movement + Wall placement combinations
- Training Method: Self-play v·ªõi 1000 episodes
- Learning Rate: Œ± = 0.1, Discount Factor: Œ≥ = 0.99

================================================================================
                    2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
================================================================================

2.1 GI·ªöI THI·ªÜU Q-LEARNING:
Q-Learning l√† thu·∫≠t to√°n Reinforcement Learning kh√¥ng c·∫ßn model, h·ªçc th√¥ng qua
t∆∞∆°ng t√°c v·ªõi m√¥i tr∆∞·ªùng ƒë·ªÉ t·ªëi ∆∞u h√≥a policy. Trong Quoridor, AI h·ªçc c√°ch
ch·ªçn moves t·ªëi ∆∞u d·ª±a tr√™n kinh nghi·ªám t·ª´ h√†ng ng√†n game.

2.2 C√îNG TH·ª®C TO√ÅN H·ªåC:
Q(s,a) = Q(s,a) + Œ±[r + Œ≥*max(Q(s',a')) - Q(s,a)]

Trong ƒë√≥:
- Q(s,a): Q-value c·ªßa state s v·ªõi action a
- Œ±: Learning rate (0.1) - t·ªëc ƒë·ªô h·ªçc
- r: Immediate reward t·ª´ action
- Œ≥: Discount factor (0.99) - t·∫ßm quan tr·ªçng future rewards
- s': Next state sau khi th·ª±c hi·ªán action
- max(Q(s',a')): Maximum Q-value c·ªßa next state

2.3 IMPLEMENTATION CHI TI·∫æT:

```csharp
public class QLearningAgent
{
    // Core Q-Table: Dictionary l∆∞u tr·ªØ Q-values
    private Dictionary<string, Dictionary<string, float>> qTable;
    
    // Hyperparameters
    private float learningRate = 0.1f;      // T·ªëc ƒë·ªô h·ªçc
    private float discountFactor = 0.99f;   // Discount future rewards
    private float epsilon = 0.995f;         // Exploration rate
    
    // Q-Learning Update Implementation
    public void UpdateQValue(string state, string action, float reward, string nextState)
    {
        // L·∫•y Q-value hi·ªán t·∫°i
        float currentQ = GetQValue(state, action);
        
        // T√¨m max Q-value c·ªßa next state
        float maxNextQ = GetMaxQValue(nextState);
        
        // √Åp d·ª•ng c√¥ng th·ª©c Q-Learning
        float newQ = currentQ + learningRate * (reward + discountFactor * maxNextQ - currentQ);
        
        // C·∫≠p nh·∫≠t Q-table
        SetQValue(state, action, newQ);
    }
}
```

2.4 STATE ENCODING:
Game state ƒë∆∞·ª£c encode th√†nh string duy nh·∫•t:
"aiPos_x,aiPos_y-humanPos_x,humanPos_y-aiWalls-humanWalls-wallsList"

V√≠ d·ª•: "4,8-4,0-9-10-7,4,False|5,6,True"
- AI ·ªü (4,8), Human ·ªü (4,0)
- AI c√≥ 9 walls, Human c√≥ 10 walls
- Walls ƒë√£ ƒë·∫∑t: (7,4,False), (5,6,True)

2.5 ACTION ENCODING:
Actions ƒë∆∞·ª£c encode th√†nh string:
- Movement: "M:x,y" (v√≠ d·ª•: "M:4,7")
- Wall: "W:x,y,horizontal" (v√≠ d·ª•: "W:3,5,True")

2.6 EPSILON-GREEDY STRATEGY:
```csharp
public AIMove ChooseAction(GameState state, List<AIMove> possibleMoves)
{
    // Exploration vs Exploitation trade-off
    if (Random.Range(0f, 1f) < epsilon)
    {
        // EXPLORATION: Ch·ªçn random ƒë·ªÉ kh√°m ph√°
        return possibleMoves[Random.Range(0, possibleMoves.Count)];
    }
    else
    {
        // EXPLOITATION: Ch·ªçn action c√≥ Q-value cao nh·∫•t
        return GetBestAction(state, possibleMoves);
    }
}
```

2.7 EPSILON DECAY:
Epsilon gi·∫£m d·∫ßn t·ª´ 0.995 ‚Üí 0.1 qua training:
- Ban ƒë·∫ßu: High exploration (99.5%)
- Cu·ªëi training: High exploitation (90%)

2.8 REWARD SYSTEM:
- Win game: +1000
- Lose game: -1000
- Move toward goal: +10
- Block opponent: +50
- Waste move: -5
- Strategic position: +15

================================================================================
                3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
================================================================================

3.1 GI·ªöI THI·ªÜU MINIMAX:
Minimax l√† thu·∫≠t to√°n game theory c·ªï ƒëi·ªÉn, t√¨m ki·∫øm move t·ªëi ∆∞u b·∫±ng c√°ch
d·ª± ƒëo√°n tr∆∞·ªõc nhi·ªÅu moves. Alpha-Beta pruning t·ªëi ∆∞u hi·ªáu su·∫•t b·∫±ng c√°ch
lo·∫°i b·ªè c√°c nh√°nh kh√¥ng c·∫ßn thi·∫øt.

3.2 NGUY√äN L√ù HO·∫†T ƒê·ªòNG:
- Maximizing Player (AI): T·ªëi ƒëa h√≥a score
- Minimizing Player (Human): T·ªëi thi·ªÉu h√≥a score AI
- Search depth: 4 levels (configurable)
- Pruning: Lo·∫°i b·ªè nh√°nh kh√¥ng promising

3.3 IMPLEMENTATION:

```csharp
float Minimax(GameState state, int depth, bool isMaximizing, float alpha, float beta)
{
    // Base case: Depth = 0 ho·∫∑c game over
    if (depth == 0 || IsGameOver(state))
        return EvaluatePosition(state);
    
    if (isMaximizing) // AI turn
    {
        float maxEval = float.NegativeInfinity;
        foreach (var move in GeneratePossibleMoves(state, true))
        {
            GameState newState = ApplyMove(state, move, true);
            float eval = Minimax(newState, depth - 1, false, alpha, beta);
            maxEval = Mathf.Max(maxEval, eval);
            alpha = Mathf.Max(alpha, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return maxEval;
    }
    else // Human turn
    {
        float minEval = float.PositiveInfinity;
        foreach (var move in GeneratePossibleMoves(state, false))
        {
            GameState newState = ApplyMove(state, move, false);
            float eval = Minimax(newState, depth - 1, true, alpha, beta);
            minEval = Mathf.Min(minEval, eval);
            beta = Mathf.Min(beta, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return minEval;
    }
}
```

3.4 EVALUATION FUNCTION:
Position evaluation d·ª±a tr√™n multiple factors:

```csharp
float EvaluatePosition(GameState state)
{
    float score = 0f;
    
    // Distance to goal (c√†ng g·∫ßn c√†ng t·ªët)
    int aiDistance = GetShortestPath(state.aiPosition, goalRow);
    int humanDistance = GetShortestPath(state.humanPosition, goalRow);
    score += (humanDistance - aiDistance) * 100;
    
    // Wall advantage
    score += (state.aiWallsLeft - state.humanWallsLeft) * 50;
    
    // Mobility (s·ªë moves c√≥ th·ªÉ)
    int aiMobility = GetValidMoves(state.aiPosition).Count;
    int humanMobility = GetValidMoves(state.humanPosition).Count;
    score += (aiMobility - humanMobility) * 20;
    
    // Center control
    score += GetCenterControlScore(state) * 30;
    
    return score;
}
```

3.5 ALPHA-BETA PRUNING HI·ªÜU QU·∫¢:
- Reduces search space t·ª´ O(b^d) xu·ªëng O(b^(d/2))
- Best case: 50% reduction in nodes evaluated
- Ordering moves by promising gi√∫p pruning hi·ªáu qu·∫£ h∆°n

================================================================================
                        4. SPECIALIZED AI STYLES SYSTEM
================================================================================

4.1 CONCEPT:
6 AI personalities v·ªõi training v√† behavior ri√™ng bi·ªát, m·ªói style c√≥:
- Ri√™ng Q-table file
- Custom reward structure
- Specific move prioritization
- Unique decision making logic

4.2 AI STYLES CHI TI·∫æT:

üî• AGGRESSIVE STYLE:
- M·ª•c ti√™u: T·∫•n c√¥ng v√† ch·∫∑n ƒë·ªëi th·ªß
- Wall Usage: 70% chance
- Reward Structure:
  * blockingOpponentReward = 50 * intensity
  * wallPlacementReward = 30 * intensity
  * defensiveWallPenalty = -20 * intensity
- Q-table: qtable_aggressive.json

üõ°Ô∏è DEFENSIVE STYLE:
- M·ª•c ti√™u: Ph√≤ng th·ªß v√† di chuy·ªÉn an to√†n
- Wall Usage: 50% chance
- Reward Structure:
  * defensiveWallReward = 35 * intensity
  * movementReward = 20 * intensity
  * aggressiveMovePenalty = -15 * intensity
- Q-table: qtable_defensive.json

üß± WALL MASTER STYLE:
- M·ª•c ti√™u: Chuy√™n gia ƒë·∫∑t t∆∞·ªùng v√† t·∫°o m√™ cung
- Wall Usage: 90% chance
- Reward Structure:
  * wallPlacementReward = 60 * intensity
  * strategicWallReward = 40 * intensity
  * wallComboReward = 50 * intensity
  * movementWithoutWallPenalty = -25 * intensity
- Q-table: qtable_wallmaster.json

‚ö° SPEED RUNNER STYLE:
- M·ª•c ti√™u: ƒê·∫°t ƒë√≠ch nhanh nh·∫•t
- Wall Usage: 10% chance
- Reward Structure:
  * movementReward = 40 * intensity
  * progressToGoalReward = 50 * intensity
  * quickWinBonus = 100 * intensity
  * wallPlacementPenalty = -20 * intensity
- Q-table: qtable_speedrunner.json

üß† TACTICAL STYLE:
- M·ª•c ti√™u: L·ª£i th·∫ø v·ªã tr√≠ v√† k·∫ø ho·∫°ch d√†i h·∫°n
- Wall Usage: Variable (strategic)
- Reward Structure:
  * tacticalMoveReward = 45 * intensity
  * positionAdvantageReward = 35 * intensity
  * mobilityReward = 25 * intensity
  * patternRecognitionBonus = 40 * intensity
- Q-table: qtable_tactical.json

‚öñÔ∏è BALANCED STYLE:
- M·ª•c ti√™u: C√¢n b·∫±ng t·∫•t c·∫£ aspects
- Wall Usage: 30% chance
- Reward Structure: Default balanced values
- Q-table: qtable.json

4.3 STYLE SWITCHING MECHANISM:

```csharp
public void SwitchToStyle(PlayStyle newStyle)
{
    // Save current Q-table
    if (allowQTableSaving) qAgent.SaveQTable(GetStyleSpecificQTablePath());
    
    // Load new style Q-table
    specializedStyle = newStyle;
    qAgent.LoadQTable(GetStyleSpecificQTablePath());
    
    // Reconfigure parameters
    ConfigureStyleSpecificTraining();
}
```

4.4 STYLE INTENSITY:
Parameter t·ª´ 0.0 ‚Üí 1.0 control m·ª©c ƒë·ªô chuy√™n h√≥a:
- 0.0: Balanced behavior
- 0.5: Moderate specialization
- 1.0: Extreme specialization

================================================================================
                        5. DECISION TREE CLASSIFIER
================================================================================

5.1 PURPOSE:
Decision Tree ph√¢n t√≠ch game context ƒë·ªÉ ch·ªçn strategy ph√π h·ª£p, d·ª±a tr√™n:
- Game phase (Early/Mid/End)
- Position advantage
- Resource advantage
- Mobility situation

5.2 FEATURE EXTRACTION:

```csharp
public struct DecisionFeatures
{
    public int aiDistanceToGoal;      // AI distance to goal line
    public int humanDistanceToGoal;   // Human distance to goal line
    public int aiWallsLeft;           // AI walls remaining
    public int humanWallsLeft;        // Human walls remaining
    public int aiMobility;            // AI possible moves
    public int humanMobility;         // Human possible moves
    public float distanceAdvantage;   // Human - AI distance
    public float wallAdvantage;       // AI - Human walls
    public bool isEarlyGame;          // > 15 moves estimated remaining
    public bool isMidGame;            // 5-15 moves remaining
    public bool isEndGame;            // < 5 moves remaining
}
```

5.3 DECISION LOGIC:

```csharp
public AIStrategy Predict(DecisionFeatures features)
{
    // Early Game: Play conservatively
    if (features.isEarlyGame)
        return AIStrategy.Balanced;
    
    // AI has significant lead: Focus on blocking
    if (features.distanceAdvantage > 2)
        return AIStrategy.Blocking;
    
    // AI is behind: Be aggressive
    if (features.distanceAdvantage < -1)
        return AIStrategy.Aggressive;
    
    // End game with wall advantage: Play defensively
    if (features.isEndGame && features.wallAdvantage > 2)
        return AIStrategy.Defensive;
    
    // Close game: Balanced approach
    return AIStrategy.Balanced;
}
```

5.4 STRATEGY OUTCOMES:
- Balanced: Equal weight to movement v√† walls
- Blocking: Prioritize moves that increase opponent path
- Aggressive: Focus on attacking positions
- Defensive: Protect own position and path

================================================================================
                        6. ENTROPY ANALYSIS & SOFTMAX
================================================================================

6.1 ENTROPY CONCEPT:
Entropy ƒëo ƒë·ªô ƒëa d·∫°ng/uncertainty c·ªßa move scores. High entropy = nhi·ªÅu moves
c√≥ scores t∆∞∆°ng ƒë∆∞∆°ng, low entropy = m·ªôt move r√µ r√†ng t·ªët nh·∫•t.

6.2 ENTROPY CALCULATION:

```csharp
float CalculateEntropy(List<float> moveScores)
{
    // Normalize th√†nh probabilities
    float total = moveScores.Sum();
    var probabilities = moveScores.Select(s => s / total);
    
    // Calculate Shannon entropy
    float entropy = 0f;
    foreach (float p in probabilities)
    {
        if (p > 0f)
            entropy -= p * Mathf.Log(p, 2f);
    }
    
    return entropy; // Range: 0 (deterministic) to log2(n) (uniform)
}
```

6.3 SOFTMAX DISTRIBUTION:

```csharp
float[] CalculateSoftmax(List<float> scores, float temperature)
{
    float[] probabilities = new float[scores.Count];
    float sum = scores.Sum(s => Mathf.Exp(s / temperature));
    
    for (int i = 0; i < scores.Count; i++)
    {
        probabilities[i] = Mathf.Exp(scores[i] / temperature) / sum;
    }
    
    return probabilities;
}
```

6.4 TEMPERATURE PARAMETER:
- Low temperature (0.1): Sharp distribution, focus on best moves
- High temperature (2.0): Flat distribution, more exploration
- Default temperature (1.0): Balanced exploitation/exploration

6.5 ADAPTIVE MOVE SELECTION:

```csharp
public AIMove ChooseActionWithEntropy(GameState state, List<AIMove> possibleMoves)
{
    var moveScores = EvaluateAllMoves(state, possibleMoves);
    float entropy = CalculateEntropy(moveScores);
    
    if (entropy > entropyThreshold) // High diversity
    {
        // Use softmax for probabilistic selection
        var probabilities = CalculateSoftmax(moveScores, softmaxTemperature);
        return SelectMoveByProbability(possibleMoves, probabilities);
    }
    else // Clear best move
    {
        // Use deterministic best move
        return GetBestScoredMove(possibleMoves, moveScores);
    }
}
```

================================================================================
                        7. TRAINING SYSTEM & SELF-PLAY
================================================================================

7.1 SELF-PLAY CONCEPT:
AI h·ªçc b·∫±ng c√°ch ch∆°i v·ªõi ch√≠nh n√≥ h√†ng ng√†n l·∫ßn, gradually improve performance
th√¥ng qua trial v√† error.

7.2 TRAINING PIPELINE:

```csharp
IEnumerator RunSelfPlayTraining()
{
    for (int episode = 1; episode <= maxEpisodes; episode++)
    {
        // Reset game state
        ResetGameForTraining();
        
        while (!IsGameOver())
        {
            // Get current state
            GameState currentState = GetCurrentGameState();
            List<AIMove> possibleMoves = GetPossibleMoves();
            
            // Choose action using Œµ-greedy
            AIMove chosenMove = qLearningAgent.ChooseAction(currentState, possibleMoves);
            
            // Execute move
            ExecuteMove(chosenMove);
            
            // Calculate reward
            float reward = CalculateReward(currentState, chosenMove);
            
            // Update Q-table
            GameState newState = GetCurrentGameState();
            qLearningAgent.UpdateQValue(
                EncodeState(currentState),
                EncodeAction(chosenMove),
                reward,
                EncodeState(newState)
            );
            
            yield return new WaitForEndOfFrame();
        }
        
        // Epsilon decay
        qLearningAgent.DecayEpsilon();
        
        // Save progress periodically
        if (episode % 100 == 0)
        {
            qLearningAgent.SaveQTable();
        }
    }
}
```

7.3 REWARD ENGINEERING:
Critical cho Q-Learning success:

```csharp
float CalculateReward(GameState state, AIMove move)
{
    float reward = 0f;
    
    // Game outcome rewards
    if (IsWin(state)) reward += 1000f;
    if (IsLoss(state)) reward -= 1000f;
    
    // Progress rewards
    if (move.moveType == MoveType.Movement)
    {
        if (IsMoveTowardGoal(move)) reward += 20f;
        else reward -= 5f;
    }
    
    // Strategic rewards
    if (move.moveType == MoveType.WallPlacement)
    {
        if (IsWallBlockingOpponent(move)) reward += 50f;
        if (IsWallStrategic(move)) reward += 30f;
        if (IsWallWasted(move)) reward -= 25f;
    }
    
    // Style-specific rewards
    reward += GetStyleSpecificReward(state, move);
    
    return reward;
}
```

7.4 TRAINING METRICS:
- Episodes: 1000 standard
- Duration: ~15 minutes
- Convergence: Usually around episode 800
- Q-table size: 13,118+ states at completion
- Final epsilon: 0.1 (90% exploitation)

================================================================================
                        8. AI TESTING FRAMEWORK
================================================================================

8.1 TESTING PHILOSOPHY:
Comprehensive testing ensures AI quality v√† reliability across different
game scenarios v√† edge cases.

8.2 TEST CATEGORIES:

A) DECISION TREE TESTS:
```csharp
void TestDecisionTreeStrategies()
{
    // Test Early Game
    var earlyGameFeatures = new DecisionFeatures
    {
        aiDistanceToGoal = 8, humanDistanceToGoal = 8,
        isEarlyGame = true
    };
    var strategy = decisionTree.Predict(earlyGameFeatures);
    Assert.AreEqual(AIStrategy.Balanced, strategy);
    
    // Test AI Leading
    var leadingFeatures = new DecisionFeatures
    {
        distanceAdvantage = 3, // AI leading
        isMidGame = true
    };
    strategy = decisionTree.Predict(leadingFeatures);
    Assert.AreEqual(AIStrategy.Blocking, strategy);
}
```

B) ENTROPY ANALYSIS TESTS:
```csharp
void TestEntropyAnalysis()
{
    // High entropy case (diverse scores)
    var highEntropyScores = new List<float> { 100f, 98f, 97f, 95f, 94f };
    float entropy = CalculateTestEntropy(highEntropyScores);
    Assert.IsTrue(entropy > 0.5f);
    
    // Low entropy case (clear winner)
    var lowEntropyScores = new List<float> { 100f, 50f, 50f, 50f, 50f };
    entropy = CalculateTestEntropy(lowEntropyScores);
    Assert.IsTrue(entropy < 0.5f);
}
```

C) SOFTMAX DISTRIBUTION TESTS:
```csharp
void TestSoftmaxDistribution()
{
    var testScores = new List<float> { 100f, 98f, 95f, 90f };
    float temperature = 1.0f;
    
    var probabilities = CalculateSoftmax(testScores, temperature);
    
    // Verify probabilities sum to 1
    float sum = probabilities.Sum();
    Assert.AreEqual(1.0f, sum, 0.001f);
    
    // Verify highest score gets highest probability
    int maxIndex = Array.IndexOf(probabilities, probabilities.Max());
    Assert.AreEqual(0, maxIndex);
}
```

D) PERFORMANCE BENCHMARKS:
- Move generation time: <100ms
- Q-Learning decision: <50ms
- Minimax decision (depth 4): <200ms
- Memory usage: <256MB

8.3 AUTOMATED TESTING:
Tests run automatically ƒë·ªÉ ensure code quality:
- Unit tests cho individual algorithms
- Integration tests cho AI system interactions
- Performance tests cho response times
- Regression tests ƒë·ªÉ prevent quality degradation

================================================================================
                        9. HYBRID AI ARCHITECTURE
================================================================================

9.1 CONCEPT:
Hybrid system k·∫øt h·ª£p strengths c·ªßa multiple algorithms:
- Q-Learning: Data-driven, learns from experience
- Minimax: Logical, guaranteed optimal given evaluation
- Decision Tree: Fast, context-aware switching
- Entropy: Adaptive, handles uncertainty

9.2 ALGORITHM SELECTION LOGIC:

```csharp
public AIMove GetBestMove()
{
    UpdateGameState();
    List<AIMove> possibleMoves = GeneratePossibleMoves();
    
    // Primary: Use Q-Learning if trained model available
    if (useQLearning && qAgent.IsTrainedModel())
    {
        Debug.Log($"üß† Using {specializedStyle} Q-Learning");
        return qAgent.ChooseAction(currentGameState, possibleMoves);
    }
    
    // Fallback: Use Minimax for guaranteed reasonable play
    else
    {
        Debug.Log($"üîç Using {specializedStyle} Minimax");
        return FindBestMoveWithMinimax();
    }
}
```

9.3 SEAMLESS SWITCHING:
- No interruption in gameplay
- Maintains consistent API
- Preserves game state integrity
- Logs algorithm used for debugging

9.4 BENEFITS:
- Robustness: Fallback ensures AI never fails
- Performance: Q-Learning for trained scenarios, Minimax for edge cases
- Flexibility: Can switch algorithms based on situation
- Development: Easy to test individual components

================================================================================
                        10. PERFORMANCE OPTIMIZATION
================================================================================

10.1 Q-TABLE OPTIMIZATION:
- Sparse representation: Only store visited states
- Compression: String interning for state keys
- Lazy loading: Load Q-table on demand
- Periodic saving: Every 100 training episodes

10.2 MINIMAX OPTIMIZATION:
- Alpha-Beta pruning: 50% search reduction
- Move ordering: Promising moves first
- Iterative deepening: Gradual depth increase
- Transposition tables: Cache evaluated positions

10.3 MEMORY MANAGEMENT:
- Object pooling cho GameState objects
- String caching cho repeated state/action encodings
- Garbage collection optimization
- Memory profiling v√† monitoring

10.4 MULTITHREADING CONSIDERATIONS:
- Q-Learning training: Background thread
- Move calculation: Main thread (Unity requirement)
- File I/O: Async operations where possible
- Thread-safe collections cho shared data

10.5 CACHING STRATEGIES:
- Path finding results
- Move generation results
- Evaluation function results
- Feature extraction results

================================================================================
                            K·∫æT LU·∫¨N
================================================================================

D·ª± √°n AI Quoridor Project th·ªÉ hi·ªán m·ªôt implementation comprehensive c·ªßa modern
game AI techniques:

TECHNICAL INNOVATIONS:
‚úÖ Hybrid Q-Learning + Minimax architecture
‚úÖ 6 specialized AI personalities v·ªõi separate training
‚úÖ Advanced decision making v·ªõi entropy analysis
‚úÖ Comprehensive testing v√† validation framework
‚úÖ Professional development tools v√† debugging

EDUCATIONAL VALUE:
‚úÖ Practical machine learning implementation
‚úÖ Traditional game AI algorithms
‚úÖ Modern software engineering practices
‚úÖ Performance optimization techniques
‚úÖ Automated testing methodologies

FUTURE ENHANCEMENTS:
üöÄ Deep Q-Learning (DQN) integration
üöÄ Multi-agent reinforcement learning
üöÄ Neural network-based evaluation functions
üöÄ Online learning v√† adaptation
üöÄ Transfer learning between game variants

D·ª± √°n n√†y demonstrating successful integration c·ªßa classical AI methods v·ªõi
modern machine learning, t·∫°o ra m·ªôt intelligent, adaptive, v√† highly
interactive gaming experience.

================================================================================
                            APPENDIX
================================================================================

FILES LI√äN QUAN:
- Assets/scripts/QLearningAgent.cs - Q-Learning implementation
- Assets/scripts/QuoridorAI.cs - Main AI controller
- Assets/scripts/DecisionTreeClassifier.cs - Decision tree logic
- Assets/scripts/QuoridorSelfPlayTrainer.cs - Training system
- Assets/scripts/AITestScript.cs - Testing framework
- Assets/scripts/Editor/ForceRetrainAI.cs - Development tools

Q-TABLE FILES:
- StreamingAssets/qtable.json - Balanced style
- StreamingAssets/qtable_aggressive.json - Aggressive style
- StreamingAssets/qtable_defensive.json - Defensive style
- StreamingAssets/qtable_wallmaster.json - Wall Master style
- StreamingAssets/qtable_speedrunner.json - Speed Runner style
- StreamingAssets/qtable_tactical.json - Tactical style

DOCUMENTATION:
- AI_ALGORITHMS_DETAILED_DESCRIPTION.txt - This file
- README.md - Project overview
- API_INTEGRATION_GUIDE.md - Voice/Chat integration
- TRAINING_GUIDE.md - AI training instructions

================================================================================
================================================================================
                    AI QUORIDOR GAME PROJECT - THU·∫¨T TO√ÅN AI CHI TI·∫æT
================================================================================

D·ª± √°n: AI Quoridor Game v·ªõi Machine Learning v√† Multi-Platform Integration
Ng√†y t·∫°o: 25/07/2025
T√°c gi·∫£: [Student Name]
Platform: Unity 3D
Language: C#

================================================================================
                            M·ª§C L·ª§C
================================================================================

1. T·ªîNG QUAN KI·∫æN TR√öC AI
2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
4. SPECIALIZED AI STYLES SYSTEM
5. DECISION TREE CLASSIFIER
6. ENTROPY ANALYSIS & SOFTMAX
7. TRAINING SYSTEM & SELF-PLAY
8. AI TESTING FRAMEWORK
9. HYBRID AI ARCHITECTURE
10. PERFORMANCE OPTIMIZATION

================================================================================
                        1. T·ªîNG QUAN KI·∫æN TR√öC AI
================================================================================

D·ª± √°n AI Quoridor s·ª≠ d·ª•ng m·ªôt ki·∫øn tr√∫c AI hybrid ti√™n ti·∫øn, k·∫øt h·ª£p:

1.1 CORE ALGORITHMS:
- Q-Learning (Reinforcement Learning) - Thu·∫≠t to√°n ch√≠nh
- Minimax + Alpha-Beta Pruning - Thu·∫≠t to√°n fallback
- Decision Tree Classifier - Ph√¢n t√≠ch ng·ªØ c·∫£nh game
- Entropy Analysis - ƒê√°nh gi√° ƒëa d·∫°ng move

1.2 SPECIALIZED SYSTEMS:
- 6 AI Personality Styles v·ªõi training ri√™ng bi·ªát
- Self-Play Training Pipeline t·ª± ƒë·ªông
- Real-time Performance Testing Framework
- Dynamic Algorithm Switching

1.3 TECHNICAL FOUNDATION:
- State Space: 13,118+ unique game states
- Action Space: Movement + Wall placement combinations
- Training Method: Self-play v·ªõi 1000 episodes
- Learning Rate: Œ± = 0.1, Discount Factor: Œ≥ = 0.99

================================================================================
                    2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
================================================================================

2.1 GI·ªöI THI·ªÜU Q-LEARNING:
Q-Learning l√† thu·∫≠t to√°n Reinforcement Learning kh√¥ng c·∫ßn model, h·ªçc th√¥ng qua
t∆∞∆°ng t√°c v·ªõi m√¥i tr∆∞·ªùng ƒë·ªÉ t·ªëi ∆∞u h√≥a policy. Trong Quoridor, AI h·ªçc c√°ch
ch·ªçn moves t·ªëi ∆∞u d·ª±a tr√™n kinh nghi·ªám t·ª´ h√†ng ng√†n game.

2.2 C√îNG TH·ª®C TO√ÅN H·ªåC:
Q(s,a) = Q(s,a) + Œ±[r + Œ≥*max(Q(s',a')) - Q(s,a)]

Trong ƒë√≥:
- Q(s,a): Q-value c·ªßa state s v·ªõi action a
- Œ±: Learning rate (0.1) - t·ªëc ƒë·ªô h·ªçc
- r: Immediate reward t·ª´ action
- Œ≥: Discount factor (0.99) - t·∫ßm quan tr·ªçng future rewards
- s': Next state sau khi th·ª±c hi·ªán action
- max(Q(s',a')): Maximum Q-value c·ªßa next state

2.3 IMPLEMENTATION CHI TI·∫æT:

```csharp
public class QLearningAgent
{
    // Core Q-Table: Dictionary l∆∞u tr·ªØ Q-values
    private Dictionary<string, Dictionary<string, float>> qTable;
    
    // Hyperparameters
    private float learningRate = 0.1f;      // T·ªëc ƒë·ªô h·ªçc
    private float discountFactor = 0.99f;   // Discount future rewards
    private float epsilon = 0.995f;         // Exploration rate
    
    // Q-Learning Update Implementation
    public void UpdateQValue(string state, string action, float reward, string nextState)
    {
        // L·∫•y Q-value hi·ªán t·∫°i
        float currentQ = GetQValue(state, action);
        
        // T√¨m max Q-value c·ªßa next state
        float maxNextQ = GetMaxQValue(nextState);
        
        // √Åp d·ª•ng c√¥ng th·ª©c Q-Learning
        float newQ = currentQ + learningRate * (reward + discountFactor * maxNextQ - currentQ);
        
        // C·∫≠p nh·∫≠t Q-table
        SetQValue(state, action, newQ);
    }
}
```

2.4 STATE ENCODING:
Game state ƒë∆∞·ª£c encode th√†nh string duy nh·∫•t:
"aiPos_x,aiPos_y-humanPos_x,humanPos_y-aiWalls-humanWalls-wallsList"

V√≠ d·ª•: "4,8-4,0-9-10-7,4,False|5,6,True"
- AI ·ªü (4,8), Human ·ªü (4,0)
- AI c√≥ 9 walls, Human c√≥ 10 walls
- Walls ƒë√£ ƒë·∫∑t: (7,4,False), (5,6,True)

2.5 ACTION ENCODING:
Actions ƒë∆∞·ª£c encode th√†nh string:
- Movement: "M:x,y" (v√≠ d·ª•: "M:4,7")
- Wall: "W:x,y,horizontal" (v√≠ d·ª•: "W:3,5,True")

2.6 EPSILON-GREEDY STRATEGY:
```csharp
public AIMove ChooseAction(GameState state, List<AIMove> possibleMoves)
{
    // Exploration vs Exploitation trade-off
    if (Random.Range(0f, 1f) < epsilon)
    {
        // EXPLORATION: Ch·ªçn random ƒë·ªÉ kh√°m ph√°
        return possibleMoves[Random.Range(0, possibleMoves.Count)];
    }
    else
    {
        // EXPLOITATION: Ch·ªçn action c√≥ Q-value cao nh·∫•t
        return GetBestAction(state, possibleMoves);
    }
}
```

2.7 EPSILON DECAY:
Epsilon gi·∫£m d·∫ßn t·ª´ 0.995 ‚Üí 0.1 qua training:
- Ban ƒë·∫ßu: High exploration (99.5%)
- Cu·ªëi training: High exploitation (90%)

2.8 REWARD SYSTEM:
- Win game: +1000
- Lose game: -1000
- Move toward goal: +10
- Block opponent: +50
- Waste move: -5
- Strategic position: +15

================================================================================
                3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
================================================================================

3.1 GI·ªöI THI·ªÜU MINIMAX:
Minimax l√† thu·∫≠t to√°n game theory c·ªï ƒëi·ªÉn, t√¨m ki·∫øm move t·ªëi ∆∞u b·∫±ng c√°ch
d·ª± ƒëo√°n tr∆∞·ªõc nhi·ªÅu moves. Alpha-Beta pruning t·ªëi ∆∞u hi·ªáu su·∫•t b·∫±ng c√°ch
lo·∫°i b·ªè c√°c nh√°nh kh√¥ng c·∫ßn thi·∫øt.

3.2 NGUY√äN L√ù HO·∫†T ƒê·ªòNG:
- Maximizing Player (AI): T·ªëi ƒëa h√≥a score
- Minimizing Player (Human): T·ªëi thi·ªÉu h√≥a score AI
- Search depth: 4 levels (configurable)
- Pruning: Lo·∫°i b·ªè nh√°nh kh√¥ng promising

3.3 IMPLEMENTATION:

```csharp
float Minimax(GameState state, int depth, bool isMaximizing, float alpha, float beta)
{
    // Base case: Depth = 0 ho·∫∑c game over
    if (depth == 0 || IsGameOver(state))
        return EvaluatePosition(state);
    
    if (isMaximizing) // AI turn
    {
        float maxEval = float.NegativeInfinity;
        foreach (var move in GeneratePossibleMoves(state, true))
        {
            GameState newState = ApplyMove(state, move, true);
            float eval = Minimax(newState, depth - 1, false, alpha, beta);
            maxEval = Mathf.Max(maxEval, eval);
            alpha = Mathf.Max(alpha, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return maxEval;
    }
    else // Human turn
    {
        float minEval = float.PositiveInfinity;
        foreach (var move in GeneratePossibleMoves(state, false))
        {
            GameState newState = ApplyMove(state, move, false);
            float eval = Minimax(newState, depth - 1, true, alpha, beta);
            minEval = Mathf.Min(minEval, eval);
            beta = Mathf.Min(beta, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return minEval;
    }
}
```

3.4 EVALUATION FUNCTION:
Position evaluation d·ª±a tr√™n multiple factors:

```csharp
float EvaluatePosition(GameState state)
{
    float score = 0f;
    
    // Distance to goal (c√†ng g·∫ßn c√†ng t·ªët)
    int aiDistance = GetShortestPath(state.aiPosition, goalRow);
    int humanDistance = GetShortestPath(state.humanPosition, goalRow);
    score += (humanDistance - aiDistance) * 100;
    
    // Wall advantage
    score += (state.aiWallsLeft - state.humanWallsLeft) * 50;
    
    // Mobility (s·ªë moves c√≥ th·ªÉ)
    int aiMobility = GetValidMoves(state.aiPosition).Count;
    int humanMobility = GetValidMoves(state.humanPosition).Count;
    score += (aiMobility - humanMobility) * 20;
    
    // Center control
    score += GetCenterControlScore(state) * 30;
    
    return score;
}
```

3.5 ALPHA-BETA PRUNING HI·ªÜU QU·∫¢:
- Reduces search space t·ª´ O(b^d) xu·ªëng O(b^(d/2))
- Best case: 50% reduction in nodes evaluated
- Ordering moves by promising gi√∫p pruning hi·ªáu qu·∫£ h∆°n

================================================================================
                        4. SPECIALIZED AI STYLES SYSTEM
================================================================================

4.1 CONCEPT:
6 AI personalities v·ªõi training v√† behavior ri√™ng bi·ªát, m·ªói style c√≥:
- Ri√™ng Q-table file
- Custom reward structure
- Specific move prioritization
- Unique decision making logic

4.2 AI STYLES CHI TI·∫æT:

üî• AGGRESSIVE STYLE:
- M·ª•c ti√™u: T·∫•n c√¥ng v√† ch·∫∑n ƒë·ªëi th·ªß
- Wall Usage: 70% chance
- Reward Structure:
  * blockingOpponentReward = 50 * intensity
  * wallPlacementReward = 30 * intensity
  * defensiveWallPenalty = -20 * intensity
- Q-table: qtable_aggressive.json

üõ°Ô∏è DEFENSIVE STYLE:
- M·ª•c ti√™u: Ph√≤ng th·ªß v√† di chuy·ªÉn an to√†n
- Wall Usage: 50% chance
- Reward Structure:
  * defensiveWallReward = 35 * intensity
  * movementReward = 20 * intensity
  * aggressiveMovePenalty = -15 * intensity
- Q-table: qtable_defensive.json

üß± WALL MASTER STYLE:
- M·ª•c ti√™u: Chuy√™n gia ƒë·∫∑t t∆∞·ªùng v√† t·∫°o m√™ cung
- Wall Usage: 90% chance
- Reward Structure:
  * wallPlacementReward = 60 * intensity
  * strategicWallReward = 40 * intensity
  * wallComboReward = 50 * intensity
  * movementWithoutWallPenalty = -25 * intensity
- Q-table: qtable_wallmaster.json

‚ö° SPEED RUNNER STYLE:
- M·ª•c ti√™u: ƒê·∫°t ƒë√≠ch nhanh nh·∫•t
- Wall Usage: 10% chance
- Reward Structure:
  * movementReward = 40 * intensity
  * progressToGoalReward = 50 * intensity
  * quickWinBonus = 100 * intensity
  * wallPlacementPenalty = -20 * intensity
- Q-table: qtable_speedrunner.json

üß† TACTICAL STYLE:
- M·ª•c ti√™u: L·ª£i th·∫ø v·ªã tr√≠ v√† k·∫ø ho·∫°ch d√†i h·∫°n
- Wall Usage: Variable (strategic)
- Reward Structure:
  * tacticalMoveReward = 45 * intensity
  * positionAdvantageReward = 35 * intensity
  * mobilityReward = 25 * intensity
  * patternRecognitionBonus = 40 * intensity
- Q-table: qtable_tactical.json

‚öñÔ∏è BALANCED STYLE:
- M·ª•c ti√™u: C√¢n b·∫±ng t·∫•t c·∫£ aspects
- Wall Usage: 30% chance
- Reward Structure: Default balanced values
- Q-table: qtable.json

4.3 STYLE SWITCHING MECHANISM:

```csharp
public void SwitchToStyle(PlayStyle newStyle)
{
    // Save current Q-table
    if (allowQTableSaving) qAgent.SaveQTable(GetStyleSpecificQTablePath());
    
    // Load new style Q-table
    specializedStyle = newStyle;
    qAgent.LoadQTable(GetStyleSpecificQTablePath());
    
    // Reconfigure parameters
    ConfigureStyleSpecificTraining();
}
```

4.4 STYLE INTENSITY:
Parameter t·ª´ 0.0 ‚Üí 1.0 control m·ª©c ƒë·ªô chuy√™n h√≥a:
- 0.0: Balanced behavior
- 0.5: Moderate specialization
- 1.0: Extreme specialization

================================================================================
                        5. DECISION TREE CLASSIFIER
================================================================================

5.1 PURPOSE:
Decision Tree ph√¢n t√≠ch game context ƒë·ªÉ ch·ªçn strategy ph√π h·ª£p, d·ª±a tr√™n:
- Game phase (Early/Mid/End)
- Position advantage
- Resource advantage
- Mobility situation

5.2 FEATURE EXTRACTION:

```csharp
public struct DecisionFeatures
{
    public int aiDistanceToGoal;      // AI distance to goal line
    public int humanDistanceToGoal;   // Human distance to goal line
    public int aiWallsLeft;           // AI walls remaining
    public int humanWallsLeft;        // Human walls remaining
    public int aiMobility;            // AI possible moves
    public int humanMobility;         // Human possible moves
    public float distanceAdvantage;   // Human - AI distance
    public float wallAdvantage;       // AI - Human walls
    public bool isEarlyGame;          // > 15 moves estimated remaining
    public bool isMidGame;            // 5-15 moves remaining
    public bool isEndGame;            // < 5 moves remaining
}
```

5.3 DECISION LOGIC:

```csharp
public AIStrategy Predict(DecisionFeatures features)
{
    // Early Game: Play conservatively
    if (features.isEarlyGame)
        return AIStrategy.Balanced;
    
    // AI has significant lead: Focus on blocking
    if (features.distanceAdvantage > 2)
        return AIStrategy.Blocking;
    
    // AI is behind: Be aggressive
    if (features.distanceAdvantage < -1)
        return AIStrategy.Aggressive;
    
    // End game with wall advantage: Play defensively
    if (features.isEndGame && features.wallAdvantage > 2)
        return AIStrategy.Defensive;
    
    // Close game: Balanced approach
    return AIStrategy.Balanced;
}
```

5.4 STRATEGY OUTCOMES:
- Balanced: Equal weight to movement v√† walls
- Blocking: Prioritize moves that increase opponent path
- Aggressive: Focus on attacking positions
- Defensive: Protect own position and path

================================================================================
                        6. ENTROPY ANALYSIS & SOFTMAX
================================================================================

6.1 ENTROPY CONCEPT:
Entropy ƒëo ƒë·ªô ƒëa d·∫°ng/uncertainty c·ªßa move scores. High entropy = nhi·ªÅu moves
c√≥ scores t∆∞∆°ng ƒë∆∞∆°ng, low entropy = m·ªôt move r√µ r√†ng t·ªët nh·∫•t.

6.2 ENTROPY CALCULATION:

```csharp
float CalculateEntropy(List<float> moveScores)
{
    // Normalize th√†nh probabilities
    float total = moveScores.Sum();
    var probabilities = moveScores.Select(s => s / total);
    
    // Calculate Shannon entropy
    float entropy = 0f;
    foreach (float p in probabilities)
    {
        if (p > 0f)
            entropy -= p * Mathf.Log(p, 2f);
    }
    
    return entropy; // Range: 0 (deterministic) to log2(n) (uniform)
}
```

6.3 SOFTMAX DISTRIBUTION:

```csharp
float[] CalculateSoftmax(List<float> scores, float temperature)
{
    float[] probabilities = new float[scores.Count];
    float sum = scores.Sum(s => Mathf.Exp(s / temperature));
    
    for (int i = 0; i < scores.Count; i++)
    {
        probabilities[i] = Mathf.Exp(scores[i] / temperature) / sum;
    }
    
    return probabilities;
}
```

6.4 TEMPERATURE PARAMETER:
- Low temperature (0.1): Sharp distribution, focus on best moves
- High temperature (2.0): Flat distribution, more exploration
- Default temperature (1.0): Balanced exploitation/exploration

6.5 ADAPTIVE MOVE SELECTION:

```csharp
public AIMove ChooseActionWithEntropy(GameState state, List<AIMove> possibleMoves)
{
    var moveScores = EvaluateAllMoves(state, possibleMoves);
    float entropy = CalculateEntropy(moveScores);
    
    if (entropy > entropyThreshold) // High diversity
    {
        // Use softmax for probabilistic selection
        var probabilities = CalculateSoftmax(moveScores, softmaxTemperature);
        return SelectMoveByProbability(possibleMoves, probabilities);
    }
    else // Clear best move
    {
        // Use deterministic best move
        return GetBestScoredMove(possibleMoves, moveScores);
    }
}
```

================================================================================
                        7. TRAINING SYSTEM & SELF-PLAY
================================================================================

7.1 SELF-PLAY CONCEPT:
AI h·ªçc b·∫±ng c√°ch ch∆°i v·ªõi ch√≠nh n√≥ h√†ng ng√†n l·∫ßn, gradually improve performance
th√¥ng qua trial v√† error.

7.2 TRAINING PIPELINE:

```csharp
IEnumerator RunSelfPlayTraining()
{
    for (int episode = 1; episode <= maxEpisodes; episode++)
    {
        // Reset game state
        ResetGameForTraining();
        
        while (!IsGameOver())
        {
            // Get current state
            GameState currentState = GetCurrentGameState();
            List<AIMove> possibleMoves = GetPossibleMoves();
            
            // Choose action using Œµ-greedy
            AIMove chosenMove = qLearningAgent.ChooseAction(currentState, possibleMoves);
            
            // Execute move
            ExecuteMove(chosenMove);
            
            // Calculate reward
            float reward = CalculateReward(currentState, chosenMove);
            
            // Update Q-table
            GameState newState = GetCurrentGameState();
            qLearningAgent.UpdateQValue(
                EncodeState(currentState),
                EncodeAction(chosenMove),
                reward,
                EncodeState(newState)
            );
            
            yield return new WaitForEndOfFrame();
        }
        
        // Epsilon decay
        qLearningAgent.DecayEpsilon();
        
        // Save progress periodically
        if (episode % 100 == 0)
        {
            qLearningAgent.SaveQTable();
        }
    }
}
```

7.3 REWARD ENGINEERING:
Critical cho Q-Learning success:

```csharp
float CalculateReward(GameState state, AIMove move)
{
    float reward = 0f;
    
    // Game outcome rewards
    if (IsWin(state)) reward += 1000f;
    if (IsLoss(state)) reward -= 1000f;
    
    // Progress rewards
    if (move.moveType == MoveType.Movement)
    {
        if (IsMoveTowardGoal(move)) reward += 20f;
        else reward -= 5f;
    }
    
    // Strategic rewards
    if (move.moveType == MoveType.WallPlacement)
    {
        if (IsWallBlockingOpponent(move)) reward += 50f;
        if (IsWallStrategic(move)) reward += 30f;
        if (IsWallWasted(move)) reward -= 25f;
    }
    
    // Style-specific rewards
    reward += GetStyleSpecificReward(state, move);
    
    return reward;
}
```

7.4 TRAINING METRICS:
- Episodes: 1000 standard
- Duration: ~15 minutes
- Convergence: Usually around episode 800
- Q-table size: 13,118+ states at completion
- Final epsilon: 0.1 (90% exploitation)

================================================================================
                        8. AI TESTING FRAMEWORK
================================================================================

8.1 TESTING PHILOSOPHY:
Comprehensive testing ensures AI quality v√† reliability across different
game scenarios v√† edge cases.

8.2 TEST CATEGORIES:

A) DECISION TREE TESTS:
```csharp
void TestDecisionTreeStrategies()
{
    // Test Early Game
    var earlyGameFeatures = new DecisionFeatures
    {
        aiDistanceToGoal = 8, humanDistanceToGoal = 8,
        isEarlyGame = true
    };
    var strategy = decisionTree.Predict(earlyGameFeatures);
    Assert.AreEqual(AIStrategy.Balanced, strategy);
    
    // Test AI Leading
    var leadingFeatures = new DecisionFeatures
    {
        distanceAdvantage = 3, // AI leading
        isMidGame = true
    };
    strategy = decisionTree.Predict(leadingFeatures);
    Assert.AreEqual(AIStrategy.Blocking, strategy);
}
```

B) ENTROPY ANALYSIS TESTS:
```csharp
void TestEntropyAnalysis()
{
    // High entropy case (diverse scores)
    var highEntropyScores = new List<float> { 100f, 98f, 97f, 95f, 94f };
    float entropy = CalculateTestEntropy(highEntropyScores);
    Assert.IsTrue(entropy > 0.5f);
    
    // Low entropy case (clear winner)
    var lowEntropyScores = new List<float> { 100f, 50f, 50f, 50f, 50f };
    entropy = CalculateTestEntropy(lowEntropyScores);
    Assert.IsTrue(entropy < 0.5f);
}
```

C) SOFTMAX DISTRIBUTION TESTS:
```csharp
void TestSoftmaxDistribution()
{
    var testScores = new List<float> { 100f, 98f, 95f, 90f };
    float temperature = 1.0f;
    
    var probabilities = CalculateSoftmax(testScores, temperature);
    
    // Verify probabilities sum to 1
    float sum = probabilities.Sum();
    Assert.AreEqual(1.0f, sum, 0.001f);
    
    // Verify highest score gets highest probability
    int maxIndex = Array.IndexOf(probabilities, probabilities.Max());
    Assert.AreEqual(0, maxIndex);
}
```

D) PERFORMANCE BENCHMARKS:
- Move generation time: <100ms
- Q-Learning decision: <50ms
- Minimax decision (depth 4): <200ms
- Memory usage: <256MB

8.3 AUTOMATED TESTING:
Tests run automatically ƒë·ªÉ ensure code quality:
- Unit tests cho individual algorithms
- Integration tests cho AI system interactions
- Performance tests cho response times
- Regression tests ƒë·ªÉ prevent quality degradation

================================================================================
                        9. HYBRID AI ARCHITECTURE
================================================================================

9.1 CONCEPT:
Hybrid system k·∫øt h·ª£p strengths c·ªßa multiple algorithms:
- Q-Learning: Data-driven, learns from experience
- Minimax: Logical, guaranteed optimal given evaluation
- Decision Tree: Fast, context-aware switching
- Entropy: Adaptive, handles uncertainty

9.2 ALGORITHM SELECTION LOGIC:

```csharp
public AIMove GetBestMove()
{
    UpdateGameState();
    List<AIMove> possibleMoves = GeneratePossibleMoves();
    
    // Primary: Use Q-Learning if trained model available
    if (useQLearning && qAgent.IsTrainedModel())
    {
        Debug.Log($"üß† Using {specializedStyle} Q-Learning");
        return qAgent.ChooseAction(currentGameState, possibleMoves);
    }
    
    // Fallback: Use Minimax for guaranteed reasonable play
    else
    {
        Debug.Log($"üîç Using {specializedStyle} Minimax");
        return FindBestMoveWithMinimax();
    }
}
```

9.3 SEAMLESS SWITCHING:
- No interruption in gameplay
- Maintains consistent API
- Preserves game state integrity
- Logs algorithm used for debugging

9.4 BENEFITS:
- Robustness: Fallback ensures AI never fails
- Performance: Q-Learning for trained scenarios, Minimax for edge cases
- Flexibility: Can switch algorithms based on situation
- Development: Easy to test individual components

================================================================================
                        10. PERFORMANCE OPTIMIZATION
================================================================================

10.1 Q-TABLE OPTIMIZATION:
- Sparse representation: Only store visited states
- Compression: String interning for state keys
- Lazy loading: Load Q-table on demand
- Periodic saving: Every 100 training episodes

10.2 MINIMAX OPTIMIZATION:
- Alpha-Beta pruning: 50% search reduction
- Move ordering: Promising moves first
- Iterative deepening: Gradual depth increase
- Transposition tables: Cache evaluated positions

10.3 MEMORY MANAGEMENT:
- Object pooling cho GameState objects
- String caching cho repeated state/action encodings
- Garbage collection optimization
- Memory profiling v√† monitoring

10.4 MULTITHREADING CONSIDERATIONS:
- Q-Learning training: Background thread
- Move calculation: Main thread (Unity requirement)
- File I/O: Async operations where possible
- Thread-safe collections cho shared data

10.5 CACHING STRATEGIES:
- Path finding results
- Move generation results
- Evaluation function results
- Feature extraction results

================================================================================
                            K·∫æT LU·∫¨N
================================================================================

D·ª± √°n AI Quoridor Project th·ªÉ hi·ªán m·ªôt implementation comprehensive c·ªßa modern
game AI techniques:

TECHNICAL INNOVATIONS:
‚úÖ Hybrid Q-Learning + Minimax architecture
‚úÖ 6 specialized AI personalities v·ªõi separate training
‚úÖ Advanced decision making v·ªõi entropy analysis
‚úÖ Comprehensive testing v√† validation framework
‚úÖ Professional development tools v√† debugging

EDUCATIONAL VALUE:
‚úÖ Practical machine learning implementation
‚úÖ Traditional game AI algorithms
‚úÖ Modern software engineering practices
‚úÖ Performance optimization techniques
‚úÖ Automated testing methodologies

FUTURE ENHANCEMENTS:
üöÄ Deep Q-Learning (DQN) integration
üöÄ Multi-agent reinforcement learning
üöÄ Neural network-based evaluation functions
üöÄ Online learning v√† adaptation
üöÄ Transfer learning between game variants

D·ª± √°n n√†y demonstrating successful integration c·ªßa classical AI methods v·ªõi
modern machine learning, t·∫°o ra m·ªôt intelligent, adaptive, v√† highly
interactive gaming experience.

================================================================================
                            APPENDIX
================================================================================

FILES LI√äN QUAN:
- Assets/scripts/QLearningAgent.cs - Q-Learning implementation
- Assets/scripts/QuoridorAI.cs - Main AI controller
- Assets/scripts/DecisionTreeClassifier.cs - Decision tree logic
- Assets/scripts/QuoridorSelfPlayTrainer.cs - Training system
- Assets/scripts/AITestScript.cs - Testing framework
- Assets/scripts/Editor/ForceRetrainAI.cs - Development tools

Q-TABLE FILES:
- StreamingAssets/qtable.json - Balanced style
- StreamingAssets/qtable_aggressive.json - Aggressive style
- StreamingAssets/qtable_defensive.json - Defensive style
- StreamingAssets/qtable_wallmaster.json - Wall Master style
- StreamingAssets/qtable_speedrunner.json - Speed Runner style
- StreamingAssets/qtable_tactical.json - Tactical style

DOCUMENTATION:
- AI_ALGORITHMS_DETAILED_DESCRIPTION.txt - This file
- README.md - Project overview
- API_INTEGRATION_GUIDE.md - Voice/Chat integration
- TRAINING_GUIDE.md - AI training instructions

================================================================================
================================================================================
                    AI QUORIDOR GAME PROJECT - THU·∫¨T TO√ÅN AI CHI TI·∫æT
================================================================================

D·ª± √°n: AI Quoridor Game v·ªõi Machine Learning v√† Multi-Platform Integration
Ng√†y t·∫°o: 25/07/2025
T√°c gi·∫£: [Student Name]
Platform: Unity 3D
Language: C#

================================================================================
                            M·ª§C L·ª§C
================================================================================

1. T·ªîNG QUAN KI·∫æN TR√öC AI
2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
4. SPECIALIZED AI STYLES SYSTEM
5. DECISION TREE CLASSIFIER
6. ENTROPY ANALYSIS & SOFTMAX
7. TRAINING SYSTEM & SELF-PLAY
8. AI TESTING FRAMEWORK
9. HYBRID AI ARCHITECTURE
10. PERFORMANCE OPTIMIZATION

================================================================================
                        1. T·ªîNG QUAN KI·∫æN TR√öC AI
================================================================================

D·ª± √°n AI Quoridor s·ª≠ d·ª•ng m·ªôt ki·∫øn tr√∫c AI hybrid ti√™n ti·∫øn, k·∫øt h·ª£p:

1.1 CORE ALGORITHMS:
- Q-Learning (Reinforcement Learning) - Thu·∫≠t to√°n ch√≠nh
- Minimax + Alpha-Beta Pruning - Thu·∫≠t to√°n fallback
- Decision Tree Classifier - Ph√¢n t√≠ch ng·ªØ c·∫£nh game
- Entropy Analysis - ƒê√°nh gi√° ƒëa d·∫°ng move

1.2 SPECIALIZED SYSTEMS:
- 6 AI Personality Styles v·ªõi training ri√™ng bi·ªát
- Self-Play Training Pipeline t·ª± ƒë·ªông
- Real-time Performance Testing Framework
- Dynamic Algorithm Switching

1.3 TECHNICAL FOUNDATION:
- State Space: 13,118+ unique game states
- Action Space: Movement + Wall placement combinations
- Training Method: Self-play v·ªõi 1000 episodes
- Learning Rate: Œ± = 0.1, Discount Factor: Œ≥ = 0.99

================================================================================
                    2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
================================================================================

2.1 GI·ªöI THI·ªÜU Q-LEARNING:
Q-Learning l√† thu·∫≠t to√°n Reinforcement Learning kh√¥ng c·∫ßn model, h·ªçc th√¥ng qua
t∆∞∆°ng t√°c v·ªõi m√¥i tr∆∞·ªùng ƒë·ªÉ t·ªëi ∆∞u h√≥a policy. Trong Quoridor, AI h·ªçc c√°ch
ch·ªçn moves t·ªëi ∆∞u d·ª±a tr√™n kinh nghi·ªám t·ª´ h√†ng ng√†n game.

2.2 C√îNG TH·ª®C TO√ÅN H·ªåC:
Q(s,a) = Q(s,a) + Œ±[r + Œ≥*max(Q(s',a')) - Q(s,a)]

Trong ƒë√≥:
- Q(s,a): Q-value c·ªßa state s v·ªõi action a
- Œ±: Learning rate (0.1) - t·ªëc ƒë·ªô h·ªçc
- r: Immediate reward t·ª´ action
- Œ≥: Discount factor (0.99) - t·∫ßm quan tr·ªçng future rewards
- s': Next state sau khi th·ª±c hi·ªán action
- max(Q(s',a')): Maximum Q-value c·ªßa next state

2.3 IMPLEMENTATION CHI TI·∫æT:

```csharp
public class QLearningAgent
{
    // Core Q-Table: Dictionary l∆∞u tr·ªØ Q-values
    private Dictionary<string, Dictionary<string, float>> qTable;
    
    // Hyperparameters
    private float learningRate = 0.1f;      // T·ªëc ƒë·ªô h·ªçc
    private float discountFactor = 0.99f;   // Discount future rewards
    private float epsilon = 0.995f;         // Exploration rate
    
    // Q-Learning Update Implementation
    public void UpdateQValue(string state, string action, float reward, string nextState)
    {
        // L·∫•y Q-value hi·ªán t·∫°i
        float currentQ = GetQValue(state, action);
        
        // T√¨m max Q-value c·ªßa next state
        float maxNextQ = GetMaxQValue(nextState);
        
        // √Åp d·ª•ng c√¥ng th·ª©c Q-Learning
        float newQ = currentQ + learningRate * (reward + discountFactor * maxNextQ - currentQ);
        
        // C·∫≠p nh·∫≠t Q-table
        SetQValue(state, action, newQ);
    }
}
```

2.4 STATE ENCODING:
Game state ƒë∆∞·ª£c encode th√†nh string duy nh·∫•t:
"aiPos_x,aiPos_y-humanPos_x,humanPos_y-aiWalls-humanWalls-wallsList"

V√≠ d·ª•: "4,8-4,0-9-10-7,4,False|5,6,True"
- AI ·ªü (4,8), Human ·ªü (4,0)
- AI c√≥ 9 walls, Human c√≥ 10 walls
- Walls ƒë√£ ƒë·∫∑t: (7,4,False), (5,6,True)

2.5 ACTION ENCODING:
Actions ƒë∆∞·ª£c encode th√†nh string:
- Movement: "M:x,y" (v√≠ d·ª•: "M:4,7")
- Wall: "W:x,y,horizontal" (v√≠ d·ª•: "W:3,5,True")

2.6 EPSILON-GREEDY STRATEGY:
```csharp
public AIMove ChooseAction(GameState state, List<AIMove> possibleMoves)
{
    // Exploration vs Exploitation trade-off
    if (Random.Range(0f, 1f) < epsilon)
    {
        // EXPLORATION: Ch·ªçn random ƒë·ªÉ kh√°m ph√°
        return possibleMoves[Random.Range(0, possibleMoves.Count)];
    }
    else
    {
        // EXPLOITATION: Ch·ªçn action c√≥ Q-value cao nh·∫•t
        return GetBestAction(state, possibleMoves);
    }
}
```

2.7 EPSILON DECAY:
Epsilon gi·∫£m d·∫ßn t·ª´ 0.995 ‚Üí 0.1 qua training:
- Ban ƒë·∫ßu: High exploration (99.5%)
- Cu·ªëi training: High exploitation (90%)

2.8 REWARD SYSTEM:
- Win game: +1000
- Lose game: -1000
- Move toward goal: +10
- Block opponent: +50
- Waste move: -5
- Strategic position: +15

================================================================================
                3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
================================================================================

3.1 GI·ªöI THI·ªÜU MINIMAX:
Minimax l√† thu·∫≠t to√°n game theory c·ªï ƒëi·ªÉn, t√¨m ki·∫øm move t·ªëi ∆∞u b·∫±ng c√°ch
d·ª± ƒëo√°n tr∆∞·ªõc nhi·ªÅu moves. Alpha-Beta pruning t·ªëi ∆∞u hi·ªáu su·∫•t b·∫±ng c√°ch
lo·∫°i b·ªè c√°c nh√°nh kh√¥ng c·∫ßn thi·∫øt.

3.2 NGUY√äN L√ù HO·∫†T ƒê·ªòNG:
- Maximizing Player (AI): T·ªëi ƒëa h√≥a score
- Minimizing Player (Human): T·ªëi thi·ªÉu h√≥a score AI
- Search depth: 4 levels (configurable)
- Pruning: Lo·∫°i b·ªè nh√°nh kh√¥ng promising

3.3 IMPLEMENTATION:

```csharp
float Minimax(GameState state, int depth, bool isMaximizing, float alpha, float beta)
{
    // Base case: Depth = 0 ho·∫∑c game over
    if (depth == 0 || IsGameOver(state))
        return EvaluatePosition(state);
    
    if (isMaximizing) // AI turn
    {
        float maxEval = float.NegativeInfinity;
        foreach (var move in GeneratePossibleMoves(state, true))
        {
            GameState newState = ApplyMove(state, move, true);
            float eval = Minimax(newState, depth - 1, false, alpha, beta);
            maxEval = Mathf.Max(maxEval, eval);
            alpha = Mathf.Max(alpha, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return maxEval;
    }
    else // Human turn
    {
        float minEval = float.PositiveInfinity;
        foreach (var move in GeneratePossibleMoves(state, false))
        {
            GameState newState = ApplyMove(state, move, false);
            float eval = Minimax(newState, depth - 1, true, alpha, beta);
            minEval = Mathf.Min(minEval, eval);
            beta = Mathf.Min(beta, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return minEval;
    }
}
```

3.4 EVALUATION FUNCTION:
Position evaluation d·ª±a tr√™n multiple factors:

```csharp
float EvaluatePosition(GameState state)
{
    float score = 0f;
    
    // Distance to goal (c√†ng g·∫ßn c√†ng t·ªët)
    int aiDistance = GetShortestPath(state.aiPosition, goalRow);
    int humanDistance = GetShortestPath(state.humanPosition, goalRow);
    score += (humanDistance - aiDistance) * 100;
    
    // Wall advantage
    score += (state.aiWallsLeft - state.humanWallsLeft) * 50;
    
    // Mobility (s·ªë moves c√≥ th·ªÉ)
    int aiMobility = GetValidMoves(state.aiPosition).Count;
    int humanMobility = GetValidMoves(state.humanPosition).Count;
    score += (aiMobility - humanMobility) * 20;
    
    // Center control
    score += GetCenterControlScore(state) * 30;
    
    return score;
}
```

3.5 ALPHA-BETA PRUNING HI·ªÜU QU·∫¢:
- Reduces search space t·ª´ O(b^d) xu·ªëng O(b^(d/2))
- Best case: 50% reduction in nodes evaluated
- Ordering moves by promising gi√∫p pruning hi·ªáu qu·∫£ h∆°n

================================================================================
                        4. SPECIALIZED AI STYLES SYSTEM
================================================================================

4.1 CONCEPT:
6 AI personalities v·ªõi training v√† behavior ri√™ng bi·ªát, m·ªói style c√≥:
- Ri√™ng Q-table file
- Custom reward structure
- Specific move prioritization
- Unique decision making logic

4.2 AI STYLES CHI TI·∫æT:

üî• AGGRESSIVE STYLE:
- M·ª•c ti√™u: T·∫•n c√¥ng v√† ch·∫∑n ƒë·ªëi th·ªß
- Wall Usage: 70% chance
- Reward Structure:
  * blockingOpponentReward = 50 * intensity
  * wallPlacementReward = 30 * intensity
  * defensiveWallPenalty = -20 * intensity
- Q-table: qtable_aggressive.json

üõ°Ô∏è DEFENSIVE STYLE:
- M·ª•c ti√™u: Ph√≤ng th·ªß v√† di chuy·ªÉn an to√†n
- Wall Usage: 50% chance
- Reward Structure:
  * defensiveWallReward = 35 * intensity
  * movementReward = 20 * intensity
  * aggressiveMovePenalty = -15 * intensity
- Q-table: qtable_defensive.json

üß± WALL MASTER STYLE:
- M·ª•c ti√™u: Chuy√™n gia ƒë·∫∑t t∆∞·ªùng v√† t·∫°o m√™ cung
- Wall Usage: 90% chance
- Reward Structure:
  * wallPlacementReward = 60 * intensity
  * strategicWallReward = 40 * intensity
  * wallComboReward = 50 * intensity
  * movementWithoutWallPenalty = -25 * intensity
- Q-table: qtable_wallmaster.json

‚ö° SPEED RUNNER STYLE:
- M·ª•c ti√™u: ƒê·∫°t ƒë√≠ch nhanh nh·∫•t
- Wall Usage: 10% chance
- Reward Structure:
  * movementReward = 40 * intensity
  * progressToGoalReward = 50 * intensity
  * quickWinBonus = 100 * intensity
  * wallPlacementPenalty = -20 * intensity
- Q-table: qtable_speedrunner.json

üß† TACTICAL STYLE:
- M·ª•c ti√™u: L·ª£i th·∫ø v·ªã tr√≠ v√† k·∫ø ho·∫°ch d√†i h·∫°n
- Wall Usage: Variable (strategic)
- Reward Structure:
  * tacticalMoveReward = 45 * intensity
  * positionAdvantageReward = 35 * intensity
  * mobilityReward = 25 * intensity
  * patternRecognitionBonus = 40 * intensity
- Q-table: qtable_tactical.json

‚öñÔ∏è BALANCED STYLE:
- M·ª•c ti√™u: C√¢n b·∫±ng t·∫•t c·∫£ aspects
- Wall Usage: 30% chance
- Reward Structure: Default balanced values
- Q-table: qtable.json

4.3 STYLE SWITCHING MECHANISM:

```csharp
public void SwitchToStyle(PlayStyle newStyle)
{
    // Save current Q-table
    if (allowQTableSaving) qAgent.SaveQTable(GetStyleSpecificQTablePath());
    
    // Load new style Q-table
    specializedStyle = newStyle;
    qAgent.LoadQTable(GetStyleSpecificQTablePath());
    
    // Reconfigure parameters
    ConfigureStyleSpecificTraining();
}
```

4.4 STYLE INTENSITY:
Parameter t·ª´ 0.0 ‚Üí 1.0 control m·ª©c ƒë·ªô chuy√™n h√≥a:
- 0.0: Balanced behavior
- 0.5: Moderate specialization
- 1.0: Extreme specialization

================================================================================
                        5. DECISION TREE CLASSIFIER
================================================================================

5.1 PURPOSE:
Decision Tree ph√¢n t√≠ch game context ƒë·ªÉ ch·ªçn strategy ph√π h·ª£p, d·ª±a tr√™n:
- Game phase (Early/Mid/End)
- Position advantage
- Resource advantage
- Mobility situation

5.2 FEATURE EXTRACTION:

```csharp
public struct DecisionFeatures
{
    public int aiDistanceToGoal;      // AI distance to goal line
    public int humanDistanceToGoal;   // Human distance to goal line
    public int aiWallsLeft;           // AI walls remaining
    public int humanWallsLeft;        // Human walls remaining
    public int aiMobility;            // AI possible moves
    public int humanMobility;         // Human possible moves
    public float distanceAdvantage;   // Human - AI distance
    public float wallAdvantage;       // AI - Human walls
    public bool isEarlyGame;          // > 15 moves estimated remaining
    public bool isMidGame;            // 5-15 moves remaining
    public bool isEndGame;            // < 5 moves remaining
}
```

5.3 DECISION LOGIC:

```csharp
public AIStrategy Predict(DecisionFeatures features)
{
    // Early Game: Play conservatively
    if (features.isEarlyGame)
        return AIStrategy.Balanced;
    
    // AI has significant lead: Focus on blocking
    if (features.distanceAdvantage > 2)
        return AIStrategy.Blocking;
    
    // AI is behind: Be aggressive
    if (features.distanceAdvantage < -1)
        return AIStrategy.Aggressive;
    
    // End game with wall advantage: Play defensively
    if (features.isEndGame && features.wallAdvantage > 2)
        return AIStrategy.Defensive;
    
    // Close game: Balanced approach
    return AIStrategy.Balanced;
}
```

5.4 STRATEGY OUTCOMES:
- Balanced: Equal weight to movement v√† walls
- Blocking: Prioritize moves that increase opponent path
- Aggressive: Focus on attacking positions
- Defensive: Protect own position and path

================================================================================
                        6. ENTROPY ANALYSIS & SOFTMAX
================================================================================

6.1 ENTROPY CONCEPT:
Entropy ƒëo ƒë·ªô ƒëa d·∫°ng/uncertainty c·ªßa move scores. High entropy = nhi·ªÅu moves
c√≥ scores t∆∞∆°ng ƒë∆∞∆°ng, low entropy = m·ªôt move r√µ r√†ng t·ªët nh·∫•t.

6.2 ENTROPY CALCULATION:

```csharp
float CalculateEntropy(List<float> moveScores)
{
    // Normalize th√†nh probabilities
    float total = moveScores.Sum();
    var probabilities = moveScores.Select(s => s / total);
    
    // Calculate Shannon entropy
    float entropy = 0f;
    foreach (float p in probabilities)
    {
        if (p > 0f)
            entropy -= p * Mathf.Log(p, 2f);
    }
    
    return entropy; // Range: 0 (deterministic) to log2(n) (uniform)
}
```

6.3 SOFTMAX DISTRIBUTION:

```csharp
float[] CalculateSoftmax(List<float> scores, float temperature)
{
    float[] probabilities = new float[scores.Count];
    float sum = scores.Sum(s => Mathf.Exp(s / temperature));
    
    for (int i = 0; i < scores.Count; i++)
    {
        probabilities[i] = Mathf.Exp(scores[i] / temperature) / sum;
    }
    
    return probabilities;
}
```

6.4 TEMPERATURE PARAMETER:
- Low temperature (0.1): Sharp distribution, focus on best moves
- High temperature (2.0): Flat distribution, more exploration
- Default temperature (1.0): Balanced exploitation/exploration

6.5 ADAPTIVE MOVE SELECTION:

```csharp
public AIMove ChooseActionWithEntropy(GameState state, List<AIMove> possibleMoves)
{
    var moveScores = EvaluateAllMoves(state, possibleMoves);
    float entropy = CalculateEntropy(moveScores);
    
    if (entropy > entropyThreshold) // High diversity
    {
        // Use softmax for probabilistic selection
        var probabilities = CalculateSoftmax(moveScores, softmaxTemperature);
        return SelectMoveByProbability(possibleMoves, probabilities);
    }
    else // Clear best move
    {
        // Use deterministic best move
        return GetBestScoredMove(possibleMoves, moveScores);
    }
}
```

================================================================================
                        7. TRAINING SYSTEM & SELF-PLAY
================================================================================

7.1 SELF-PLAY CONCEPT:
AI h·ªçc b·∫±ng c√°ch ch∆°i v·ªõi ch√≠nh n√≥ h√†ng ng√†n l·∫ßn, gradually improve performance
th√¥ng qua trial v√† error.

7.2 TRAINING PIPELINE:

```csharp
IEnumerator RunSelfPlayTraining()
{
    for (int episode = 1; episode <= maxEpisodes; episode++)
    {
        // Reset game state
        ResetGameForTraining();
        
        while (!IsGameOver())
        {
            // Get current state
            GameState currentState = GetCurrentGameState();
            List<AIMove> possibleMoves = GetPossibleMoves();
            
            // Choose action using Œµ-greedy
            AIMove chosenMove = qLearningAgent.ChooseAction(currentState, possibleMoves);
            
            // Execute move
            ExecuteMove(chosenMove);
            
            // Calculate reward
            float reward = CalculateReward(currentState, chosenMove);
            
            // Update Q-table
            GameState newState = GetCurrentGameState();
            qLearningAgent.UpdateQValue(
                EncodeState(currentState),
                EncodeAction(chosenMove),
                reward,
                EncodeState(newState)
            );
            
            yield return new WaitForEndOfFrame();
        }
        
        // Epsilon decay
        qLearningAgent.DecayEpsilon();
        
        // Save progress periodically
        if (episode % 100 == 0)
        {
            qLearningAgent.SaveQTable();
        }
    }
}
```

7.3 REWARD ENGINEERING:
Critical cho Q-Learning success:

```csharp
float CalculateReward(GameState state, AIMove move)
{
    float reward = 0f;
    
    // Game outcome rewards
    if (IsWin(state)) reward += 1000f;
    if (IsLoss(state)) reward -= 1000f;
    
    // Progress rewards
    if (move.moveType == MoveType.Movement)
    {
        if (IsMoveTowardGoal(move)) reward += 20f;
        else reward -= 5f;
    }
    
    // Strategic rewards
    if (move.moveType == MoveType.WallPlacement)
    {
        if (IsWallBlockingOpponent(move)) reward += 50f;
        if (IsWallStrategic(move)) reward += 30f;
        if (IsWallWasted(move)) reward -= 25f;
    }
    
    // Style-specific rewards
    reward += GetStyleSpecificReward(state, move);
    
    return reward;
}
```

7.4 TRAINING METRICS:
- Episodes: 1000 standard
- Duration: ~15 minutes
- Convergence: Usually around episode 800
- Q-table size: 13,118+ states at completion
- Final epsilon: 0.1 (90% exploitation)

================================================================================
                        8. AI TESTING FRAMEWORK
================================================================================

8.1 TESTING PHILOSOPHY:
Comprehensive testing ensures AI quality v√† reliability across different
game scenarios v√† edge cases.

8.2 TEST CATEGORIES:

A) DECISION TREE TESTS:
```csharp
void TestDecisionTreeStrategies()
{
    // Test Early Game
    var earlyGameFeatures = new DecisionFeatures
    {
        aiDistanceToGoal = 8, humanDistanceToGoal = 8,
        isEarlyGame = true
    };
    var strategy = decisionTree.Predict(earlyGameFeatures);
    Assert.AreEqual(AIStrategy.Balanced, strategy);
    
    // Test AI Leading
    var leadingFeatures = new DecisionFeatures
    {
        distanceAdvantage = 3, // AI leading
        isMidGame = true
    };
    strategy = decisionTree.Predict(leadingFeatures);
    Assert.AreEqual(AIStrategy.Blocking, strategy);
}
```

B) ENTROPY ANALYSIS TESTS:
```csharp
void TestEntropyAnalysis()
{
    // High entropy case (diverse scores)
    var highEntropyScores = new List<float> { 100f, 98f, 97f, 95f, 94f };
    float entropy = CalculateTestEntropy(highEntropyScores);
    Assert.IsTrue(entropy > 0.5f);
    
    // Low entropy case (clear winner)
    var lowEntropyScores = new List<float> { 100f, 50f, 50f, 50f, 50f };
    entropy = CalculateTestEntropy(lowEntropyScores);
    Assert.IsTrue(entropy < 0.5f);
}
```

C) SOFTMAX DISTRIBUTION TESTS:
```csharp
void TestSoftmaxDistribution()
{
    var testScores = new List<float> { 100f, 98f, 95f, 90f };
    float temperature = 1.0f;
    
    var probabilities = CalculateSoftmax(testScores, temperature);
    
    // Verify probabilities sum to 1
    float sum = probabilities.Sum();
    Assert.AreEqual(1.0f, sum, 0.001f);
    
    // Verify highest score gets highest probability
    int maxIndex = Array.IndexOf(probabilities, probabilities.Max());
    Assert.AreEqual(0, maxIndex);
}
```

D) PERFORMANCE BENCHMARKS:
- Move generation time: <100ms
- Q-Learning decision: <50ms
- Minimax decision (depth 4): <200ms
- Memory usage: <256MB

8.3 AUTOMATED TESTING:
Tests run automatically ƒë·ªÉ ensure code quality:
- Unit tests cho individual algorithms
- Integration tests cho AI system interactions
- Performance tests cho response times
- Regression tests ƒë·ªÉ prevent quality degradation

================================================================================
                        9. HYBRID AI ARCHITECTURE
================================================================================

9.1 CONCEPT:
Hybrid system k·∫øt h·ª£p strengths c·ªßa multiple algorithms:
- Q-Learning: Data-driven, learns from experience
- Minimax: Logical, guaranteed optimal given evaluation
- Decision Tree: Fast, context-aware switching
- Entropy: Adaptive, handles uncertainty

9.2 ALGORITHM SELECTION LOGIC:

```csharp
public AIMove GetBestMove()
{
    UpdateGameState();
    List<AIMove> possibleMoves = GeneratePossibleMoves();
    
    // Primary: Use Q-Learning if trained model available
    if (useQLearning && qAgent.IsTrainedModel())
    {
        Debug.Log($"üß† Using {specializedStyle} Q-Learning");
        return qAgent.ChooseAction(currentGameState, possibleMoves);
    }
    
    // Fallback: Use Minimax for guaranteed reasonable play
    else
    {
        Debug.Log($"üîç Using {specializedStyle} Minimax");
        return FindBestMoveWithMinimax();
    }
}
```

9.3 SEAMLESS SWITCHING:
- No interruption in gameplay
- Maintains consistent API
- Preserves game state integrity
- Logs algorithm used for debugging

9.4 BENEFITS:
- Robustness: Fallback ensures AI never fails
- Performance: Q-Learning for trained scenarios, Minimax for edge cases
- Flexibility: Can switch algorithms based on situation
- Development: Easy to test individual components

================================================================================
                        10. PERFORMANCE OPTIMIZATION
================================================================================

10.1 Q-TABLE OPTIMIZATION:
- Sparse representation: Only store visited states
- Compression: String interning for state keys
- Lazy loading: Load Q-table on demand
- Periodic saving: Every 100 training episodes

10.2 MINIMAX OPTIMIZATION:
- Alpha-Beta pruning: 50% search reduction
- Move ordering: Promising moves first
- Iterative deepening: Gradual depth increase
- Transposition tables: Cache evaluated positions

10.3 MEMORY MANAGEMENT:
- Object pooling cho GameState objects
- String caching cho repeated state/action encodings
- Garbage collection optimization
- Memory profiling v√† monitoring

10.4 MULTITHREADING CONSIDERATIONS:
- Q-Learning training: Background thread
- Move calculation: Main thread (Unity requirement)
- File I/O: Async operations where possible
- Thread-safe collections cho shared data

10.5 CACHING STRATEGIES:
- Path finding results
- Move generation results
- Evaluation function results
- Feature extraction results

================================================================================
                            K·∫æT LU·∫¨N
================================================================================

D·ª± √°n AI Quoridor Project th·ªÉ hi·ªán m·ªôt implementation comprehensive c·ªßa modern
game AI techniques:

TECHNICAL INNOVATIONS:
‚úÖ Hybrid Q-Learning + Minimax architecture
‚úÖ 6 specialized AI personalities v·ªõi separate training
‚úÖ Advanced decision making v·ªõi entropy analysis
‚úÖ Comprehensive testing v√† validation framework
‚úÖ Professional development tools v√† debugging

EDUCATIONAL VALUE:
‚úÖ Practical machine learning implementation
‚úÖ Traditional game AI algorithms
‚úÖ Modern software engineering practices
‚úÖ Performance optimization techniques
‚úÖ Automated testing methodologies

FUTURE ENHANCEMENTS:
üöÄ Deep Q-Learning (DQN) integration
üöÄ Multi-agent reinforcement learning
üöÄ Neural network-based evaluation functions
üöÄ Online learning v√† adaptation
üöÄ Transfer learning between game variants

D·ª± √°n n√†y demonstrating successful integration c·ªßa classical AI methods v·ªõi
modern machine learning, t·∫°o ra m·ªôt intelligent, adaptive, v√† highly
interactive gaming experience.

================================================================================
                            APPENDIX
================================================================================

FILES LI√äN QUAN:
- Assets/scripts/QLearningAgent.cs - Q-Learning implementation
- Assets/scripts/QuoridorAI.cs - Main AI controller
- Assets/scripts/DecisionTreeClassifier.cs - Decision tree logic
- Assets/scripts/QuoridorSelfPlayTrainer.cs - Training system
- Assets/scripts/AITestScript.cs - Testing framework
- Assets/scripts/Editor/ForceRetrainAI.cs - Development tools

Q-TABLE FILES:
- StreamingAssets/qtable.json - Balanced style
- StreamingAssets/qtable_aggressive.json - Aggressive style
- StreamingAssets/qtable_defensive.json - Defensive style
- StreamingAssets/qtable_wallmaster.json - Wall Master style
- StreamingAssets/qtable_speedrunner.json - Speed Runner style
- StreamingAssets/qtable_tactical.json - Tactical style

DOCUMENTATION:
- AI_ALGORITHMS_DETAILED_DESCRIPTION.txt - This file
- README.md - Project overview
- API_INTEGRATION_GUIDE.md - Voice/Chat integration
- TRAINING_GUIDE.md - AI training instructions

================================================================================
================================================================================
                    AI QUORIDOR GAME PROJECT - THU·∫¨T TO√ÅN AI CHI TI·∫æT
================================================================================

D·ª± √°n: AI Quoridor Game v·ªõi Machine Learning v√† Multi-Platform Integration
Ng√†y t·∫°o: 25/07/2025
T√°c gi·∫£: [Student Name]
Platform: Unity 3D
Language: C#

================================================================================
                            M·ª§C L·ª§C
================================================================================

1. T·ªîNG QUAN KI·∫æN TR√öC AI
2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
4. SPECIALIZED AI STYLES SYSTEM
5. DECISION TREE CLASSIFIER
6. ENTROPY ANALYSIS & SOFTMAX
7. TRAINING SYSTEM & SELF-PLAY
8. AI TESTING FRAMEWORK
9. HYBRID AI ARCHITECTURE
10. PERFORMANCE OPTIMIZATION

================================================================================
                        1. T·ªîNG QUAN KI·∫æN TR√öC AI
================================================================================

D·ª± √°n AI Quoridor s·ª≠ d·ª•ng m·ªôt ki·∫øn tr√∫c AI hybrid ti√™n ti·∫øn, k·∫øt h·ª£p:

1.1 CORE ALGORITHMS:
- Q-Learning (Reinforcement Learning) - Thu·∫≠t to√°n ch√≠nh
- Minimax + Alpha-Beta Pruning - Thu·∫≠t to√°n fallback
- Decision Tree Classifier - Ph√¢n t√≠ch ng·ªØ c·∫£nh game
- Entropy Analysis - ƒê√°nh gi√° ƒëa d·∫°ng move

1.2 SPECIALIZED SYSTEMS:
- 6 AI Personality Styles v·ªõi training ri√™ng bi·ªát
- Self-Play Training Pipeline t·ª± ƒë·ªông
- Real-time Performance Testing Framework
- Dynamic Algorithm Switching

1.3 TECHNICAL FOUNDATION:
- State Space: 13,118+ unique game states
- Action Space: Movement + Wall placement combinations
- Training Method: Self-play v·ªõi 1000 episodes
- Learning Rate: Œ± = 0.1, Discount Factor: Œ≥ = 0.99

================================================================================
                    2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
================================================================================

2.1 GI·ªöI THI·ªÜU Q-LEARNING:
Q-Learning l√† thu·∫≠t to√°n Reinforcement Learning kh√¥ng c·∫ßn model, h·ªçc th√¥ng qua
t∆∞∆°ng t√°c v·ªõi m√¥i tr∆∞·ªùng ƒë·ªÉ t·ªëi ∆∞u h√≥a policy. Trong Quoridor, AI h·ªçc c√°ch
ch·ªçn moves t·ªëi ∆∞u d·ª±a tr√™n kinh nghi·ªám t·ª´ h√†ng ng√†n game.

2.2 C√îNG TH·ª®C TO√ÅN H·ªåC:
Q(s,a) = Q(s,a) + Œ±[r + Œ≥*max(Q(s',a')) - Q(s,a)]

Trong ƒë√≥:
- Q(s,a): Q-value c·ªßa state s v·ªõi action a
- Œ±: Learning rate (0.1) - t·ªëc ƒë·ªô h·ªçc
- r: Immediate reward t·ª´ action
- Œ≥: Discount factor (0.99) - t·∫ßm quan tr·ªçng future rewards
- s': Next state sau khi th·ª±c hi·ªán action
- max(Q(s',a')): Maximum Q-value c·ªßa next state

2.3 IMPLEMENTATION CHI TI·∫æT:

```csharp
public class QLearningAgent
{
    // Core Q-Table: Dictionary l∆∞u tr·ªØ Q-values
    private Dictionary<string, Dictionary<string, float>> qTable;
    
    // Hyperparameters
    private float learningRate = 0.1f;      // T·ªëc ƒë·ªô h·ªçc
    private float discountFactor = 0.99f;   // Discount future rewards
    private float epsilon = 0.995f;         // Exploration rate
    
    // Q-Learning Update Implementation
    public void UpdateQValue(string state, string action, float reward, string nextState)
    {
        // L·∫•y Q-value hi·ªán t·∫°i
        float currentQ = GetQValue(state, action);
        
        // T√¨m max Q-value c·ªßa next state
        float maxNextQ = GetMaxQValue(nextState);
        
        // √Åp d·ª•ng c√¥ng th·ª©c Q-Learning
        float newQ = currentQ + learningRate * (reward + discountFactor * maxNextQ - currentQ);
        
        // C·∫≠p nh·∫≠t Q-table
        SetQValue(state, action, newQ);
    }
}
```

2.4 STATE ENCODING:
Game state ƒë∆∞·ª£c encode th√†nh string duy nh·∫•t:
"aiPos_x,aiPos_y-humanPos_x,humanPos_y-aiWalls-humanWalls-wallsList"

V√≠ d·ª•: "4,8-4,0-9-10-7,4,False|5,6,True"
- AI ·ªü (4,8), Human ·ªü (4,0)
- AI c√≥ 9 walls, Human c√≥ 10 walls
- Walls ƒë√£ ƒë·∫∑t: (7,4,False), (5,6,True)

2.5 ACTION ENCODING:
Actions ƒë∆∞·ª£c encode th√†nh string:
- Movement: "M:x,y" (v√≠ d·ª•: "M:4,7")
- Wall: "W:x,y,horizontal" (v√≠ d·ª•: "W:3,5,True")

2.6 EPSILON-GREEDY STRATEGY:
```csharp
public AIMove ChooseAction(GameState state, List<AIMove> possibleMoves)
{
    // Exploration vs Exploitation trade-off
    if (Random.Range(0f, 1f) < epsilon)
    {
        // EXPLORATION: Ch·ªçn random ƒë·ªÉ kh√°m ph√°
        return possibleMoves[Random.Range(0, possibleMoves.Count)];
    }
    else
    {
        // EXPLOITATION: Ch·ªçn action c√≥ Q-value cao nh·∫•t
        return GetBestAction(state, possibleMoves);
    }
}
```

2.7 EPSILON DECAY:
Epsilon gi·∫£m d·∫ßn t·ª´ 0.995 ‚Üí 0.1 qua training:
- Ban ƒë·∫ßu: High exploration (99.5%)
- Cu·ªëi training: High exploitation (90%)

2.8 REWARD SYSTEM:
- Win game: +1000
- Lose game: -1000
- Move toward goal: +10
- Block opponent: +50
- Waste move: -5
- Strategic position: +15

================================================================================
                3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
================================================================================

3.1 GI·ªöI THI·ªÜU MINIMAX:
Minimax l√† thu·∫≠t to√°n game theory c·ªï ƒëi·ªÉn, t√¨m ki·∫øm move t·ªëi ∆∞u b·∫±ng c√°ch
d·ª± ƒëo√°n tr∆∞·ªõc nhi·ªÅu moves. Alpha-Beta pruning t·ªëi ∆∞u hi·ªáu su·∫•t b·∫±ng c√°ch
lo·∫°i b·ªè c√°c nh√°nh kh√¥ng c·∫ßn thi·∫øt.

3.2 NGUY√äN L√ù HO·∫†T ƒê·ªòNG:
- Maximizing Player (AI): T·ªëi ƒëa h√≥a score
- Minimizing Player (Human): T·ªëi thi·ªÉu h√≥a score AI
- Search depth: 4 levels (configurable)
- Pruning: Lo·∫°i b·ªè nh√°nh kh√¥ng promising

3.3 IMPLEMENTATION:

```csharp
float Minimax(GameState state, int depth, bool isMaximizing, float alpha, float beta)
{
    // Base case: Depth = 0 ho·∫∑c game over
    if (depth == 0 || IsGameOver(state))
        return EvaluatePosition(state);
    
    if (isMaximizing) // AI turn
    {
        float maxEval = float.NegativeInfinity;
        foreach (var move in GeneratePossibleMoves(state, true))
        {
            GameState newState = ApplyMove(state, move, true);
            float eval = Minimax(newState, depth - 1, false, alpha, beta);
            maxEval = Mathf.Max(maxEval, eval);
            alpha = Mathf.Max(alpha, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return maxEval;
    }
    else // Human turn
    {
        float minEval = float.PositiveInfinity;
        foreach (var move in GeneratePossibleMoves(state, false))
        {
            GameState newState = ApplyMove(state, move, false);
            float eval = Minimax(newState, depth - 1, true, alpha, beta);
            minEval = Mathf.Min(minEval, eval);
            beta = Mathf.Min(beta, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return minEval;
    }
}
```

3.4 EVALUATION FUNCTION:
Position evaluation d·ª±a tr√™n multiple factors:

```csharp
float EvaluatePosition(GameState state)
{
    float score = 0f;
    
    // Distance to goal (c√†ng g·∫ßn c√†ng t·ªët)
    int aiDistance = GetShortestPath(state.aiPosition, goalRow);
    int humanDistance = GetShortestPath(state.humanPosition, goalRow);
    score += (humanDistance - aiDistance) * 100;
    
    // Wall advantage
    score += (state.aiWallsLeft - state.humanWallsLeft) * 50;
    
    // Mobility (s·ªë moves c√≥ th·ªÉ)
    int aiMobility = GetValidMoves(state.aiPosition).Count;
    int humanMobility = GetValidMoves(state.humanPosition).Count;
    score += (aiMobility - humanMobility) * 20;
    
    // Center control
    score += GetCenterControlScore(state) * 30;
    
    return score;
}
```

3.5 ALPHA-BETA PRUNING HI·ªÜU QU·∫¢:
- Reduces search space t·ª´ O(b^d) xu·ªëng O(b^(d/2))
- Best case: 50% reduction in nodes evaluated
- Ordering moves by promising gi√∫p pruning hi·ªáu qu·∫£ h∆°n

================================================================================
                        4. SPECIALIZED AI STYLES SYSTEM
================================================================================

4.1 CONCEPT:
6 AI personalities v·ªõi training v√† behavior ri√™ng bi·ªát, m·ªói style c√≥:
- Ri√™ng Q-table file
- Custom reward structure
- Specific move prioritization
- Unique decision making logic

4.2 AI STYLES CHI TI·∫æT:

üî• AGGRESSIVE STYLE:
- M·ª•c ti√™u: T·∫•n c√¥ng v√† ch·∫∑n ƒë·ªëi th·ªß
- Wall Usage: 70% chance
- Reward Structure:
  * blockingOpponentReward = 50 * intensity
  * wallPlacementReward = 30 * intensity
  * defensiveWallPenalty = -20 * intensity
- Q-table: qtable_aggressive.json

üõ°Ô∏è DEFENSIVE STYLE:
- M·ª•c ti√™u: Ph√≤ng th·ªß v√† di chuy·ªÉn an to√†n
- Wall Usage: 50% chance
- Reward Structure:
  * defensiveWallReward = 35 * intensity
  * movementReward = 20 * intensity
  * aggressiveMovePenalty = -15 * intensity
- Q-table: qtable_defensive.json

üß± WALL MASTER STYLE:
- M·ª•c ti√™u: Chuy√™n gia ƒë·∫∑t t∆∞·ªùng v√† t·∫°o m√™ cung
- Wall Usage: 90% chance
- Reward Structure:
  * wallPlacementReward = 60 * intensity
  * strategicWallReward = 40 * intensity
  * wallComboReward = 50 * intensity
  * movementWithoutWallPenalty = -25 * intensity
- Q-table: qtable_wallmaster.json

‚ö° SPEED RUNNER STYLE:
- M·ª•c ti√™u: ƒê·∫°t ƒë√≠ch nhanh nh·∫•t
- Wall Usage: 10% chance
- Reward Structure:
  * movementReward = 40 * intensity
  * progressToGoalReward = 50 * intensity
  * quickWinBonus = 100 * intensity
  * wallPlacementPenalty = -20 * intensity
- Q-table: qtable_speedrunner.json

üß† TACTICAL STYLE:
- M·ª•c ti√™u: L·ª£i th·∫ø v·ªã tr√≠ v√† k·∫ø ho·∫°ch d√†i h·∫°n
- Wall Usage: Variable (strategic)
- Reward Structure:
  * tacticalMoveReward = 45 * intensity
  * positionAdvantageReward = 35 * intensity
  * mobilityReward = 25 * intensity
  * patternRecognitionBonus = 40 * intensity
- Q-table: qtable_tactical.json

‚öñÔ∏è BALANCED STYLE:
- M·ª•c ti√™u: C√¢n b·∫±ng t·∫•t c·∫£ aspects
- Wall Usage: 30% chance
- Reward Structure: Default balanced values
- Q-table: qtable.json

4.3 STYLE SWITCHING MECHANISM:

```csharp
public void SwitchToStyle(PlayStyle newStyle)
{
    // Save current Q-table
    if (allowQTableSaving) qAgent.SaveQTable(GetStyleSpecificQTablePath());
    
    // Load new style Q-table
    specializedStyle = newStyle;
    qAgent.LoadQTable(GetStyleSpecificQTablePath());
    
    // Reconfigure parameters
    ConfigureStyleSpecificTraining();
}
```

4.4 STYLE INTENSITY:
Parameter t·ª´ 0.0 ‚Üí 1.0 control m·ª©c ƒë·ªô chuy√™n h√≥a:
- 0.0: Balanced behavior
- 0.5: Moderate specialization
- 1.0: Extreme specialization

================================================================================
                        5. DECISION TREE CLASSIFIER
================================================================================

5.1 PURPOSE:
Decision Tree ph√¢n t√≠ch game context ƒë·ªÉ ch·ªçn strategy ph√π h·ª£p, d·ª±a tr√™n:
- Game phase (Early/Mid/End)
- Position advantage
- Resource advantage
- Mobility situation

5.2 FEATURE EXTRACTION:

```csharp
public struct DecisionFeatures
{
    public int aiDistanceToGoal;      // AI distance to goal line
    public int humanDistanceToGoal;   // Human distance to goal line
    public int aiWallsLeft;           // AI walls remaining
    public int humanWallsLeft;        // Human walls remaining
    public int aiMobility;            // AI possible moves
    public int humanMobility;         // Human possible moves
    public float distanceAdvantage;   // Human - AI distance
    public float wallAdvantage;       // AI - Human walls
    public bool isEarlyGame;          // > 15 moves estimated remaining
    public bool isMidGame;            // 5-15 moves remaining
    public bool isEndGame;            // < 5 moves remaining
}
```

5.3 DECISION LOGIC:

```csharp
public AIStrategy Predict(DecisionFeatures features)
{
    // Early Game: Play conservatively
    if (features.isEarlyGame)
        return AIStrategy.Balanced;
    
    // AI has significant lead: Focus on blocking
    if (features.distanceAdvantage > 2)
        return AIStrategy.Blocking;
    
    // AI is behind: Be aggressive
    if (features.distanceAdvantage < -1)
        return AIStrategy.Aggressive;
    
    // End game with wall advantage: Play defensively
    if (features.isEndGame && features.wallAdvantage > 2)
        return AIStrategy.Defensive;
    
    // Close game: Balanced approach
    return AIStrategy.Balanced;
}
```

5.4 STRATEGY OUTCOMES:
- Balanced: Equal weight to movement v√† walls
- Blocking: Prioritize moves that increase opponent path
- Aggressive: Focus on attacking positions
- Defensive: Protect own position and path

================================================================================
                        6. ENTROPY ANALYSIS & SOFTMAX
================================================================================

6.1 ENTROPY CONCEPT:
Entropy ƒëo ƒë·ªô ƒëa d·∫°ng/uncertainty c·ªßa move scores. High entropy = nhi·ªÅu moves
c√≥ scores t∆∞∆°ng ƒë∆∞∆°ng, low entropy = m·ªôt move r√µ r√†ng t·ªët nh·∫•t.

6.2 ENTROPY CALCULATION:

```csharp
float CalculateEntropy(List<float> moveScores)
{
    // Normalize th√†nh probabilities
    float total = moveScores.Sum();
    var probabilities = moveScores.Select(s => s / total);
    
    // Calculate Shannon entropy
    float entropy = 0f;
    foreach (float p in probabilities)
    {
        if (p > 0f)
            entropy -= p * Mathf.Log(p, 2f);
    }
    
    return entropy; // Range: 0 (deterministic) to log2(n) (uniform)
}
```

6.3 SOFTMAX DISTRIBUTION:

```csharp
float[] CalculateSoftmax(List<float> scores, float temperature)
{
    float[] probabilities = new float[scores.Count];
    float sum = scores.Sum(s => Mathf.Exp(s / temperature));
    
    for (int i = 0; i < scores.Count; i++)
    {
        probabilities[i] = Mathf.Exp(scores[i] / temperature) / sum;
    }
    
    return probabilities;
}
```

6.4 TEMPERATURE PARAMETER:
- Low temperature (0.1): Sharp distribution, focus on best moves
- High temperature (2.0): Flat distribution, more exploration
- Default temperature (1.0): Balanced exploitation/exploration

6.5 ADAPTIVE MOVE SELECTION:

```csharp
public AIMove ChooseActionWithEntropy(GameState state, List<AIMove> possibleMoves)
{
    var moveScores = EvaluateAllMoves(state, possibleMoves);
    float entropy = CalculateEntropy(moveScores);
    
    if (entropy > entropyThreshold) // High diversity
    {
        // Use softmax for probabilistic selection
        var probabilities = CalculateSoftmax(moveScores, softmaxTemperature);
        return SelectMoveByProbability(possibleMoves, probabilities);
    }
    else // Clear best move
    {
        // Use deterministic best move
        return GetBestScoredMove(possibleMoves, moveScores);
    }
}
```

================================================================================
                        7. TRAINING SYSTEM & SELF-PLAY
================================================================================

7.1 SELF-PLAY CONCEPT:
AI h·ªçc b·∫±ng c√°ch ch∆°i v·ªõi ch√≠nh n√≥ h√†ng ng√†n l·∫ßn, gradually improve performance
th√¥ng qua trial v√† error.

7.2 TRAINING PIPELINE:

```csharp
IEnumerator RunSelfPlayTraining()
{
    for (int episode = 1; episode <= maxEpisodes; episode++)
    {
        // Reset game state
        ResetGameForTraining();
        
        while (!IsGameOver())
        {
            // Get current state
            GameState currentState = GetCurrentGameState();
            List<AIMove> possibleMoves = GetPossibleMoves();
            
            // Choose action using Œµ-greedy
            AIMove chosenMove = qLearningAgent.ChooseAction(currentState, possibleMoves);
            
            // Execute move
            ExecuteMove(chosenMove);
            
            // Calculate reward
            float reward = CalculateReward(currentState, chosenMove);
            
            // Update Q-table
            GameState newState = GetCurrentGameState();
            qLearningAgent.UpdateQValue(
                EncodeState(currentState),
                EncodeAction(chosenMove),
                reward,
                EncodeState(newState)
            );
            
            yield return new WaitForEndOfFrame();
        }
        
        // Epsilon decay
        qLearningAgent.DecayEpsilon();
        
        // Save progress periodically
        if (episode % 100 == 0)
        {
            qLearningAgent.SaveQTable();
        }
    }
}
```

7.3 REWARD ENGINEERING:
Critical cho Q-Learning success:

```csharp
float CalculateReward(GameState state, AIMove move)
{
    float reward = 0f;
    
    // Game outcome rewards
    if (IsWin(state)) reward += 1000f;
    if (IsLoss(state)) reward -= 1000f;
    
    // Progress rewards
    if (move.moveType == MoveType.Movement)
    {
        if (IsMoveTowardGoal(move)) reward += 20f;
        else reward -= 5f;
    }
    
    // Strategic rewards
    if (move.moveType == MoveType.WallPlacement)
    {
        if (IsWallBlockingOpponent(move)) reward += 50f;
        if (IsWallStrategic(move)) reward += 30f;
        if (IsWallWasted(move)) reward -= 25f;
    }
    
    // Style-specific rewards
    reward += GetStyleSpecificReward(state, move);
    
    return reward;
}
```

7.4 TRAINING METRICS:
- Episodes: 1000 standard
- Duration: ~15 minutes
- Convergence: Usually around episode 800
- Q-table size: 13,118+ states at completion
- Final epsilon: 0.1 (90% exploitation)

================================================================================
                        8. AI TESTING FRAMEWORK
================================================================================

8.1 TESTING PHILOSOPHY:
Comprehensive testing ensures AI quality v√† reliability across different
game scenarios v√† edge cases.

8.2 TEST CATEGORIES:

A) DECISION TREE TESTS:
```csharp
void TestDecisionTreeStrategies()
{
    // Test Early Game
    var earlyGameFeatures = new DecisionFeatures
    {
        aiDistanceToGoal = 8, humanDistanceToGoal = 8,
        isEarlyGame = true
    };
    var strategy = decisionTree.Predict(earlyGameFeatures);
    Assert.AreEqual(AIStrategy.Balanced, strategy);
    
    // Test AI Leading
    var leadingFeatures = new DecisionFeatures
    {
        distanceAdvantage = 3, // AI leading
        isMidGame = true
    };
    strategy = decisionTree.Predict(leadingFeatures);
    Assert.AreEqual(AIStrategy.Blocking, strategy);
}
```

B) ENTROPY ANALYSIS TESTS:
```csharp
void TestEntropyAnalysis()
{
    // High entropy case (diverse scores)
    var highEntropyScores = new List<float> { 100f, 98f, 97f, 95f, 94f };
    float entropy = CalculateTestEntropy(highEntropyScores);
    Assert.IsTrue(entropy > 0.5f);
    
    // Low entropy case (clear winner)
    var lowEntropyScores = new List<float> { 100f, 50f, 50f, 50f, 50f };
    entropy = CalculateTestEntropy(lowEntropyScores);
    Assert.IsTrue(entropy < 0.5f);
}
```

C) SOFTMAX DISTRIBUTION TESTS:
```csharp
void TestSoftmaxDistribution()
{
    var testScores = new List<float> { 100f, 98f, 95f, 90f };
    float temperature = 1.0f;
    
    var probabilities = CalculateSoftmax(testScores, temperature);
    
    // Verify probabilities sum to 1
    float sum = probabilities.Sum();
    Assert.AreEqual(1.0f, sum, 0.001f);
    
    // Verify highest score gets highest probability
    int maxIndex = Array.IndexOf(probabilities, probabilities.Max());
    Assert.AreEqual(0, maxIndex);
}
```

D) PERFORMANCE BENCHMARKS:
- Move generation time: <100ms
- Q-Learning decision: <50ms
- Minimax decision (depth 4): <200ms
- Memory usage: <256MB

8.3 AUTOMATED TESTING:
Tests run automatically ƒë·ªÉ ensure code quality:
- Unit tests cho individual algorithms
- Integration tests cho AI system interactions
- Performance tests cho response times
- Regression tests ƒë·ªÉ prevent quality degradation

================================================================================
                        9. HYBRID AI ARCHITECTURE
================================================================================

9.1 CONCEPT:
Hybrid system k·∫øt h·ª£p strengths c·ªßa multiple algorithms:
- Q-Learning: Data-driven, learns from experience
- Minimax: Logical, guaranteed optimal given evaluation
- Decision Tree: Fast, context-aware switching
- Entropy: Adaptive, handles uncertainty

9.2 ALGORITHM SELECTION LOGIC:

```csharp
public AIMove GetBestMove()
{
    UpdateGameState();
    List<AIMove> possibleMoves = GeneratePossibleMoves();
    
    // Primary: Use Q-Learning if trained model available
    if (useQLearning && qAgent.IsTrainedModel())
    {
        Debug.Log($"üß† Using {specializedStyle} Q-Learning");
        return qAgent.ChooseAction(currentGameState, possibleMoves);
    }
    
    // Fallback: Use Minimax for guaranteed reasonable play
    else
    {
        Debug.Log($"üîç Using {specializedStyle} Minimax");
        return FindBestMoveWithMinimax();
    }
}
```

9.3 SEAMLESS SWITCHING:
- No interruption in gameplay
- Maintains consistent API
- Preserves game state integrity
- Logs algorithm used for debugging

9.4 BENEFITS:
- Robustness: Fallback ensures AI never fails
- Performance: Q-Learning for trained scenarios, Minimax for edge cases
- Flexibility: Can switch algorithms based on situation
- Development: Easy to test individual components

================================================================================
                        10. PERFORMANCE OPTIMIZATION
================================================================================

10.1 Q-TABLE OPTIMIZATION:
- Sparse representation: Only store visited states
- Compression: String interning for state keys
- Lazy loading: Load Q-table on demand
- Periodic saving: Every 100 training episodes

10.2 MINIMAX OPTIMIZATION:
- Alpha-Beta pruning: 50% search reduction
- Move ordering: Promising moves first
- Iterative deepening: Gradual depth increase
- Transposition tables: Cache evaluated positions

10.3 MEMORY MANAGEMENT:
- Object pooling cho GameState objects
- String caching cho repeated state/action encodings
- Garbage collection optimization
- Memory profiling v√† monitoring

10.4 MULTITHREADING CONSIDERATIONS:
- Q-Learning training: Background thread
- Move calculation: Main thread (Unity requirement)
- File I/O: Async operations where possible
- Thread-safe collections cho shared data

10.5 CACHING STRATEGIES:
- Path finding results
- Move generation results
- Evaluation function results
- Feature extraction results

================================================================================
                            K·∫æT LU·∫¨N
================================================================================

D·ª± √°n AI Quoridor Project th·ªÉ hi·ªán m·ªôt implementation comprehensive c·ªßa modern
game AI techniques:

TECHNICAL INNOVATIONS:
‚úÖ Hybrid Q-Learning + Minimax architecture
‚úÖ 6 specialized AI personalities v·ªõi separate training
‚úÖ Advanced decision making v·ªõi entropy analysis
‚úÖ Comprehensive testing v√† validation framework
‚úÖ Professional development tools v√† debugging

EDUCATIONAL VALUE:
‚úÖ Practical machine learning implementation
‚úÖ Traditional game AI algorithms
‚úÖ Modern software engineering practices
‚úÖ Performance optimization techniques
‚úÖ Automated testing methodologies

FUTURE ENHANCEMENTS:
üöÄ Deep Q-Learning (DQN) integration
üöÄ Multi-agent reinforcement learning
üöÄ Neural network-based evaluation functions
üöÄ Online learning v√† adaptation
üöÄ Transfer learning between game variants

D·ª± √°n n√†y demonstrating successful integration c·ªßa classical AI methods v·ªõi
modern machine learning, t·∫°o ra m·ªôt intelligent, adaptive, v√† highly
interactive gaming experience.

================================================================================
                            APPENDIX
================================================================================

FILES LI√äN QUAN:
- Assets/scripts/QLearningAgent.cs - Q-Learning implementation
- Assets/scripts/QuoridorAI.cs - Main AI controller
- Assets/scripts/DecisionTreeClassifier.cs - Decision tree logic
- Assets/scripts/QuoridorSelfPlayTrainer.cs - Training system
- Assets/scripts/AITestScript.cs - Testing framework
- Assets/scripts/Editor/ForceRetrainAI.cs - Development tools

Q-TABLE FILES:
- StreamingAssets/qtable.json - Balanced style
- StreamingAssets/qtable_aggressive.json - Aggressive style
- StreamingAssets/qtable_defensive.json - Defensive style
- StreamingAssets/qtable_wallmaster.json - Wall Master style
- StreamingAssets/qtable_speedrunner.json - Speed Runner style
- StreamingAssets/qtable_tactical.json - Tactical style

DOCUMENTATION:
- AI_ALGORITHMS_DETAILED_DESCRIPTION.txt - This file
- README.md - Project overview
- API_INTEGRATION_GUIDE.md - Voice/Chat integration
- TRAINING_GUIDE.md - AI training instructions

================================================================================
================================================================================
                    AI QUORIDOR GAME PROJECT - THU·∫¨T TO√ÅN AI CHI TI·∫æT
================================================================================

D·ª± √°n: AI Quoridor Game v·ªõi Machine Learning v√† Multi-Platform Integration
Ng√†y t·∫°o: 25/07/2025
T√°c gi·∫£: [Student Name]
Platform: Unity 3D
Language: C#

================================================================================
                            M·ª§C L·ª§C
================================================================================

1. T·ªîNG QUAN KI·∫æN TR√öC AI
2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
4. SPECIALIZED AI STYLES SYSTEM
5. DECISION TREE CLASSIFIER
6. ENTROPY ANALYSIS & SOFTMAX
7. TRAINING SYSTEM & SELF-PLAY
8. AI TESTING FRAMEWORK
9. HYBRID AI ARCHITECTURE
10. PERFORMANCE OPTIMIZATION

================================================================================
                        1. T·ªîNG QUAN KI·∫æN TR√öC AI
================================================================================

D·ª± √°n AI Quoridor s·ª≠ d·ª•ng m·ªôt ki·∫øn tr√∫c AI hybrid ti√™n ti·∫øn, k·∫øt h·ª£p:

1.1 CORE ALGORITHMS:
- Q-Learning (Reinforcement Learning) - Thu·∫≠t to√°n ch√≠nh
- Minimax + Alpha-Beta Pruning - Thu·∫≠t to√°n fallback
- Decision Tree Classifier - Ph√¢n t√≠ch ng·ªØ c·∫£nh game
- Entropy Analysis - ƒê√°nh gi√° ƒëa d·∫°ng move

1.2 SPECIALIZED SYSTEMS:
- 6 AI Personality Styles v·ªõi training ri√™ng bi·ªát
- Self-Play Training Pipeline t·ª± ƒë·ªông
- Real-time Performance Testing Framework
- Dynamic Algorithm Switching

1.3 TECHNICAL FOUNDATION:
- State Space: 13,118+ unique game states
- Action Space: Movement + Wall placement combinations
- Training Method: Self-play v·ªõi 1000 episodes
- Learning Rate: Œ± = 0.1, Discount Factor: Œ≥ = 0.99

================================================================================
                    2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
================================================================================

2.1 GI·ªöI THI·ªÜU Q-LEARNING:
Q-Learning l√† thu·∫≠t to√°n Reinforcement Learning kh√¥ng c·∫ßn model, h·ªçc th√¥ng qua
t∆∞∆°ng t√°c v·ªõi m√¥i tr∆∞·ªùng ƒë·ªÉ t·ªëi ∆∞u h√≥a policy. Trong Quoridor, AI h·ªçc c√°ch
ch·ªçn moves t·ªëi ∆∞u d·ª±a tr√™n kinh nghi·ªám t·ª´ h√†ng ng√†n game.

2.2 C√îNG TH·ª®C TO√ÅN H·ªåC:
Q(s,a) = Q(s,a) + Œ±[r + Œ≥*max(Q(s',a')) - Q(s,a)]

Trong ƒë√≥:
- Q(s,a): Q-value c·ªßa state s v·ªõi action a
- Œ±: Learning rate (0.1) - t·ªëc ƒë·ªô h·ªçc
- r: Immediate reward t·ª´ action
- Œ≥: Discount factor (0.99) - t·∫ßm quan tr·ªçng future rewards
- s': Next state sau khi th·ª±c hi·ªán action
- max(Q(s',a')): Maximum Q-value c·ªßa next state

2.3 IMPLEMENTATION CHI TI·∫æT:

```csharp
public class QLearningAgent
{
    // Core Q-Table: Dictionary l∆∞u tr·ªØ Q-values
    private Dictionary<string, Dictionary<string, float>> qTable;
    
    // Hyperparameters
    private float learningRate = 0.1f;      // T·ªëc ƒë·ªô h·ªçc
    private float discountFactor = 0.99f;   // Discount future rewards
    private float epsilon = 0.995f;         // Exploration rate
    
    // Q-Learning Update Implementation
    public void UpdateQValue(string state, string action, float reward, string nextState)
    {
        // L·∫•y Q-value hi·ªán t·∫°i
        float currentQ = GetQValue(state, action);
        
        // T√¨m max Q-value c·ªßa next state
        float maxNextQ = GetMaxQValue(nextState);
        
        // √Åp d·ª•ng c√¥ng th·ª©c Q-Learning
        float newQ = currentQ + learningRate * (reward + discountFactor * maxNextQ - currentQ);
        
        // C·∫≠p nh·∫≠t Q-table
        SetQValue(state, action, newQ);
    }
}
```

2.4 STATE ENCODING:
Game state ƒë∆∞·ª£c encode th√†nh string duy nh·∫•t:
"aiPos_x,aiPos_y-humanPos_x,humanPos_y-aiWalls-humanWalls-wallsList"

V√≠ d·ª•: "4,8-4,0-9-10-7,4,False|5,6,True"
- AI ·ªü (4,8), Human ·ªü (4,0)
- AI c√≥ 9 walls, Human c√≥ 10 walls
- Walls ƒë√£ ƒë·∫∑t: (7,4,False), (5,6,True)

2.5 ACTION ENCODING:
Actions ƒë∆∞·ª£c encode th√†nh string:
- Movement: "M:x,y" (v√≠ d·ª•: "M:4,7")
- Wall: "W:x,y,horizontal" (v√≠ d·ª•: "W:3,5,True")

2.6 EPSILON-GREEDY STRATEGY:
```csharp
public AIMove ChooseAction(GameState state, List<AIMove> possibleMoves)
{
    // Exploration vs Exploitation trade-off
    if (Random.Range(0f, 1f) < epsilon)
    {
        // EXPLORATION: Ch·ªçn random ƒë·ªÉ kh√°m ph√°
        return possibleMoves[Random.Range(0, possibleMoves.Count)];
    }
    else
    {
        // EXPLOITATION: Ch·ªçn action c√≥ Q-value cao nh·∫•t
        return GetBestAction(state, possibleMoves);
    }
}
```

2.7 EPSILON DECAY:
Epsilon gi·∫£m d·∫ßn t·ª´ 0.995 ‚Üí 0.1 qua training:
- Ban ƒë·∫ßu: High exploration (99.5%)
- Cu·ªëi training: High exploitation (90%)

2.8 REWARD SYSTEM:
- Win game: +1000
- Lose game: -1000
- Move toward goal: +10
- Block opponent: +50
- Waste move: -5
- Strategic position: +15

================================================================================
                3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
================================================================================

3.1 GI·ªöI THI·ªÜU MINIMAX:
Minimax l√† thu·∫≠t to√°n game theory c·ªï ƒëi·ªÉn, t√¨m ki·∫øm move t·ªëi ∆∞u b·∫±ng c√°ch
d·ª± ƒëo√°n tr∆∞·ªõc nhi·ªÅu moves. Alpha-Beta pruning t·ªëi ∆∞u hi·ªáu su·∫•t b·∫±ng c√°ch
lo·∫°i b·ªè c√°c nh√°nh kh√¥ng c·∫ßn thi·∫øt.

3.2 NGUY√äN L√ù HO·∫†T ƒê·ªòNG:
- Maximizing Player (AI): T·ªëi ƒëa h√≥a score
- Minimizing Player (Human): T·ªëi thi·ªÉu h√≥a score AI
- Search depth: 4 levels (configurable)
- Pruning: Lo·∫°i b·ªè nh√°nh kh√¥ng promising

3.3 IMPLEMENTATION:

```csharp
float Minimax(GameState state, int depth, bool isMaximizing, float alpha, float beta)
{
    // Base case: Depth = 0 ho·∫∑c game over
    if (depth == 0 || IsGameOver(state))
        return EvaluatePosition(state);
    
    if (isMaximizing) // AI turn
    {
        float maxEval = float.NegativeInfinity;
        foreach (var move in GeneratePossibleMoves(state, true))
        {
            GameState newState = ApplyMove(state, move, true);
            float eval = Minimax(newState, depth - 1, false, alpha, beta);
            maxEval = Mathf.Max(maxEval, eval);
            alpha = Mathf.Max(alpha, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return maxEval;
    }
    else // Human turn
    {
        float minEval = float.PositiveInfinity;
        foreach (var move in GeneratePossibleMoves(state, false))
        {
            GameState newState = ApplyMove(state, move, false);
            float eval = Minimax(newState, depth - 1, true, alpha, beta);
            minEval = Mathf.Min(minEval, eval);
            beta = Mathf.Min(beta, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return minEval;
    }
}
```

3.4 EVALUATION FUNCTION:
Position evaluation d·ª±a tr√™n multiple factors:

```csharp
float EvaluatePosition(GameState state)
{
    float score = 0f;
    
    // Distance to goal (c√†ng g·∫ßn c√†ng t·ªët)
    int aiDistance = GetShortestPath(state.aiPosition, goalRow);
    int humanDistance = GetShortestPath(state.humanPosition, goalRow);
    score += (humanDistance - aiDistance) * 100;
    
    // Wall advantage
    score += (state.aiWallsLeft - state.humanWallsLeft) * 50;
    
    // Mobility (s·ªë moves c√≥ th·ªÉ)
    int aiMobility = GetValidMoves(state.aiPosition).Count;
    int humanMobility = GetValidMoves(state.humanPosition).Count;
    score += (aiMobility - humanMobility) * 20;
    
    // Center control
    score += GetCenterControlScore(state) * 30;
    
    return score;
}
```

3.5 ALPHA-BETA PRUNING HI·ªÜU QU·∫¢:
- Reduces search space t·ª´ O(b^d) xu·ªëng O(b^(d/2))
- Best case: 50% reduction in nodes evaluated
- Ordering moves by promising gi√∫p pruning hi·ªáu qu·∫£ h∆°n

================================================================================
                        4. SPECIALIZED AI STYLES SYSTEM
================================================================================

4.1 CONCEPT:
6 AI personalities v·ªõi training v√† behavior ri√™ng bi·ªát, m·ªói style c√≥:
- Ri√™ng Q-table file
- Custom reward structure
- Specific move prioritization
- Unique decision making logic

4.2 AI STYLES CHI TI·∫æT:

üî• AGGRESSIVE STYLE:
- M·ª•c ti√™u: T·∫•n c√¥ng v√† ch·∫∑n ƒë·ªëi th·ªß
- Wall Usage: 70% chance
- Reward Structure:
  * blockingOpponentReward = 50 * intensity
  * wallPlacementReward = 30 * intensity
  * defensiveWallPenalty = -20 * intensity
- Q-table: qtable_aggressive.json

üõ°Ô∏è DEFENSIVE STYLE:
- M·ª•c ti√™u: Ph√≤ng th·ªß v√† di chuy·ªÉn an to√†n
- Wall Usage: 50% chance
- Reward Structure:
  * defensiveWallReward = 35 * intensity
  * movementReward = 20 * intensity
  * aggressiveMovePenalty = -15 * intensity
- Q-table: qtable_defensive.json

üß± WALL MASTER STYLE:
- M·ª•c ti√™u: Chuy√™n gia ƒë·∫∑t t∆∞·ªùng v√† t·∫°o m√™ cung
- Wall Usage: 90% chance
- Reward Structure:
  * wallPlacementReward = 60 * intensity
  * strategicWallReward = 40 * intensity
  * wallComboReward = 50 * intensity
  * movementWithoutWallPenalty = -25 * intensity
- Q-table: qtable_wallmaster.json

‚ö° SPEED RUNNER STYLE:
- M·ª•c ti√™u: ƒê·∫°t ƒë√≠ch nhanh nh·∫•t
- Wall Usage: 10% chance
- Reward Structure:
  * movementReward = 40 * intensity
  * progressToGoalReward = 50 * intensity
  * quickWinBonus = 100 * intensity
  * wallPlacementPenalty = -20 * intensity
- Q-table: qtable_speedrunner.json

üß† TACTICAL STYLE:
- M·ª•c ti√™u: L·ª£i th·∫ø v·ªã tr√≠ v√† k·∫ø ho·∫°ch d√†i h·∫°n
- Wall Usage: Variable (strategic)
- Reward Structure:
  * tacticalMoveReward = 45 * intensity
  * positionAdvantageReward = 35 * intensity
  * mobilityReward = 25 * intensity
  * patternRecognitionBonus = 40 * intensity
- Q-table: qtable_tactical.json

‚öñÔ∏è BALANCED STYLE:
- M·ª•c ti√™u: C√¢n b·∫±ng t·∫•t c·∫£ aspects
- Wall Usage: 30% chance
- Reward Structure: Default balanced values
- Q-table: qtable.json

4.3 STYLE SWITCHING MECHANISM:

```csharp
public void SwitchToStyle(PlayStyle newStyle)
{
    // Save current Q-table
    if (allowQTableSaving) qAgent.SaveQTable(GetStyleSpecificQTablePath());
    
    // Load new style Q-table
    specializedStyle = newStyle;
    qAgent.LoadQTable(GetStyleSpecificQTablePath());
    
    // Reconfigure parameters
    ConfigureStyleSpecificTraining();
}
```

4.4 STYLE INTENSITY:
Parameter t·ª´ 0.0 ‚Üí 1.0 control m·ª©c ƒë·ªô chuy√™n h√≥a:
- 0.0: Balanced behavior
- 0.5: Moderate specialization
- 1.0: Extreme specialization

================================================================================
                        5. DECISION TREE CLASSIFIER
================================================================================

5.1 PURPOSE:
Decision Tree ph√¢n t√≠ch game context ƒë·ªÉ ch·ªçn strategy ph√π h·ª£p, d·ª±a tr√™n:
- Game phase (Early/Mid/End)
- Position advantage
- Resource advantage
- Mobility situation

5.2 FEATURE EXTRACTION:

```csharp
public struct DecisionFeatures
{
    public int aiDistanceToGoal;      // AI distance to goal line
    public int humanDistanceToGoal;   // Human distance to goal line
    public int aiWallsLeft;           // AI walls remaining
    public int humanWallsLeft;        // Human walls remaining
    public int aiMobility;            // AI possible moves
    public int humanMobility;         // Human possible moves
    public float distanceAdvantage;   // Human - AI distance
    public float wallAdvantage;       // AI - Human walls
    public bool isEarlyGame;          // > 15 moves estimated remaining
    public bool isMidGame;            // 5-15 moves remaining
    public bool isEndGame;            // < 5 moves remaining
}
```

5.3 DECISION LOGIC:

```csharp
public AIStrategy Predict(DecisionFeatures features)
{
    // Early Game: Play conservatively
    if (features.isEarlyGame)
        return AIStrategy.Balanced;
    
    // AI has significant lead: Focus on blocking
    if (features.distanceAdvantage > 2)
        return AIStrategy.Blocking;
    
    // AI is behind: Be aggressive
    if (features.distanceAdvantage < -1)
        return AIStrategy.Aggressive;
    
    // End game with wall advantage: Play defensively
    if (features.isEndGame && features.wallAdvantage > 2)
        return AIStrategy.Defensive;
    
    // Close game: Balanced approach
    return AIStrategy.Balanced;
}
```

5.4 STRATEGY OUTCOMES:
- Balanced: Equal weight to movement v√† walls
- Blocking: Prioritize moves that increase opponent path
- Aggressive: Focus on attacking positions
- Defensive: Protect own position and path

================================================================================
                        6. ENTROPY ANALYSIS & SOFTMAX
================================================================================

6.1 ENTROPY CONCEPT:
Entropy ƒëo ƒë·ªô ƒëa d·∫°ng/uncertainty c·ªßa move scores. High entropy = nhi·ªÅu moves
c√≥ scores t∆∞∆°ng ƒë∆∞∆°ng, low entropy = m·ªôt move r√µ r√†ng t·ªët nh·∫•t.

6.2 ENTROPY CALCULATION:

```csharp
float CalculateEntropy(List<float> moveScores)
{
    // Normalize th√†nh probabilities
    float total = moveScores.Sum();
    var probabilities = moveScores.Select(s => s / total);
    
    // Calculate Shannon entropy
    float entropy = 0f;
    foreach (float p in probabilities)
    {
        if (p > 0f)
            entropy -= p * Mathf.Log(p, 2f);
    }
    
    return entropy; // Range: 0 (deterministic) to log2(n) (uniform)
}
```

6.3 SOFTMAX DISTRIBUTION:

```csharp
float[] CalculateSoftmax(List<float> scores, float temperature)
{
    float[] probabilities = new float[scores.Count];
    float sum = scores.Sum(s => Mathf.Exp(s / temperature));
    
    for (int i = 0; i < scores.Count; i++)
    {
        probabilities[i] = Mathf.Exp(scores[i] / temperature) / sum;
    }
    
    return probabilities;
}
```

6.4 TEMPERATURE PARAMETER:
- Low temperature (0.1): Sharp distribution, focus on best moves
- High temperature (2.0): Flat distribution, more exploration
- Default temperature (1.0): Balanced exploitation/exploration

6.5 ADAPTIVE MOVE SELECTION:

```csharp
public AIMove ChooseActionWithEntropy(GameState state, List<AIMove> possibleMoves)
{
    var moveScores = EvaluateAllMoves(state, possibleMoves);
    float entropy = CalculateEntropy(moveScores);
    
    if (entropy > entropyThreshold) // High diversity
    {
        // Use softmax for probabilistic selection
        var probabilities = CalculateSoftmax(moveScores, softmaxTemperature);
        return SelectMoveByProbability(possibleMoves, probabilities);
    }
    else // Clear best move
    {
        // Use deterministic best move
        return GetBestScoredMove(possibleMoves, moveScores);
    }
}
```

================================================================================
                        7. TRAINING SYSTEM & SELF-PLAY
================================================================================

7.1 SELF-PLAY CONCEPT:
AI h·ªçc b·∫±ng c√°ch ch∆°i v·ªõi ch√≠nh n√≥ h√†ng ng√†n l·∫ßn, gradually improve performance
th√¥ng qua trial v√† error.

7.2 TRAINING PIPELINE:

```csharp
IEnumerator RunSelfPlayTraining()
{
    for (int episode = 1; episode <= maxEpisodes; episode++)
    {
        // Reset game state
        ResetGameForTraining();
        
        while (!IsGameOver())
        {
            // Get current state
            GameState currentState = GetCurrentGameState();
            List<AIMove> possibleMoves = GetPossibleMoves();
            
            // Choose action using Œµ-greedy
            AIMove chosenMove = qLearningAgent.ChooseAction(currentState, possibleMoves);
            
            // Execute move
            ExecuteMove(chosenMove);
            
            // Calculate reward
            float reward = CalculateReward(currentState, chosenMove);
            
            // Update Q-table
            GameState newState = GetCurrentGameState();
            qLearningAgent.UpdateQValue(
                EncodeState(currentState),
                EncodeAction(chosenMove),
                reward,
                EncodeState(newState)
            );
            
            yield return new WaitForEndOfFrame();
        }
        
        // Epsilon decay
        qLearningAgent.DecayEpsilon();
        
        // Save progress periodically
        if (episode % 100 == 0)
        {
            qLearningAgent.SaveQTable();
        }
    }
}
```

7.3 REWARD ENGINEERING:
Critical cho Q-Learning success:

```csharp
float CalculateReward(GameState state, AIMove move)
{
    float reward = 0f;
    
    // Game outcome rewards
    if (IsWin(state)) reward += 1000f;
    if (IsLoss(state)) reward -= 1000f;
    
    // Progress rewards
    if (move.moveType == MoveType.Movement)
    {
        if (IsMoveTowardGoal(move)) reward += 20f;
        else reward -= 5f;
    }
    
    // Strategic rewards
    if (move.moveType == MoveType.WallPlacement)
    {
        if (IsWallBlockingOpponent(move)) reward += 50f;
        if (IsWallStrategic(move)) reward += 30f;
        if (IsWallWasted(move)) reward -= 25f;
    }
    
    // Style-specific rewards
    reward += GetStyleSpecificReward(state, move);
    
    return reward;
}
```

7.4 TRAINING METRICS:
- Episodes: 1000 standard
- Duration: ~15 minutes
- Convergence: Usually around episode 800
- Q-table size: 13,118+ states at completion
- Final epsilon: 0.1 (90% exploitation)

================================================================================
                        8. AI TESTING FRAMEWORK
================================================================================

8.1 TESTING PHILOSOPHY:
Comprehensive testing ensures AI quality v√† reliability across different
game scenarios v√† edge cases.

8.2 TEST CATEGORIES:

A) DECISION TREE TESTS:
```csharp
void TestDecisionTreeStrategies()
{
    // Test Early Game
    var earlyGameFeatures = new DecisionFeatures
    {
        aiDistanceToGoal = 8, humanDistanceToGoal = 8,
        isEarlyGame = true
    };
    var strategy = decisionTree.Predict(earlyGameFeatures);
    Assert.AreEqual(AIStrategy.Balanced, strategy);
    
    // Test AI Leading
    var leadingFeatures = new DecisionFeatures
    {
        distanceAdvantage = 3, // AI leading
        isMidGame = true
    };
    strategy = decisionTree.Predict(leadingFeatures);
    Assert.AreEqual(AIStrategy.Blocking, strategy);
}
```

B) ENTROPY ANALYSIS TESTS:
```csharp
void TestEntropyAnalysis()
{
    // High entropy case (diverse scores)
    var highEntropyScores = new List<float> { 100f, 98f, 97f, 95f, 94f };
    float entropy = CalculateTestEntropy(highEntropyScores);
    Assert.IsTrue(entropy > 0.5f);
    
    // Low entropy case (clear winner)
    var lowEntropyScores = new List<float> { 100f, 50f, 50f, 50f, 50f };
    entropy = CalculateTestEntropy(lowEntropyScores);
    Assert.IsTrue(entropy < 0.5f);
}
```

C) SOFTMAX DISTRIBUTION TESTS:
```csharp
void TestSoftmaxDistribution()
{
    var testScores = new List<float> { 100f, 98f, 95f, 90f };
    float temperature = 1.0f;
    
    var probabilities = CalculateSoftmax(testScores, temperature);
    
    // Verify probabilities sum to 1
    float sum = probabilities.Sum();
    Assert.AreEqual(1.0f, sum, 0.001f);
    
    // Verify highest score gets highest probability
    int maxIndex = Array.IndexOf(probabilities, probabilities.Max());
    Assert.AreEqual(0, maxIndex);
}
```

D) PERFORMANCE BENCHMARKS:
- Move generation time: <100ms
- Q-Learning decision: <50ms
- Minimax decision (depth 4): <200ms
- Memory usage: <256MB

8.3 AUTOMATED TESTING:
Tests run automatically ƒë·ªÉ ensure code quality:
- Unit tests cho individual algorithms
- Integration tests cho AI system interactions
- Performance tests cho response times
- Regression tests ƒë·ªÉ prevent quality degradation

================================================================================
                        9. HYBRID AI ARCHITECTURE
================================================================================

9.1 CONCEPT:
Hybrid system k·∫øt h·ª£p strengths c·ªßa multiple algorithms:
- Q-Learning: Data-driven, learns from experience
- Minimax: Logical, guaranteed optimal given evaluation
- Decision Tree: Fast, context-aware switching
- Entropy: Adaptive, handles uncertainty

9.2 ALGORITHM SELECTION LOGIC:

```csharp
public AIMove GetBestMove()
{
    UpdateGameState();
    List<AIMove> possibleMoves = GeneratePossibleMoves();
    
    // Primary: Use Q-Learning if trained model available
    if (useQLearning && qAgent.IsTrainedModel())
    {
        Debug.Log($"üß† Using {specializedStyle} Q-Learning");
        return qAgent.ChooseAction(currentGameState, possibleMoves);
    }
    
    // Fallback: Use Minimax for guaranteed reasonable play
    else
    {
        Debug.Log($"üîç Using {specializedStyle} Minimax");
        return FindBestMoveWithMinimax();
    }
}
```

9.3 SEAMLESS SWITCHING:
- No interruption in gameplay
- Maintains consistent API
- Preserves game state integrity
- Logs algorithm used for debugging

9.4 BENEFITS:
- Robustness: Fallback ensures AI never fails
- Performance: Q-Learning for trained scenarios, Minimax for edge cases
- Flexibility: Can switch algorithms based on situation
- Development: Easy to test individual components

================================================================================
                        10. PERFORMANCE OPTIMIZATION
================================================================================

10.1 Q-TABLE OPTIMIZATION:
- Sparse representation: Only store visited states
- Compression: String interning for state keys
- Lazy loading: Load Q-table on demand
- Periodic saving: Every 100 training episodes

10.2 MINIMAX OPTIMIZATION:
- Alpha-Beta pruning: 50% search reduction
- Move ordering: Promising moves first
- Iterative deepening: Gradual depth increase
- Transposition tables: Cache evaluated positions

10.3 MEMORY MANAGEMENT:
- Object pooling cho GameState objects
- String caching cho repeated state/action encodings
- Garbage collection optimization
- Memory profiling v√† monitoring

10.4 MULTITHREADING CONSIDERATIONS:
- Q-Learning training: Background thread
- Move calculation: Main thread (Unity requirement)
- File I/O: Async operations where possible
- Thread-safe collections cho shared data

10.5 CACHING STRATEGIES:
- Path finding results
- Move generation results
- Evaluation function results
- Feature extraction results

================================================================================
                            K·∫æT LU·∫¨N
================================================================================

D·ª± √°n AI Quoridor Project th·ªÉ hi·ªán m·ªôt implementation comprehensive c·ªßa modern
game AI techniques:

TECHNICAL INNOVATIONS:
‚úÖ Hybrid Q-Learning + Minimax architecture
‚úÖ 6 specialized AI personalities v·ªõi separate training
‚úÖ Advanced decision making v·ªõi entropy analysis
‚úÖ Comprehensive testing v√† validation framework
‚úÖ Professional development tools v√† debugging

EDUCATIONAL VALUE:
‚úÖ Practical machine learning implementation
‚úÖ Traditional game AI algorithms
‚úÖ Modern software engineering practices
‚úÖ Performance optimization techniques
‚úÖ Automated testing methodologies

FUTURE ENHANCEMENTS:
üöÄ Deep Q-Learning (DQN) integration
üöÄ Multi-agent reinforcement learning
üöÄ Neural network-based evaluation functions
üöÄ Online learning v√† adaptation
üöÄ Transfer learning between game variants

D·ª± √°n n√†y demonstrating successful integration c·ªßa classical AI methods v·ªõi
modern machine learning, t·∫°o ra m·ªôt intelligent, adaptive, v√† highly
interactive gaming experience.

================================================================================
                            APPENDIX
================================================================================

FILES LI√äN QUAN:
- Assets/scripts/QLearningAgent.cs - Q-Learning implementation
- Assets/scripts/QuoridorAI.cs - Main AI controller
- Assets/scripts/DecisionTreeClassifier.cs - Decision tree logic
- Assets/scripts/QuoridorSelfPlayTrainer.cs - Training system
- Assets/scripts/AITestScript.cs - Testing framework
- Assets/scripts/Editor/ForceRetrainAI.cs - Development tools

Q-TABLE FILES:
- StreamingAssets/qtable.json - Balanced style
- StreamingAssets/qtable_aggressive.json - Aggressive style
- StreamingAssets/qtable_defensive.json - Defensive style
- StreamingAssets/qtable_wallmaster.json - Wall Master style
- StreamingAssets/qtable_speedrunner.json - Speed Runner style
- StreamingAssets/qtable_tactical.json - Tactical style

DOCUMENTATION:
- AI_ALGORITHMS_DETAILED_DESCRIPTION.txt - This file
- README.md - Project overview
- API_INTEGRATION_GUIDE.md - Voice/Chat integration
- TRAINING_GUIDE.md - AI training instructions

================================================================================
================================================================================
                    AI QUORIDOR GAME PROJECT - THU·∫¨T TO√ÅN AI CHI TI·∫æT
================================================================================

D·ª± √°n: AI Quoridor Game v·ªõi Machine Learning v√† Multi-Platform Integration
Ng√†y t·∫°o: 25/07/2025
T√°c gi·∫£: [Student Name]
Platform: Unity 3D
Language: C#

================================================================================
                            M·ª§C L·ª§C
================================================================================

1. T·ªîNG QUAN KI·∫æN TR√öC AI
2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
4. SPECIALIZED AI STYLES SYSTEM
5. DECISION TREE CLASSIFIER
6. ENTROPY ANALYSIS & SOFTMAX
7. TRAINING SYSTEM & SELF-PLAY
8. AI TESTING FRAMEWORK
9. HYBRID AI ARCHITECTURE
10. PERFORMANCE OPTIMIZATION

================================================================================
                        1. T·ªîNG QUAN KI·∫æN TR√öC AI
================================================================================

D·ª± √°n AI Quoridor s·ª≠ d·ª•ng m·ªôt ki·∫øn tr√∫c AI hybrid ti√™n ti·∫øn, k·∫øt h·ª£p:

1.1 CORE ALGORITHMS:
- Q-Learning (Reinforcement Learning) - Thu·∫≠t to√°n ch√≠nh
- Minimax + Alpha-Beta Pruning - Thu·∫≠t to√°n fallback
- Decision Tree Classifier - Ph√¢n t√≠ch ng·ªØ c·∫£nh game
- Entropy Analysis - ƒê√°nh gi√° ƒëa d·∫°ng move

1.2 SPECIALIZED SYSTEMS:
- 6 AI Personality Styles v·ªõi training ri√™ng bi·ªát
- Self-Play Training Pipeline t·ª± ƒë·ªông
- Real-time Performance Testing Framework
- Dynamic Algorithm Switching

1.3 TECHNICAL FOUNDATION:
- State Space: 13,118+ unique game states
- Action Space: Movement + Wall placement combinations
- Training Method: Self-play v·ªõi 1000 episodes
- Learning Rate: Œ± = 0.1, Discount Factor: Œ≥ = 0.99

================================================================================
                    2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
================================================================================

2.1 GI·ªöI THI·ªÜU Q-LEARNING:
Q-Learning l√† thu·∫≠t to√°n Reinforcement Learning kh√¥ng c·∫ßn model, h·ªçc th√¥ng qua
t∆∞∆°ng t√°c v·ªõi m√¥i tr∆∞·ªùng ƒë·ªÉ t·ªëi ∆∞u h√≥a policy. Trong Quoridor, AI h·ªçc c√°ch
ch·ªçn moves t·ªëi ∆∞u d·ª±a tr√™n kinh nghi·ªám t·ª´ h√†ng ng√†n game.

2.2 C√îNG TH·ª®C TO√ÅN H·ªåC:
Q(s,a) = Q(s,a) + Œ±[r + Œ≥*max(Q(s',a')) - Q(s,a)]

Trong ƒë√≥:
- Q(s,a): Q-value c·ªßa state s v·ªõi action a
- Œ±: Learning rate (0.1) - t·ªëc ƒë·ªô h·ªçc
- r: Immediate reward t·ª´ action
- Œ≥: Discount factor (0.99) - t·∫ßm quan tr·ªçng future rewards
- s': Next state sau khi th·ª±c hi·ªán action
- max(Q(s',a')): Maximum Q-value c·ªßa next state

2.3 IMPLEMENTATION CHI TI·∫æT:

```csharp
public class QLearningAgent
{
    // Core Q-Table: Dictionary l∆∞u tr·ªØ Q-values
    private Dictionary<string, Dictionary<string, float>> qTable;
    
    // Hyperparameters
    private float learningRate = 0.1f;      // T·ªëc ƒë·ªô h·ªçc
    private float discountFactor = 0.99f;   // Discount future rewards
    private float epsilon = 0.995f;         // Exploration rate
    
    // Q-Learning Update Implementation
    public void UpdateQValue(string state, string action, float reward, string nextState)
    {
        // L·∫•y Q-value hi·ªán t·∫°i
        float currentQ = GetQValue(state, action);
        
        // T√¨m max Q-value c·ªßa next state
        float maxNextQ = GetMaxQValue(nextState);
        
        // √Åp d·ª•ng c√¥ng th·ª©c Q-Learning
        float newQ = currentQ + learningRate * (reward + discountFactor * maxNextQ - currentQ);
        
        // C·∫≠p nh·∫≠t Q-table
        SetQValue(state, action, newQ);
    }
}
```

2.4 STATE ENCODING:
Game state ƒë∆∞·ª£c encode th√†nh string duy nh·∫•t:
"aiPos_x,aiPos_y-humanPos_x,humanPos_y-aiWalls-humanWalls-wallsList"

V√≠ d·ª•: "4,8-4,0-9-10-7,4,False|5,6,True"
- AI ·ªü (4,8), Human ·ªü (4,0)
- AI c√≥ 9 walls, Human c√≥ 10 walls
- Walls ƒë√£ ƒë·∫∑t: (7,4,False), (5,6,True)

2.5 ACTION ENCODING:
Actions ƒë∆∞·ª£c encode th√†nh string:
- Movement: "M:x,y" (v√≠ d·ª•: "M:4,7")
- Wall: "W:x,y,horizontal" (v√≠ d·ª•: "W:3,5,True")

2.6 EPSILON-GREEDY STRATEGY:
```csharp
public AIMove ChooseAction(GameState state, List<AIMove> possibleMoves)
{
    // Exploration vs Exploitation trade-off
    if (Random.Range(0f, 1f) < epsilon)
    {
        // EXPLORATION: Ch·ªçn random ƒë·ªÉ kh√°m ph√°
        return possibleMoves[Random.Range(0, possibleMoves.Count)];
    }
    else
    {
        // EXPLOITATION: Ch·ªçn action c√≥ Q-value cao nh·∫•t
        return GetBestAction(state, possibleMoves);
    }
}
```

2.7 EPSILON DECAY:
Epsilon gi·∫£m d·∫ßn t·ª´ 0.995 ‚Üí 0.1 qua training:
- Ban ƒë·∫ßu: High exploration (99.5%)
- Cu·ªëi training: High exploitation (90%)

2.8 REWARD SYSTEM:
- Win game: +1000
- Lose game: -1000
- Move toward goal: +10
- Block opponent: +50
- Waste move: -5
- Strategic position: +15

================================================================================
                3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
================================================================================

3.1 GI·ªöI THI·ªÜU MINIMAX:
Minimax l√† thu·∫≠t to√°n game theory c·ªï ƒëi·ªÉn, t√¨m ki·∫øm move t·ªëi ∆∞u b·∫±ng c√°ch
d·ª± ƒëo√°n tr∆∞·ªõc nhi·ªÅu moves. Alpha-Beta pruning t·ªëi ∆∞u hi·ªáu su·∫•t b·∫±ng c√°ch
lo·∫°i b·ªè c√°c nh√°nh kh√¥ng c·∫ßn thi·∫øt.

3.2 NGUY√äN L√ù HO·∫†T ƒê·ªòNG:
- Maximizing Player (AI): T·ªëi ƒëa h√≥a score
- Minimizing Player (Human): T·ªëi thi·ªÉu h√≥a score AI
- Search depth: 4 levels (configurable)
- Pruning: Lo·∫°i b·ªè nh√°nh kh√¥ng promising

3.3 IMPLEMENTATION:

```csharp
float Minimax(GameState state, int depth, bool isMaximizing, float alpha, float beta)
{
    // Base case: Depth = 0 ho·∫∑c game over
    if (depth == 0 || IsGameOver(state))
        return EvaluatePosition(state);
    
    if (isMaximizing) // AI turn
    {
        float maxEval = float.NegativeInfinity;
        foreach (var move in GeneratePossibleMoves(state, true))
        {
            GameState newState = ApplyMove(state, move, true);
            float eval = Minimax(newState, depth - 1, false, alpha, beta);
            maxEval = Mathf.Max(maxEval, eval);
            alpha = Mathf.Max(alpha, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return maxEval;
    }
    else // Human turn
    {
        float minEval = float.PositiveInfinity;
        foreach (var move in GeneratePossibleMoves(state, false))
        {
            GameState newState = ApplyMove(state, move, false);
            float eval = Minimax(newState, depth - 1, true, alpha, beta);
            minEval = Mathf.Min(minEval, eval);
            beta = Mathf.Min(beta, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return minEval;
    }
}
```

3.4 EVALUATION FUNCTION:
Position evaluation d·ª±a tr√™n multiple factors:

```csharp
float EvaluatePosition(GameState state)
{
    float score = 0f;
    
    // Distance to goal (c√†ng g·∫ßn c√†ng t·ªët)
    int aiDistance = GetShortestPath(state.aiPosition, goalRow);
    int humanDistance = GetShortestPath(state.humanPosition, goalRow);
    score += (humanDistance - aiDistance) * 100;
    
    // Wall advantage
    score += (state.aiWallsLeft - state.humanWallsLeft) * 50;
    
    // Mobility (s·ªë moves c√≥ th·ªÉ)
    int aiMobility = GetValidMoves(state.aiPosition).Count;
    int humanMobility = GetValidMoves(state.humanPosition).Count;
    score += (aiMobility - humanMobility) * 20;
    
    // Center control
    score += GetCenterControlScore(state) * 30;
    
    return score;
}
```

3.5 ALPHA-BETA PRUNING HI·ªÜU QU·∫¢:
- Reduces search space t·ª´ O(b^d) xu·ªëng O(b^(d/2))
- Best case: 50% reduction in nodes evaluated
- Ordering moves by promising gi√∫p pruning hi·ªáu qu·∫£ h∆°n

================================================================================
                        4. SPECIALIZED AI STYLES SYSTEM
================================================================================

4.1 CONCEPT:
6 AI personalities v·ªõi training v√† behavior ri√™ng bi·ªát, m·ªói style c√≥:
- Ri√™ng Q-table file
- Custom reward structure
- Specific move prioritization
- Unique decision making logic

4.2 AI STYLES CHI TI·∫æT:

üî• AGGRESSIVE STYLE:
- M·ª•c ti√™u: T·∫•n c√¥ng v√† ch·∫∑n ƒë·ªëi th·ªß
- Wall Usage: 70% chance
- Reward Structure:
  * blockingOpponentReward = 50 * intensity
  * wallPlacementReward = 30 * intensity
  * defensiveWallPenalty = -20 * intensity
- Q-table: qtable_aggressive.json

üõ°Ô∏è DEFENSIVE STYLE:
- M·ª•c ti√™u: Ph√≤ng th·ªß v√† di chuy·ªÉn an to√†n
- Wall Usage: 50% chance
- Reward Structure:
  * defensiveWallReward = 35 * intensity
  * movementReward = 20 * intensity
  * aggressiveMovePenalty = -15 * intensity
- Q-table: qtable_defensive.json

üß± WALL MASTER STYLE:
- M·ª•c ti√™u: Chuy√™n gia ƒë·∫∑t t∆∞·ªùng v√† t·∫°o m√™ cung
- Wall Usage: 90% chance
- Reward Structure:
  * wallPlacementReward = 60 * intensity
  * strategicWallReward = 40 * intensity
  * wallComboReward = 50 * intensity
  * movementWithoutWallPenalty = -25 * intensity
- Q-table: qtable_wallmaster.json

‚ö° SPEED RUNNER STYLE:
- M·ª•c ti√™u: ƒê·∫°t ƒë√≠ch nhanh nh·∫•t
- Wall Usage: 10% chance
- Reward Structure:
  * movementReward = 40 * intensity
  * progressToGoalReward = 50 * intensity
  * quickWinBonus = 100 * intensity
  * wallPlacementPenalty = -20 * intensity
- Q-table: qtable_speedrunner.json

üß† TACTICAL STYLE:
- M·ª•c ti√™u: L·ª£i th·∫ø v·ªã tr√≠ v√† k·∫ø ho·∫°ch d√†i h·∫°n
- Wall Usage: Variable (strategic)
- Reward Structure:
  * tacticalMoveReward = 45 * intensity
  * positionAdvantageReward = 35 * intensity
  * mobilityReward = 25 * intensity
  * patternRecognitionBonus = 40 * intensity
- Q-table: qtable_tactical.json

‚öñÔ∏è BALANCED STYLE:
- M·ª•c ti√™u: C√¢n b·∫±ng t·∫•t c·∫£ aspects
- Wall Usage: 30% chance
- Reward Structure: Default balanced values
- Q-table: qtable.json

4.3 STYLE SWITCHING MECHANISM:

```csharp
public void SwitchToStyle(PlayStyle newStyle)
{
    // Save current Q-table
    if (allowQTableSaving) qAgent.SaveQTable(GetStyleSpecificQTablePath());
    
    // Load new style Q-table
    specializedStyle = newStyle;
    qAgent.LoadQTable(GetStyleSpecificQTablePath());
    
    // Reconfigure parameters
    ConfigureStyleSpecificTraining();
}
```

4.4 STYLE INTENSITY:
Parameter t·ª´ 0.0 ‚Üí 1.0 control m·ª©c ƒë·ªô chuy√™n h√≥a:
- 0.0: Balanced behavior
- 0.5: Moderate specialization
- 1.0: Extreme specialization

================================================================================
                        5. DECISION TREE CLASSIFIER
================================================================================

5.1 PURPOSE:
Decision Tree ph√¢n t√≠ch game context ƒë·ªÉ ch·ªçn strategy ph√π h·ª£p, d·ª±a tr√™n:
- Game phase (Early/Mid/End)
- Position advantage
- Resource advantage
- Mobility situation

5.2 FEATURE EXTRACTION:

```csharp
public struct DecisionFeatures
{
    public int aiDistanceToGoal;      // AI distance to goal line
    public int humanDistanceToGoal;   // Human distance to goal line
    public int aiWallsLeft;           // AI walls remaining
    public int humanWallsLeft;        // Human walls remaining
    public int aiMobility;            // AI possible moves
    public int humanMobility;         // Human possible moves
    public float distanceAdvantage;   // Human - AI distance
    public float wallAdvantage;       // AI - Human walls
    public bool isEarlyGame;          // > 15 moves estimated remaining
    public bool isMidGame;            // 5-15 moves remaining
    public bool isEndGame;            // < 5 moves remaining
}
```

5.3 DECISION LOGIC:

```csharp
public AIStrategy Predict(DecisionFeatures features)
{
    // Early Game: Play conservatively
    if (features.isEarlyGame)
        return AIStrategy.Balanced;
    
    // AI has significant lead: Focus on blocking
    if (features.distanceAdvantage > 2)
        return AIStrategy.Blocking;
    
    // AI is behind: Be aggressive
    if (features.distanceAdvantage < -1)
        return AIStrategy.Aggressive;
    
    // End game with wall advantage: Play defensively
    if (features.isEndGame && features.wallAdvantage > 2)
        return AIStrategy.Defensive;
    
    // Close game: Balanced approach
    return AIStrategy.Balanced;
}
```

5.4 STRATEGY OUTCOMES:
- Balanced: Equal weight to movement v√† walls
- Blocking: Prioritize moves that increase opponent path
- Aggressive: Focus on attacking positions
- Defensive: Protect own position and path

================================================================================
                        6. ENTROPY ANALYSIS & SOFTMAX
================================================================================

6.1 ENTROPY CONCEPT:
Entropy ƒëo ƒë·ªô ƒëa d·∫°ng/uncertainty c·ªßa move scores. High entropy = nhi·ªÅu moves
c√≥ scores t∆∞∆°ng ƒë∆∞∆°ng, low entropy = m·ªôt move r√µ r√†ng t·ªët nh·∫•t.

6.2 ENTROPY CALCULATION:

```csharp
float CalculateEntropy(List<float> moveScores)
{
    // Normalize th√†nh probabilities
    float total = moveScores.Sum();
    var probabilities = moveScores.Select(s => s / total);
    
    // Calculate Shannon entropy
    float entropy = 0f;
    foreach (float p in probabilities)
    {
        if (p > 0f)
            entropy -= p * Mathf.Log(p, 2f);
    }
    
    return entropy; // Range: 0 (deterministic) to log2(n) (uniform)
}
```

6.3 SOFTMAX DISTRIBUTION:

```csharp
float[] CalculateSoftmax(List<float> scores, float temperature)
{
    float[] probabilities = new float[scores.Count];
    float sum = scores.Sum(s => Mathf.Exp(s / temperature));
    
    for (int i = 0; i < scores.Count; i++)
    {
        probabilities[i] = Mathf.Exp(scores[i] / temperature) / sum;
    }
    
    return probabilities;
}
```

6.4 TEMPERATURE PARAMETER:
- Low temperature (0.1): Sharp distribution, focus on best moves
- High temperature (2.0): Flat distribution, more exploration
- Default temperature (1.0): Balanced exploitation/exploration

6.5 ADAPTIVE MOVE SELECTION:

```csharp
public AIMove ChooseActionWithEntropy(GameState state, List<AIMove> possibleMoves)
{
    var moveScores = EvaluateAllMoves(state, possibleMoves);
    float entropy = CalculateEntropy(moveScores);
    
    if (entropy > entropyThreshold) // High diversity
    {
        // Use softmax for probabilistic selection
        var probabilities = CalculateSoftmax(moveScores, softmaxTemperature);
        return SelectMoveByProbability(possibleMoves, probabilities);
    }
    else // Clear best move
    {
        // Use deterministic best move
        return GetBestScoredMove(possibleMoves, moveScores);
    }
}
```

================================================================================
                        7. TRAINING SYSTEM & SELF-PLAY
================================================================================

7.1 SELF-PLAY CONCEPT:
AI h·ªçc b·∫±ng c√°ch ch∆°i v·ªõi ch√≠nh n√≥ h√†ng ng√†n l·∫ßn, gradually improve performance
th√¥ng qua trial v√† error.

7.2 TRAINING PIPELINE:

```csharp
IEnumerator RunSelfPlayTraining()
{
    for (int episode = 1; episode <= maxEpisodes; episode++)
    {
        // Reset game state
        ResetGameForTraining();
        
        while (!IsGameOver())
        {
            // Get current state
            GameState currentState = GetCurrentGameState();
            List<AIMove> possibleMoves = GetPossibleMoves();
            
            // Choose action using Œµ-greedy
            AIMove chosenMove = qLearningAgent.ChooseAction(currentState, possibleMoves);
            
            // Execute move
            ExecuteMove(chosenMove);
            
            // Calculate reward
            float reward = CalculateReward(currentState, chosenMove);
            
            // Update Q-table
            GameState newState = GetCurrentGameState();
            qLearningAgent.UpdateQValue(
                EncodeState(currentState),
                EncodeAction(chosenMove),
                reward,
                EncodeState(newState)
            );
            
            yield return new WaitForEndOfFrame();
        }
        
        // Epsilon decay
        qLearningAgent.DecayEpsilon();
        
        // Save progress periodically
        if (episode % 100 == 0)
        {
            qLearningAgent.SaveQTable();
        }
    }
}
```

7.3 REWARD ENGINEERING:
Critical cho Q-Learning success:

```csharp
float CalculateReward(GameState state, AIMove move)
{
    float reward = 0f;
    
    // Game outcome rewards
    if (IsWin(state)) reward += 1000f;
    if (IsLoss(state)) reward -= 1000f;
    
    // Progress rewards
    if (move.moveType == MoveType.Movement)
    {
        if (IsMoveTowardGoal(move)) reward += 20f;
        else reward -= 5f;
    }
    
    // Strategic rewards
    if (move.moveType == MoveType.WallPlacement)
    {
        if (IsWallBlockingOpponent(move)) reward += 50f;
        if (IsWallStrategic(move)) reward += 30f;
        if (IsWallWasted(move)) reward -= 25f;
    }
    
    // Style-specific rewards
    reward += GetStyleSpecificReward(state, move);
    
    return reward;
}
```

7.4 TRAINING METRICS:
- Episodes: 1000 standard
- Duration: ~15 minutes
- Convergence: Usually around episode 800
- Q-table size: 13,118+ states at completion
- Final epsilon: 0.1 (90% exploitation)

================================================================================
                        8. AI TESTING FRAMEWORK
================================================================================

8.1 TESTING PHILOSOPHY:
Comprehensive testing ensures AI quality v√† reliability across different
game scenarios v√† edge cases.

8.2 TEST CATEGORIES:

A) DECISION TREE TESTS:
```csharp
void TestDecisionTreeStrategies()
{
    // Test Early Game
    var earlyGameFeatures = new DecisionFeatures
    {
        aiDistanceToGoal = 8, humanDistanceToGoal = 8,
        isEarlyGame = true
    };
    var strategy = decisionTree.Predict(earlyGameFeatures);
    Assert.AreEqual(AIStrategy.Balanced, strategy);
    
    // Test AI Leading
    var leadingFeatures = new DecisionFeatures
    {
        distanceAdvantage = 3, // AI leading
        isMidGame = true
    };
    strategy = decisionTree.Predict(leadingFeatures);
    Assert.AreEqual(AIStrategy.Blocking, strategy);
}
```

B) ENTROPY ANALYSIS TESTS:
```csharp
void TestEntropyAnalysis()
{
    // High entropy case (diverse scores)
    var highEntropyScores = new List<float> { 100f, 98f, 97f, 95f, 94f };
    float entropy = CalculateTestEntropy(highEntropyScores);
    Assert.IsTrue(entropy > 0.5f);
    
    // Low entropy case (clear winner)
    var lowEntropyScores = new List<float> { 100f, 50f, 50f, 50f, 50f };
    entropy = CalculateTestEntropy(lowEntropyScores);
    Assert.IsTrue(entropy < 0.5f);
}
```

C) SOFTMAX DISTRIBUTION TESTS:
```csharp
void TestSoftmaxDistribution()
{
    var testScores = new List<float> { 100f, 98f, 95f, 90f };
    float temperature = 1.0f;
    
    var probabilities = CalculateSoftmax(testScores, temperature);
    
    // Verify probabilities sum to 1
    float sum = probabilities.Sum();
    Assert.AreEqual(1.0f, sum, 0.001f);
    
    // Verify highest score gets highest probability
    int maxIndex = Array.IndexOf(probabilities, probabilities.Max());
    Assert.AreEqual(0, maxIndex);
}
```

D) PERFORMANCE BENCHMARKS:
- Move generation time: <100ms
- Q-Learning decision: <50ms
- Minimax decision (depth 4): <200ms
- Memory usage: <256MB

8.3 AUTOMATED TESTING:
Tests run automatically ƒë·ªÉ ensure code quality:
- Unit tests cho individual algorithms
- Integration tests cho AI system interactions
- Performance tests cho response times
- Regression tests ƒë·ªÉ prevent quality degradation

================================================================================
                        9. HYBRID AI ARCHITECTURE
================================================================================

9.1 CONCEPT:
Hybrid system k·∫øt h·ª£p strengths c·ªßa multiple algorithms:
- Q-Learning: Data-driven, learns from experience
- Minimax: Logical, guaranteed optimal given evaluation
- Decision Tree: Fast, context-aware switching
- Entropy: Adaptive, handles uncertainty

9.2 ALGORITHM SELECTION LOGIC:

```csharp
public AIMove GetBestMove()
{
    UpdateGameState();
    List<AIMove> possibleMoves = GeneratePossibleMoves();
    
    // Primary: Use Q-Learning if trained model available
    if (useQLearning && qAgent.IsTrainedModel())
    {
        Debug.Log($"üß† Using {specializedStyle} Q-Learning");
        return qAgent.ChooseAction(currentGameState, possibleMoves);
    }
    
    // Fallback: Use Minimax for guaranteed reasonable play
    else
    {
        Debug.Log($"üîç Using {specializedStyle} Minimax");
        return FindBestMoveWithMinimax();
    }
}
```

9.3 SEAMLESS SWITCHING:
- No interruption in gameplay
- Maintains consistent API
- Preserves game state integrity
- Logs algorithm used for debugging

9.4 BENEFITS:
- Robustness: Fallback ensures AI never fails
- Performance: Q-Learning for trained scenarios, Minimax for edge cases
- Flexibility: Can switch algorithms based on situation
- Development: Easy to test individual components

================================================================================
                        10. PERFORMANCE OPTIMIZATION
================================================================================

10.1 Q-TABLE OPTIMIZATION:
- Sparse representation: Only store visited states
- Compression: String interning for state keys
- Lazy loading: Load Q-table on demand
- Periodic saving: Every 100 training episodes

10.2 MINIMAX OPTIMIZATION:
- Alpha-Beta pruning: 50% search reduction
- Move ordering: Promising moves first
- Iterative deepening: Gradual depth increase
- Transposition tables: Cache evaluated positions

10.3 MEMORY MANAGEMENT:
- Object pooling cho GameState objects
- String caching cho repeated state/action encodings
- Garbage collection optimization
- Memory profiling v√† monitoring

10.4 MULTITHREADING CONSIDERATIONS:
- Q-Learning training: Background thread
- Move calculation: Main thread (Unity requirement)
- File I/O: Async operations where possible
- Thread-safe collections cho shared data

10.5 CACHING STRATEGIES:
- Path finding results
- Move generation results
- Evaluation function results
- Feature extraction results

================================================================================
                            K·∫æT LU·∫¨N
================================================================================

D·ª± √°n AI Quoridor Project th·ªÉ hi·ªán m·ªôt implementation comprehensive c·ªßa modern
game AI techniques:

TECHNICAL INNOVATIONS:
‚úÖ Hybrid Q-Learning + Minimax architecture
‚úÖ 6 specialized AI personalities v·ªõi separate training
‚úÖ Advanced decision making v·ªõi entropy analysis
‚úÖ Comprehensive testing v√† validation framework
‚úÖ Professional development tools v√† debugging

EDUCATIONAL VALUE:
‚úÖ Practical machine learning implementation
‚úÖ Traditional game AI algorithms
‚úÖ Modern software engineering practices
‚úÖ Performance optimization techniques
‚úÖ Automated testing methodologies

FUTURE ENHANCEMENTS:
üöÄ Deep Q-Learning (DQN) integration
üöÄ Multi-agent reinforcement learning
üöÄ Neural network-based evaluation functions
üöÄ Online learning v√† adaptation
üöÄ Transfer learning between game variants

D·ª± √°n n√†y demonstrating successful integration c·ªßa classical AI methods v·ªõi
modern machine learning, t·∫°o ra m·ªôt intelligent, adaptive, v√† highly
interactive gaming experience.

================================================================================
                            APPENDIX
================================================================================

FILES LI√äN QUAN:
- Assets/scripts/QLearningAgent.cs - Q-Learning implementation
- Assets/scripts/QuoridorAI.cs - Main AI controller
- Assets/scripts/DecisionTreeClassifier.cs - Decision tree logic
- Assets/scripts/QuoridorSelfPlayTrainer.cs - Training system
- Assets/scripts/AITestScript.cs - Testing framework
- Assets/scripts/Editor/ForceRetrainAI.cs - Development tools

Q-TABLE FILES:
- StreamingAssets/qtable.json - Balanced style
- StreamingAssets/qtable_aggressive.json - Aggressive style
- StreamingAssets/qtable_defensive.json - Defensive style
- StreamingAssets/qtable_wallmaster.json - Wall Master style
- StreamingAssets/qtable_speedrunner.json - Speed Runner style
- StreamingAssets/qtable_tactical.json - Tactical style

DOCUMENTATION:
- AI_ALGORITHMS_DETAILED_DESCRIPTION.txt - This file
- README.md - Project overview
- API_INTEGRATION_GUIDE.md - Voice/Chat integration
- TRAINING_GUIDE.md - AI training instructions

================================================================================
================================================================================
                    AI QUORIDOR GAME PROJECT - THU·∫¨T TO√ÅN AI CHI TI·∫æT
================================================================================

D·ª± √°n: AI Quoridor Game v·ªõi Machine Learning v√† Multi-Platform Integration
Ng√†y t·∫°o: 25/07/2025
T√°c gi·∫£: [Student Name]
Platform: Unity 3D
Language: C#

================================================================================
                            M·ª§C L·ª§C
================================================================================

1. T·ªîNG QUAN KI·∫æN TR√öC AI
2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
4. SPECIALIZED AI STYLES SYSTEM
5. DECISION TREE CLASSIFIER
6. ENTROPY ANALYSIS & SOFTMAX
7. TRAINING SYSTEM & SELF-PLAY
8. AI TESTING FRAMEWORK
9. HYBRID AI ARCHITECTURE
10. PERFORMANCE OPTIMIZATION

================================================================================
                        1. T·ªîNG QUAN KI·∫æN TR√öC AI
================================================================================

D·ª± √°n AI Quoridor s·ª≠ d·ª•ng m·ªôt ki·∫øn tr√∫c AI hybrid ti√™n ti·∫øn, k·∫øt h·ª£p:

1.1 CORE ALGORITHMS:
- Q-Learning (Reinforcement Learning) - Thu·∫≠t to√°n ch√≠nh
- Minimax + Alpha-Beta Pruning - Thu·∫≠t to√°n fallback
- Decision Tree Classifier - Ph√¢n t√≠ch ng·ªØ c·∫£nh game
- Entropy Analysis - ƒê√°nh gi√° ƒëa d·∫°ng move

1.2 SPECIALIZED SYSTEMS:
- 6 AI Personality Styles v·ªõi training ri√™ng bi·ªát
- Self-Play Training Pipeline t·ª± ƒë·ªông
- Real-time Performance Testing Framework
- Dynamic Algorithm Switching

1.3 TECHNICAL FOUNDATION:
- State Space: 13,118+ unique game states
- Action Space: Movement + Wall placement combinations
- Training Method: Self-play v·ªõi 1000 episodes
- Learning Rate: Œ± = 0.1, Discount Factor: Œ≥ = 0.99

================================================================================
                    2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
================================================================================

2.1 GI·ªöI THI·ªÜU Q-LEARNING:
Q-Learning l√† thu·∫≠t to√°n Reinforcement Learning kh√¥ng c·∫ßn model, h·ªçc th√¥ng qua
t∆∞∆°ng t√°c v·ªõi m√¥i tr∆∞·ªùng ƒë·ªÉ t·ªëi ∆∞u h√≥a policy. Trong Quoridor, AI h·ªçc c√°ch
ch·ªçn moves t·ªëi ∆∞u d·ª±a tr√™n kinh nghi·ªám t·ª´ h√†ng ng√†n game.

2.2 C√îNG TH·ª®C TO√ÅN H·ªåC:
Q(s,a) = Q(s,a) + Œ±[r + Œ≥*max(Q(s',a')) - Q(s,a)]

Trong ƒë√≥:
- Q(s,a): Q-value c·ªßa state s v·ªõi action a
- Œ±: Learning rate (0.1) - t·ªëc ƒë·ªô h·ªçc
- r: Immediate reward t·ª´ action
- Œ≥: Discount factor (0.99) - t·∫ßm quan tr·ªçng future rewards
- s': Next state sau khi th·ª±c hi·ªán action
- max(Q(s',a')): Maximum Q-value c·ªßa next state

2.3 IMPLEMENTATION CHI TI·∫æT:

```csharp
public class QLearningAgent
{
    // Core Q-Table: Dictionary l∆∞u tr·ªØ Q-values
    private Dictionary<string, Dictionary<string, float>> qTable;
    
    // Hyperparameters
    private float learningRate = 0.1f;      // T·ªëc ƒë·ªô h·ªçc
    private float discountFactor = 0.99f;   // Discount future rewards
    private float epsilon = 0.995f;         // Exploration rate
    
    // Q-Learning Update Implementation
    public void UpdateQValue(string state, string action, float reward, string nextState)
    {
        // L·∫•y Q-value hi·ªán t·∫°i
        float currentQ = GetQValue(state, action);
        
        // T√¨m max Q-value c·ªßa next state
        float maxNextQ = GetMaxQValue(nextState);
        
        // √Åp d·ª•ng c√¥ng th·ª©c Q-Learning
        float newQ = currentQ + learningRate * (reward + discountFactor * maxNextQ - currentQ);
        
        // C·∫≠p nh·∫≠t Q-table
        SetQValue(state, action, newQ);
    }
}
```

2.4 STATE ENCODING:
Game state ƒë∆∞·ª£c encode th√†nh string duy nh·∫•t:
"aiPos_x,aiPos_y-humanPos_x,humanPos_y-aiWalls-humanWalls-wallsList"

V√≠ d·ª•: "4,8-4,0-9-10-7,4,False|5,6,True"
- AI ·ªü (4,8), Human ·ªü (4,0)
- AI c√≥ 9 walls, Human c√≥ 10 walls
- Walls ƒë√£ ƒë·∫∑t: (7,4,False), (5,6,True)

2.5 ACTION ENCODING:
Actions ƒë∆∞·ª£c encode th√†nh string:
- Movement: "M:x,y" (v√≠ d·ª•: "M:4,7")
- Wall: "W:x,y,horizontal" (v√≠ d·ª•: "W:3,5,True")

2.6 EPSILON-GREEDY STRATEGY:
```csharp
public AIMove ChooseAction(GameState state, List<AIMove> possibleMoves)
{
    // Exploration vs Exploitation trade-off
    if (Random.Range(0f, 1f) < epsilon)
    {
        // EXPLORATION: Ch·ªçn random ƒë·ªÉ kh√°m ph√°
        return possibleMoves[Random.Range(0, possibleMoves.Count)];
    }
    else
    {
        // EXPLOITATION: Ch·ªçn action c√≥ Q-value cao nh·∫•t
        return GetBestAction(state, possibleMoves);
    }
}
```

2.7 EPSILON DECAY:
Epsilon gi·∫£m d·∫ßn t·ª´ 0.995 ‚Üí 0.1 qua training:
- Ban ƒë·∫ßu: High exploration (99.5%)
- Cu·ªëi training: High exploitation (90%)

2.8 REWARD SYSTEM:
- Win game: +1000
- Lose game: -1000
- Move toward goal: +10
- Block opponent: +50
- Waste move: -5
- Strategic position: +15

================================================================================
                3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
================================================================================

3.1 GI·ªöI THI·ªÜU MINIMAX:
Minimax l√† thu·∫≠t to√°n game theory c·ªï ƒëi·ªÉn, t√¨m ki·∫øm move t·ªëi ∆∞u b·∫±ng c√°ch
d·ª± ƒëo√°n tr∆∞·ªõc nhi·ªÅu moves. Alpha-Beta pruning t·ªëi ∆∞u hi·ªáu su·∫•t b·∫±ng c√°ch
lo·∫°i b·ªè c√°c nh√°nh kh√¥ng c·∫ßn thi·∫øt.

3.2 NGUY√äN L√ù HO·∫†T ƒê·ªòNG:
- Maximizing Player (AI): T·ªëi ƒëa h√≥a score
- Minimizing Player (Human): T·ªëi thi·ªÉu h√≥a score AI
- Search depth: 4 levels (configurable)
- Pruning: Lo·∫°i b·ªè nh√°nh kh√¥ng promising

3.3 IMPLEMENTATION:

```csharp
float Minimax(GameState state, int depth, bool isMaximizing, float alpha, float beta)
{
    // Base case: Depth = 0 ho·∫∑c game over
    if (depth == 0 || IsGameOver(state))
        return EvaluatePosition(state);
    
    if (isMaximizing) // AI turn
    {
        float maxEval = float.NegativeInfinity;
        foreach (var move in GeneratePossibleMoves(state, true))
        {
            GameState newState = ApplyMove(state, move, true);
            float eval = Minimax(newState, depth - 1, false, alpha, beta);
            maxEval = Mathf.Max(maxEval, eval);
            alpha = Mathf.Max(alpha, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return maxEval;
    }
    else // Human turn
    {
        float minEval = float.PositiveInfinity;
        foreach (var move in GeneratePossibleMoves(state, false))
        {
            GameState newState = ApplyMove(state, move, false);
            float eval = Minimax(newState, depth - 1, true, alpha, beta);
            minEval = Mathf.Min(minEval, eval);
            beta = Mathf.Min(beta, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return minEval;
    }
}
```

3.4 EVALUATION FUNCTION:
Position evaluation d·ª±a tr√™n multiple factors:

```csharp
float EvaluatePosition(GameState state)
{
    float score = 0f;
    
    // Distance to goal (c√†ng g·∫ßn c√†ng t·ªët)
    int aiDistance = GetShortestPath(state.aiPosition, goalRow);
    int humanDistance = GetShortestPath(state.humanPosition, goalRow);
    score += (humanDistance - aiDistance) * 100;
    
    // Wall advantage
    score += (state.aiWallsLeft - state.humanWallsLeft) * 50;
    
    // Mobility (s·ªë moves c√≥ th·ªÉ)
    int aiMobility = GetValidMoves(state.aiPosition).Count;
    int humanMobility = GetValidMoves(state.humanPosition).Count;
    score += (aiMobility - humanMobility) * 20;
    
    // Center control
    score += GetCenterControlScore(state) * 30;
    
    return score;
}
```

3.5 ALPHA-BETA PRUNING HI·ªÜU QU·∫¢:
- Reduces search space t·ª´ O(b^d) xu·ªëng O(b^(d/2))
- Best case: 50% reduction in nodes evaluated
- Ordering moves by promising gi√∫p pruning hi·ªáu qu·∫£ h∆°n

================================================================================
                        4. SPECIALIZED AI STYLES SYSTEM
================================================================================

4.1 CONCEPT:
6 AI personalities v·ªõi training v√† behavior ri√™ng bi·ªát, m·ªói style c√≥:
- Ri√™ng Q-table file
- Custom reward structure
- Specific move prioritization
- Unique decision making logic

4.2 AI STYLES CHI TI·∫æT:

üî• AGGRESSIVE STYLE:
- M·ª•c ti√™u: T·∫•n c√¥ng v√† ch·∫∑n ƒë·ªëi th·ªß
- Wall Usage: 70% chance
- Reward Structure:
  * blockingOpponentReward = 50 * intensity
  * wallPlacementReward = 30 * intensity
  * defensiveWallPenalty = -20 * intensity
- Q-table: qtable_aggressive.json

üõ°Ô∏è DEFENSIVE STYLE:
- M·ª•c ti√™u: Ph√≤ng th·ªß v√† di chuy·ªÉn an to√†n
- Wall Usage: 50% chance
- Reward Structure:
  * defensiveWallReward = 35 * intensity
  * movementReward = 20 * intensity
  * aggressiveMovePenalty = -15 * intensity
- Q-table: qtable_defensive.json

üß± WALL MASTER STYLE:
- M·ª•c ti√™u: Chuy√™n gia ƒë·∫∑t t∆∞·ªùng v√† t·∫°o m√™ cung
- Wall Usage: 90% chance
- Reward Structure:
  * wallPlacementReward = 60 * intensity
  * strategicWallReward = 40 * intensity
  * wallComboReward = 50 * intensity
  * movementWithoutWallPenalty = -25 * intensity
- Q-table: qtable_wallmaster.json

‚ö° SPEED RUNNER STYLE:
- M·ª•c ti√™u: ƒê·∫°t ƒë√≠ch nhanh nh·∫•t
- Wall Usage: 10% chance
- Reward Structure:
  * movementReward = 40 * intensity
  * progressToGoalReward = 50 * intensity
  * quickWinBonus = 100 * intensity
  * wallPlacementPenalty = -20 * intensity
- Q-table: qtable_speedrunner.json

üß† TACTICAL STYLE:
- M·ª•c ti√™u: L·ª£i th·∫ø v·ªã tr√≠ v√† k·∫ø ho·∫°ch d√†i h·∫°n
- Wall Usage: Variable (strategic)
- Reward Structure:
  * tacticalMoveReward = 45 * intensity
  * positionAdvantageReward = 35 * intensity
  * mobilityReward = 25 * intensity
  * patternRecognitionBonus = 40 * intensity
- Q-table: qtable_tactical.json

‚öñÔ∏è BALANCED STYLE:
- M·ª•c ti√™u: C√¢n b·∫±ng t·∫•t c·∫£ aspects
- Wall Usage: 30% chance
- Reward Structure: Default balanced values
- Q-table: qtable.json

4.3 STYLE SWITCHING MECHANISM:

```csharp
public void SwitchToStyle(PlayStyle newStyle)
{
    // Save current Q-table
    if (allowQTableSaving) qAgent.SaveQTable(GetStyleSpecificQTablePath());
    
    // Load new style Q-table
    specializedStyle = newStyle;
    qAgent.LoadQTable(GetStyleSpecificQTablePath());
    
    // Reconfigure parameters
    ConfigureStyleSpecificTraining();
}
```

4.4 STYLE INTENSITY:
Parameter t·ª´ 0.0 ‚Üí 1.0 control m·ª©c ƒë·ªô chuy√™n h√≥a:
- 0.0: Balanced behavior
- 0.5: Moderate specialization
- 1.0: Extreme specialization

================================================================================
                        5. DECISION TREE CLASSIFIER
================================================================================

5.1 PURPOSE:
Decision Tree ph√¢n t√≠ch game context ƒë·ªÉ ch·ªçn strategy ph√π h·ª£p, d·ª±a tr√™n:
- Game phase (Early/Mid/End)
- Position advantage
- Resource advantage
- Mobility situation

5.2 FEATURE EXTRACTION:

```csharp
public struct DecisionFeatures
{
    public int aiDistanceToGoal;      // AI distance to goal line
    public int humanDistanceToGoal;   // Human distance to goal line
    public int aiWallsLeft;           // AI walls remaining
    public int humanWallsLeft;        // Human walls remaining
    public int aiMobility;            // AI possible moves
    public int humanMobility;         // Human possible moves
    public float distanceAdvantage;   // Human - AI distance
    public float wallAdvantage;       // AI - Human walls
    public bool isEarlyGame;          // > 15 moves estimated remaining
    public bool isMidGame;            // 5-15 moves remaining
    public bool isEndGame;            // < 5 moves remaining
}
```

5.3 DECISION LOGIC:

```csharp
public AIStrategy Predict(DecisionFeatures features)
{
    // Early Game: Play conservatively
    if (features.isEarlyGame)
        return AIStrategy.Balanced;
    
    // AI has significant lead: Focus on blocking
    if (features.distanceAdvantage > 2)
        return AIStrategy.Blocking;
    
    // AI is behind: Be aggressive
    if (features.distanceAdvantage < -1)
        return AIStrategy.Aggressive;
    
    // End game with wall advantage: Play defensively
    if (features.isEndGame && features.wallAdvantage > 2)
        return AIStrategy.Defensive;
    
    // Close game: Balanced approach
    return AIStrategy.Balanced;
}
```

5.4 STRATEGY OUTCOMES:
- Balanced: Equal weight to movement v√† walls
- Blocking: Prioritize moves that increase opponent path
- Aggressive: Focus on attacking positions
- Defensive: Protect own position and path

================================================================================
                        6. ENTROPY ANALYSIS & SOFTMAX
================================================================================

6.1 ENTROPY CONCEPT:
Entropy ƒëo ƒë·ªô ƒëa d·∫°ng/uncertainty c·ªßa move scores. High entropy = nhi·ªÅu moves
c√≥ scores t∆∞∆°ng ƒë∆∞∆°ng, low entropy = m·ªôt move r√µ r√†ng t·ªët nh·∫•t.

6.2 ENTROPY CALCULATION:

```csharp
float CalculateEntropy(List<float> moveScores)
{
    // Normalize th√†nh probabilities
    float total = moveScores.Sum();
    var probabilities = moveScores.Select(s => s / total);
    
    // Calculate Shannon entropy
    float entropy = 0f;
    foreach (float p in probabilities)
    {
        if (p > 0f)
            entropy -= p * Mathf.Log(p, 2f);
    }
    
    return entropy; // Range: 0 (deterministic) to log2(n) (uniform)
}
```

6.3 SOFTMAX DISTRIBUTION:

```csharp
float[] CalculateSoftmax(List<float> scores, float temperature)
{
    float[] probabilities = new float[scores.Count];
    float sum = scores.Sum(s => Mathf.Exp(s / temperature));
    
    for (int i = 0; i < scores.Count; i++)
    {
        probabilities[i] = Mathf.Exp(scores[i] / temperature) / sum;
    }
    
    return probabilities;
}
```

6.4 TEMPERATURE PARAMETER:
- Low temperature (0.1): Sharp distribution, focus on best moves
- High temperature (2.0): Flat distribution, more exploration
- Default temperature (1.0): Balanced exploitation/exploration

6.5 ADAPTIVE MOVE SELECTION:

```csharp
public AIMove ChooseActionWithEntropy(GameState state, List<AIMove> possibleMoves)
{
    var moveScores = EvaluateAllMoves(state, possibleMoves);
    float entropy = CalculateEntropy(moveScores);
    
    if (entropy > entropyThreshold) // High diversity
    {
        // Use softmax for probabilistic selection
        var probabilities = CalculateSoftmax(moveScores, softmaxTemperature);
        return SelectMoveByProbability(possibleMoves, probabilities);
    }
    else // Clear best move
    {
        // Use deterministic best move
        return GetBestScoredMove(possibleMoves, moveScores);
    }
}
```

================================================================================
                        7. TRAINING SYSTEM & SELF-PLAY
================================================================================

7.1 SELF-PLAY CONCEPT:
AI h·ªçc b·∫±ng c√°ch ch∆°i v·ªõi ch√≠nh n√≥ h√†ng ng√†n l·∫ßn, gradually improve performance
th√¥ng qua trial v√† error.

7.2 TRAINING PIPELINE:

```csharp
IEnumerator RunSelfPlayTraining()
{
    for (int episode = 1; episode <= maxEpisodes; episode++)
    {
        // Reset game state
        ResetGameForTraining();
        
        while (!IsGameOver())
        {
            // Get current state
            GameState currentState = GetCurrentGameState();
            List<AIMove> possibleMoves = GetPossibleMoves();
            
            // Choose action using Œµ-greedy
            AIMove chosenMove = qLearningAgent.ChooseAction(currentState, possibleMoves);
            
            // Execute move
            ExecuteMove(chosenMove);
            
            // Calculate reward
            float reward = CalculateReward(currentState, chosenMove);
            
            // Update Q-table
            GameState newState = GetCurrentGameState();
            qLearningAgent.UpdateQValue(
                EncodeState(currentState),
                EncodeAction(chosenMove),
                reward,
                EncodeState(newState)
            );
            
            yield return new WaitForEndOfFrame();
        }
        
        // Epsilon decay
        qLearningAgent.DecayEpsilon();
        
        // Save progress periodically
        if (episode % 100 == 0)
        {
            qLearningAgent.SaveQTable();
        }
    }
}
```

7.3 REWARD ENGINEERING:
Critical cho Q-Learning success:

```csharp
float CalculateReward(GameState state, AIMove move)
{
    float reward = 0f;
    
    // Game outcome rewards
    if (IsWin(state)) reward += 1000f;
    if (IsLoss(state)) reward -= 1000f;
    
    // Progress rewards
    if (move.moveType == MoveType.Movement)
    {
        if (IsMoveTowardGoal(move)) reward += 20f;
        else reward -= 5f;
    }
    
    // Strategic rewards
    if (move.moveType == MoveType.WallPlacement)
    {
        if (IsWallBlockingOpponent(move)) reward += 50f;
        if (IsWallStrategic(move)) reward += 30f;
        if (IsWallWasted(move)) reward -= 25f;
    }
    
    // Style-specific rewards
    reward += GetStyleSpecificReward(state, move);
    
    return reward;
}
```

7.4 TRAINING METRICS:
- Episodes: 1000 standard
- Duration: ~15 minutes
- Convergence: Usually around episode 800
- Q-table size: 13,118+ states at completion
- Final epsilon: 0.1 (90% exploitation)

================================================================================
                        8. AI TESTING FRAMEWORK
================================================================================

8.1 TESTING PHILOSOPHY:
Comprehensive testing ensures AI quality v√† reliability across different
game scenarios v√† edge cases.

8.2 TEST CATEGORIES:

A) DECISION TREE TESTS:
```csharp
void TestDecisionTreeStrategies()
{
    // Test Early Game
    var earlyGameFeatures = new DecisionFeatures
    {
        aiDistanceToGoal = 8, humanDistanceToGoal = 8,
        isEarlyGame = true
    };
    var strategy = decisionTree.Predict(earlyGameFeatures);
    Assert.AreEqual(AIStrategy.Balanced, strategy);
    
    // Test AI Leading
    var leadingFeatures = new DecisionFeatures
    {
        distanceAdvantage = 3, // AI leading
        isMidGame = true
    };
    strategy = decisionTree.Predict(leadingFeatures);
    Assert.AreEqual(AIStrategy.Blocking, strategy);
}
```

B) ENTROPY ANALYSIS TESTS:
```csharp
void TestEntropyAnalysis()
{
    // High entropy case (diverse scores)
    var highEntropyScores = new List<float> { 100f, 98f, 97f, 95f, 94f };
    float entropy = CalculateTestEntropy(highEntropyScores);
    Assert.IsTrue(entropy > 0.5f);
    
    // Low entropy case (clear winner)
    var lowEntropyScores = new List<float> { 100f, 50f, 50f, 50f, 50f };
    entropy = CalculateTestEntropy(lowEntropyScores);
    Assert.IsTrue(entropy < 0.5f);
}
```

C) SOFTMAX DISTRIBUTION TESTS:
```csharp
void TestSoftmaxDistribution()
{
    var testScores = new List<float> { 100f, 98f, 95f, 90f };
    float temperature = 1.0f;
    
    var probabilities = CalculateSoftmax(testScores, temperature);
    
    // Verify probabilities sum to 1
    float sum = probabilities.Sum();
    Assert.AreEqual(1.0f, sum, 0.001f);
    
    // Verify highest score gets highest probability
    int maxIndex = Array.IndexOf(probabilities, probabilities.Max());
    Assert.AreEqual(0, maxIndex);
}
```

D) PERFORMANCE BENCHMARKS:
- Move generation time: <100ms
- Q-Learning decision: <50ms
- Minimax decision (depth 4): <200ms
- Memory usage: <256MB

8.3 AUTOMATED TESTING:
Tests run automatically ƒë·ªÉ ensure code quality:
- Unit tests cho individual algorithms
- Integration tests cho AI system interactions
- Performance tests cho response times
- Regression tests ƒë·ªÉ prevent quality degradation

================================================================================
                        9. HYBRID AI ARCHITECTURE
================================================================================

9.1 CONCEPT:
Hybrid system k·∫øt h·ª£p strengths c·ªßa multiple algorithms:
- Q-Learning: Data-driven, learns from experience
- Minimax: Logical, guaranteed optimal given evaluation
- Decision Tree: Fast, context-aware switching
- Entropy: Adaptive, handles uncertainty

9.2 ALGORITHM SELECTION LOGIC:

```csharp
public AIMove GetBestMove()
{
    UpdateGameState();
    List<AIMove> possibleMoves = GeneratePossibleMoves();
    
    // Primary: Use Q-Learning if trained model available
    if (useQLearning && qAgent.IsTrainedModel())
    {
        Debug.Log($"üß† Using {specializedStyle} Q-Learning");
        return qAgent.ChooseAction(currentGameState, possibleMoves);
    }
    
    // Fallback: Use Minimax for guaranteed reasonable play
    else
    {
        Debug.Log($"üîç Using {specializedStyle} Minimax");
        return FindBestMoveWithMinimax();
    }
}
```

9.3 SEAMLESS SWITCHING:
- No interruption in gameplay
- Maintains consistent API
- Preserves game state integrity
- Logs algorithm used for debugging

9.4 BENEFITS:
- Robustness: Fallback ensures AI never fails
- Performance: Q-Learning for trained scenarios, Minimax for edge cases
- Flexibility: Can switch algorithms based on situation
- Development: Easy to test individual components

================================================================================
                        10. PERFORMANCE OPTIMIZATION
================================================================================

10.1 Q-TABLE OPTIMIZATION:
- Sparse representation: Only store visited states
- Compression: String interning for state keys
- Lazy loading: Load Q-table on demand
- Periodic saving: Every 100 training episodes

10.2 MINIMAX OPTIMIZATION:
- Alpha-Beta pruning: 50% search reduction
- Move ordering: Promising moves first
- Iterative deepening: Gradual depth increase
- Transposition tables: Cache evaluated positions

10.3 MEMORY MANAGEMENT:
- Object pooling cho GameState objects
- String caching cho repeated state/action encodings
- Garbage collection optimization
- Memory profiling v√† monitoring

10.4 MULTITHREADING CONSIDERATIONS:
- Q-Learning training: Background thread
- Move calculation: Main thread (Unity requirement)
- File I/O: Async operations where possible
- Thread-safe collections cho shared data

10.5 CACHING STRATEGIES:
- Path finding results
- Move generation results
- Evaluation function results
- Feature extraction results

================================================================================
                            K·∫æT LU·∫¨N
================================================================================

D·ª± √°n AI Quoridor Project th·ªÉ hi·ªán m·ªôt implementation comprehensive c·ªßa modern
game AI techniques:

TECHNICAL INNOVATIONS:
‚úÖ Hybrid Q-Learning + Minimax architecture
‚úÖ 6 specialized AI personalities v·ªõi separate training
‚úÖ Advanced decision making v·ªõi entropy analysis
‚úÖ Comprehensive testing v√† validation framework
‚úÖ Professional development tools v√† debugging

EDUCATIONAL VALUE:
‚úÖ Practical machine learning implementation
‚úÖ Traditional game AI algorithms
‚úÖ Modern software engineering practices
‚úÖ Performance optimization techniques
‚úÖ Automated testing methodologies

FUTURE ENHANCEMENTS:
üöÄ Deep Q-Learning (DQN) integration
üöÄ Multi-agent reinforcement learning
üöÄ Neural network-based evaluation functions
üöÄ Online learning v√† adaptation
üöÄ Transfer learning between game variants

D·ª± √°n n√†y demonstrating successful integration c·ªßa classical AI methods v·ªõi
modern machine learning, t·∫°o ra m·ªôt intelligent, adaptive, v√† highly
interactive gaming experience.

================================================================================
                            APPENDIX
================================================================================

FILES LI√äN QUAN:
- Assets/scripts/QLearningAgent.cs - Q-Learning implementation
- Assets/scripts/QuoridorAI.cs - Main AI controller
- Assets/scripts/DecisionTreeClassifier.cs - Decision tree logic
- Assets/scripts/QuoridorSelfPlayTrainer.cs - Training system
- Assets/scripts/AITestScript.cs - Testing framework
- Assets/scripts/Editor/ForceRetrainAI.cs - Development tools

Q-TABLE FILES:
- StreamingAssets/qtable.json - Balanced style
- StreamingAssets/qtable_aggressive.json - Aggressive style
- StreamingAssets/qtable_defensive.json - Defensive style
- StreamingAssets/qtable_wallmaster.json - Wall Master style
- StreamingAssets/qtable_speedrunner.json - Speed Runner style
- StreamingAssets/qtable_tactical.json - Tactical style

DOCUMENTATION:
- AI_ALGORITHMS_DETAILED_DESCRIPTION.txt - This file
- README.md - Project overview
- API_INTEGRATION_GUIDE.md - Voice/Chat integration
- TRAINING_GUIDE.md - AI training instructions

================================================================================
================================================================================
                    AI QUORIDOR GAME PROJECT - THU·∫¨T TO√ÅN AI CHI TI·∫æT
================================================================================

D·ª± √°n: AI Quoridor Game v·ªõi Machine Learning v√† Multi-Platform Integration
Ng√†y t·∫°o: 25/07/2025
T√°c gi·∫£: [Student Name]
Platform: Unity 3D
Language: C#

================================================================================
                            M·ª§C L·ª§C
================================================================================

1. T·ªîNG QUAN KI·∫æN TR√öC AI
2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
4. SPECIALIZED AI STYLES SYSTEM
5. DECISION TREE CLASSIFIER
6. ENTROPY ANALYSIS & SOFTMAX
7. TRAINING SYSTEM & SELF-PLAY
8. AI TESTING FRAMEWORK
9. HYBRID AI ARCHITECTURE
10. PERFORMANCE OPTIMIZATION

================================================================================
                        1. T·ªîNG QUAN KI·∫æN TR√öC AI
================================================================================

D·ª± √°n AI Quoridor s·ª≠ d·ª•ng m·ªôt ki·∫øn tr√∫c AI hybrid ti√™n ti·∫øn, k·∫øt h·ª£p:

1.1 CORE ALGORITHMS:
- Q-Learning (Reinforcement Learning) - Thu·∫≠t to√°n ch√≠nh
- Minimax + Alpha-Beta Pruning - Thu·∫≠t to√°n fallback
- Decision Tree Classifier - Ph√¢n t√≠ch ng·ªØ c·∫£nh game
- Entropy Analysis - ƒê√°nh gi√° ƒëa d·∫°ng move

1.2 SPECIALIZED SYSTEMS:
- 6 AI Personality Styles v·ªõi training ri√™ng bi·ªát
- Self-Play Training Pipeline t·ª± ƒë·ªông
- Real-time Performance Testing Framework
- Dynamic Algorithm Switching

1.3 TECHNICAL FOUNDATION:
- State Space: 13,118+ unique game states
- Action Space: Movement + Wall placement combinations
- Training Method: Self-play v·ªõi 1000 episodes
- Learning Rate: Œ± = 0.1, Discount Factor: Œ≥ = 0.99

================================================================================
                    2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
================================================================================

2.1 GI·ªöI THI·ªÜU Q-LEARNING:
Q-Learning l√† thu·∫≠t to√°n Reinforcement Learning kh√¥ng c·∫ßn model, h·ªçc th√¥ng qua
t∆∞∆°ng t√°c v·ªõi m√¥i tr∆∞·ªùng ƒë·ªÉ t·ªëi ∆∞u h√≥a policy. Trong Quoridor, AI h·ªçc c√°ch
ch·ªçn moves t·ªëi ∆∞u d·ª±a tr√™n kinh nghi·ªám t·ª´ h√†ng ng√†n game.

2.2 C√îNG TH·ª®C TO√ÅN H·ªåC:
Q(s,a) = Q(s,a) + Œ±[r + Œ≥*max(Q(s',a')) - Q(s,a)]

Trong ƒë√≥:
- Q(s,a): Q-value c·ªßa state s v·ªõi action a
- Œ±: Learning rate (0.1) - t·ªëc ƒë·ªô h·ªçc
- r: Immediate reward t·ª´ action
- Œ≥: Discount factor (0.99) - t·∫ßm quan tr·ªçng future rewards
- s': Next state sau khi th·ª±c hi·ªán action
- max(Q(s',a')): Maximum Q-value c·ªßa next state

2.3 IMPLEMENTATION CHI TI·∫æT:

```csharp
public class QLearningAgent
{
    // Core Q-Table: Dictionary l∆∞u tr·ªØ Q-values
    private Dictionary<string, Dictionary<string, float>> qTable;
    
    // Hyperparameters
    private float learningRate = 0.1f;      // T·ªëc ƒë·ªô h·ªçc
    private float discountFactor = 0.99f;   // Discount future rewards
    private float epsilon = 0.995f;         // Exploration rate
    
    // Q-Learning Update Implementation
    public void UpdateQValue(string state, string action, float reward, string nextState)
    {
        // L·∫•y Q-value hi·ªán t·∫°i
        float currentQ = GetQValue(state, action);
        
        // T√¨m max Q-value c·ªßa next state
        float maxNextQ = GetMaxQValue(nextState);
        
        // √Åp d·ª•ng c√¥ng th·ª©c Q-Learning
        float newQ = currentQ + learningRate * (reward + discountFactor * maxNextQ - currentQ);
        
        // C·∫≠p nh·∫≠t Q-table
        SetQValue(state, action, newQ);
    }
}
```

2.4 STATE ENCODING:
Game state ƒë∆∞·ª£c encode th√†nh string duy nh·∫•t:
"aiPos_x,aiPos_y-humanPos_x,humanPos_y-aiWalls-humanWalls-wallsList"

V√≠ d·ª•: "4,8-4,0-9-10-7,4,False|5,6,True"
- AI ·ªü (4,8), Human ·ªü (4,0)
- AI c√≥ 9 walls, Human c√≥ 10 walls
- Walls ƒë√£ ƒë·∫∑t: (7,4,False), (5,6,True)

2.5 ACTION ENCODING:
Actions ƒë∆∞·ª£c encode th√†nh string:
- Movement: "M:x,y" (v√≠ d·ª•: "M:4,7")
- Wall: "W:x,y,horizontal" (v√≠ d·ª•: "W:3,5,True")

2.6 EPSILON-GREEDY STRATEGY:
```csharp
public AIMove ChooseAction(GameState state, List<AIMove> possibleMoves)
{
    // Exploration vs Exploitation trade-off
    if (Random.Range(0f, 1f) < epsilon)
    {
        // EXPLORATION: Ch·ªçn random ƒë·ªÉ kh√°m ph√°
        return possibleMoves[Random.Range(0, possibleMoves.Count)];
    }
    else
    {
        // EXPLOITATION: Ch·ªçn action c√≥ Q-value cao nh·∫•t
        return GetBestAction(state, possibleMoves);
    }
}
```

2.7 EPSILON DECAY:
Epsilon gi·∫£m d·∫ßn t·ª´ 0.995 ‚Üí 0.1 qua training:
- Ban ƒë·∫ßu: High exploration (99.5%)
- Cu·ªëi training: High exploitation (90%)

2.8 REWARD SYSTEM:
- Win game: +1000
- Lose game: -1000
- Move toward goal: +10
- Block opponent: +50
- Waste move: -5
- Strategic position: +15

================================================================================
                3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
================================================================================

3.1 GI·ªöI THI·ªÜU MINIMAX:
Minimax l√† thu·∫≠t to√°n game theory c·ªï ƒëi·ªÉn, t√¨m ki·∫øm move t·ªëi ∆∞u b·∫±ng c√°ch
d·ª± ƒëo√°n tr∆∞·ªõc nhi·ªÅu moves. Alpha-Beta pruning t·ªëi ∆∞u hi·ªáu su·∫•t b·∫±ng c√°ch
lo·∫°i b·ªè c√°c nh√°nh kh√¥ng c·∫ßn thi·∫øt.

3.2 NGUY√äN L√ù HO·∫†T ƒê·ªòNG:
- Maximizing Player (AI): T·ªëi ƒëa h√≥a score
- Minimizing Player (Human): T·ªëi thi·ªÉu h√≥a score AI
- Search depth: 4 levels (configurable)
- Pruning: Lo·∫°i b·ªè nh√°nh kh√¥ng promising

3.3 IMPLEMENTATION:

```csharp
float Minimax(GameState state, int depth, bool isMaximizing, float alpha, float beta)
{
    // Base case: Depth = 0 ho·∫∑c game over
    if (depth == 0 || IsGameOver(state))
        return EvaluatePosition(state);
    
    if (isMaximizing) // AI turn
    {
        float maxEval = float.NegativeInfinity;
        foreach (var move in GeneratePossibleMoves(state, true))
        {
            GameState newState = ApplyMove(state, move, true);
            float eval = Minimax(newState, depth - 1, false, alpha, beta);
            maxEval = Mathf.Max(maxEval, eval);
            alpha = Mathf.Max(alpha, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return maxEval;
    }
    else // Human turn
    {
        float minEval = float.PositiveInfinity;
        foreach (var move in GeneratePossibleMoves(state, false))
        {
            GameState newState = ApplyMove(state, move, false);
            float eval = Minimax(newState, depth - 1, true, alpha, beta);
            minEval = Mathf.Min(minEval, eval);
            beta = Mathf.Min(beta, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return minEval;
    }
}
```

3.4 EVALUATION FUNCTION:
Position evaluation d·ª±a tr√™n multiple factors:

```csharp
float EvaluatePosition(GameState state)
{
    float score = 0f;
    
    // Distance to goal (c√†ng g·∫ßn c√†ng t·ªët)
    int aiDistance = GetShortestPath(state.aiPosition, goalRow);
    int humanDistance = GetShortestPath(state.humanPosition, goalRow);
    score += (humanDistance - aiDistance) * 100;
    
    // Wall advantage
    score += (state.aiWallsLeft - state.humanWallsLeft) * 50;
    
    // Mobility (s·ªë moves c√≥ th·ªÉ)
    int aiMobility = GetValidMoves(state.aiPosition).Count;
    int humanMobility = GetValidMoves(state.humanPosition).Count;
    score += (aiMobility - humanMobility) * 20;
    
    // Center control
    score += GetCenterControlScore(state) * 30;
    
    return score;
}
```

3.5 ALPHA-BETA PRUNING HI·ªÜU QU·∫¢:
- Reduces search space t·ª´ O(b^d) xu·ªëng O(b^(d/2))
- Best case: 50% reduction in nodes evaluated
- Ordering moves by promising gi√∫p pruning hi·ªáu qu·∫£ h∆°n

================================================================================
                        4. SPECIALIZED AI STYLES SYSTEM
================================================================================

4.1 CONCEPT:
6 AI personalities v·ªõi training v√† behavior ri√™ng bi·ªát, m·ªói style c√≥:
- Ri√™ng Q-table file
- Custom reward structure
- Specific move prioritization
- Unique decision making logic

4.2 AI STYLES CHI TI·∫æT:

üî• AGGRESSIVE STYLE:
- M·ª•c ti√™u: T·∫•n c√¥ng v√† ch·∫∑n ƒë·ªëi th·ªß
- Wall Usage: 70% chance
- Reward Structure:
  * blockingOpponentReward = 50 * intensity
  * wallPlacementReward = 30 * intensity
  * defensiveWallPenalty = -20 * intensity
- Q-table: qtable_aggressive.json

üõ°Ô∏è DEFENSIVE STYLE:
- M·ª•c ti√™u: Ph√≤ng th·ªß v√† di chuy·ªÉn an to√†n
- Wall Usage: 50% chance
- Reward Structure:
  * defensiveWallReward = 35 * intensity
  * movementReward = 20 * intensity
  * aggressiveMovePenalty = -15 * intensity
- Q-table: qtable_defensive.json

üß± WALL MASTER STYLE:
- M·ª•c ti√™u: Chuy√™n gia ƒë·∫∑t t∆∞·ªùng v√† t·∫°o m√™ cung
- Wall Usage: 90% chance
- Reward Structure:
  * wallPlacementReward = 60 * intensity
  * strategicWallReward = 40 * intensity
  * wallComboReward = 50 * intensity
  * movementWithoutWallPenalty = -25 * intensity
- Q-table: qtable_wallmaster.json

‚ö° SPEED RUNNER STYLE:
- M·ª•c ti√™u: ƒê·∫°t ƒë√≠ch nhanh nh·∫•t
- Wall Usage: 10% chance
- Reward Structure:
  * movementReward = 40 * intensity
  * progressToGoalReward = 50 * intensity
  * quickWinBonus = 100 * intensity
  * wallPlacementPenalty = -20 * intensity
- Q-table: qtable_speedrunner.json

üß† TACTICAL STYLE:
- M·ª•c ti√™u: L·ª£i th·∫ø v·ªã tr√≠ v√† k·∫ø ho·∫°ch d√†i h·∫°n
- Wall Usage: Variable (strategic)
- Reward Structure:
  * tacticalMoveReward = 45 * intensity
  * positionAdvantageReward = 35 * intensity
  * mobilityReward = 25 * intensity
  * patternRecognitionBonus = 40 * intensity
- Q-table: qtable_tactical.json

‚öñÔ∏è BALANCED STYLE:
- M·ª•c ti√™u: C√¢n b·∫±ng t·∫•t c·∫£ aspects
- Wall Usage: 30% chance
- Reward Structure: Default balanced values
- Q-table: qtable.json

4.3 STYLE SWITCHING MECHANISM:

```csharp
public void SwitchToStyle(PlayStyle newStyle)
{
    // Save current Q-table
    if (allowQTableSaving) qAgent.SaveQTable(GetStyleSpecificQTablePath());
    
    // Load new style Q-table
    specializedStyle = newStyle;
    qAgent.LoadQTable(GetStyleSpecificQTablePath());
    
    // Reconfigure parameters
    ConfigureStyleSpecificTraining();
}
```

4.4 STYLE INTENSITY:
Parameter t·ª´ 0.0 ‚Üí 1.0 control m·ª©c ƒë·ªô chuy√™n h√≥a:
- 0.0: Balanced behavior
- 0.5: Moderate specialization
- 1.0: Extreme specialization

================================================================================
                        5. DECISION TREE CLASSIFIER
================================================================================

5.1 PURPOSE:
Decision Tree ph√¢n t√≠ch game context ƒë·ªÉ ch·ªçn strategy ph√π h·ª£p, d·ª±a tr√™n:
- Game phase (Early/Mid/End)
- Position advantage
- Resource advantage
- Mobility situation

5.2 FEATURE EXTRACTION:

```csharp
public struct DecisionFeatures
{
    public int aiDistanceToGoal;      // AI distance to goal line
    public int humanDistanceToGoal;   // Human distance to goal line
    public int aiWallsLeft;           // AI walls remaining
    public int humanWallsLeft;        // Human walls remaining
    public int aiMobility;            // AI possible moves
    public int humanMobility;         // Human possible moves
    public float distanceAdvantage;   // Human - AI distance
    public float wallAdvantage;       // AI - Human walls
    public bool isEarlyGame;          // > 15 moves estimated remaining
    public bool isMidGame;            // 5-15 moves remaining
    public bool isEndGame;            // < 5 moves remaining
}
```

5.3 DECISION LOGIC:

```csharp
public AIStrategy Predict(DecisionFeatures features)
{
    // Early Game: Play conservatively
    if (features.isEarlyGame)
        return AIStrategy.Balanced;
    
    // AI has significant lead: Focus on blocking
    if (features.distanceAdvantage > 2)
        return AIStrategy.Blocking;
    
    // AI is behind: Be aggressive
    if (features.distanceAdvantage < -1)
        return AIStrategy.Aggressive;
    
    // End game with wall advantage: Play defensively
    if (features.isEndGame && features.wallAdvantage > 2)
        return AIStrategy.Defensive;
    
    // Close game: Balanced approach
    return AIStrategy.Balanced;
}
```

5.4 STRATEGY OUTCOMES:
- Balanced: Equal weight to movement v√† walls
- Blocking: Prioritize moves that increase opponent path
- Aggressive: Focus on attacking positions
- Defensive: Protect own position and path

================================================================================
                        6. ENTROPY ANALYSIS & SOFTMAX
================================================================================

6.1 ENTROPY CONCEPT:
Entropy ƒëo ƒë·ªô ƒëa d·∫°ng/uncertainty c·ªßa move scores. High entropy = nhi·ªÅu moves
c√≥ scores t∆∞∆°ng ƒë∆∞∆°ng, low entropy = m·ªôt move r√µ r√†ng t·ªët nh·∫•t.

6.2 ENTROPY CALCULATION:

```csharp
float CalculateEntropy(List<float> moveScores)
{
    // Normalize th√†nh probabilities
    float total = moveScores.Sum();
    var probabilities = moveScores.Select(s => s / total);
    
    // Calculate Shannon entropy
    float entropy = 0f;
    foreach (float p in probabilities)
    {
        if (p > 0f)
            entropy -= p * Mathf.Log(p, 2f);
    }
    
    return entropy; // Range: 0 (deterministic) to log2(n) (uniform)
}
```

6.3 SOFTMAX DISTRIBUTION:

```csharp
float[] CalculateSoftmax(List<float> scores, float temperature)
{
    float[] probabilities = new float[scores.Count];
    float sum = scores.Sum(s => Mathf.Exp(s / temperature));
    
    for (int i = 0; i < scores.Count; i++)
    {
        probabilities[i] = Mathf.Exp(scores[i] / temperature) / sum;
    }
    
    return probabilities;
}
```

6.4 TEMPERATURE PARAMETER:
- Low temperature (0.1): Sharp distribution, focus on best moves
- High temperature (2.0): Flat distribution, more exploration
- Default temperature (1.0): Balanced exploitation/exploration

6.5 ADAPTIVE MOVE SELECTION:

```csharp
public AIMove ChooseActionWithEntropy(GameState state, List<AIMove> possibleMoves)
{
    var moveScores = EvaluateAllMoves(state, possibleMoves);
    float entropy = CalculateEntropy(moveScores);
    
    if (entropy > entropyThreshold) // High diversity
    {
        // Use softmax for probabilistic selection
        var probabilities = CalculateSoftmax(moveScores, softmaxTemperature);
        return SelectMoveByProbability(possibleMoves, probabilities);
    }
    else // Clear best move
    {
        // Use deterministic best move
        return GetBestScoredMove(possibleMoves, moveScores);
    }
}
```

================================================================================
                        7. TRAINING SYSTEM & SELF-PLAY
================================================================================

7.1 SELF-PLAY CONCEPT:
AI h·ªçc b·∫±ng c√°ch ch∆°i v·ªõi ch√≠nh n√≥ h√†ng ng√†n l·∫ßn, gradually improve performance
th√¥ng qua trial v√† error.

7.2 TRAINING PIPELINE:

```csharp
IEnumerator RunSelfPlayTraining()
{
    for (int episode = 1; episode <= maxEpisodes; episode++)
    {
        // Reset game state
        ResetGameForTraining();
        
        while (!IsGameOver())
        {
            // Get current state
            GameState currentState = GetCurrentGameState();
            List<AIMove> possibleMoves = GetPossibleMoves();
            
            // Choose action using Œµ-greedy
            AIMove chosenMove = qLearningAgent.ChooseAction(currentState, possibleMoves);
            
            // Execute move
            ExecuteMove(chosenMove);
            
            // Calculate reward
            float reward = CalculateReward(currentState, chosenMove);
            
            // Update Q-table
            GameState newState = GetCurrentGameState();
            qLearningAgent.UpdateQValue(
                EncodeState(currentState),
                EncodeAction(chosenMove),
                reward,
                EncodeState(newState)
            );
            
            yield return new WaitForEndOfFrame();
        }
        
        // Epsilon decay
        qLearningAgent.DecayEpsilon();
        
        // Save progress periodically
        if (episode % 100 == 0)
        {
            qLearningAgent.SaveQTable();
        }
    }
}
```

7.3 REWARD ENGINEERING:
Critical cho Q-Learning success:

```csharp
float CalculateReward(GameState state, AIMove move)
{
    float reward = 0f;
    
    // Game outcome rewards
    if (IsWin(state)) reward += 1000f;
    if (IsLoss(state)) reward -= 1000f;
    
    // Progress rewards
    if (move.moveType == MoveType.Movement)
    {
        if (IsMoveTowardGoal(move)) reward += 20f;
        else reward -= 5f;
    }
    
    // Strategic rewards
    if (move.moveType == MoveType.WallPlacement)
    {
        if (IsWallBlockingOpponent(move)) reward += 50f;
        if (IsWallStrategic(move)) reward += 30f;
        if (IsWallWasted(move)) reward -= 25f;
    }
    
    // Style-specific rewards
    reward += GetStyleSpecificReward(state, move);
    
    return reward;
}
```

7.4 TRAINING METRICS:
- Episodes: 1000 standard
- Duration: ~15 minutes
- Convergence: Usually around episode 800
- Q-table size: 13,118+ states at completion
- Final epsilon: 0.1 (90% exploitation)

================================================================================
                        8. AI TESTING FRAMEWORK
================================================================================

8.1 TESTING PHILOSOPHY:
Comprehensive testing ensures AI quality v√† reliability across different
game scenarios v√† edge cases.

8.2 TEST CATEGORIES:

A) DECISION TREE TESTS:
```csharp
void TestDecisionTreeStrategies()
{
    // Test Early Game
    var earlyGameFeatures = new DecisionFeatures
    {
        aiDistanceToGoal = 8, humanDistanceToGoal = 8,
        isEarlyGame = true
    };
    var strategy = decisionTree.Predict(earlyGameFeatures);
    Assert.AreEqual(AIStrategy.Balanced, strategy);
    
    // Test AI Leading
    var leadingFeatures = new DecisionFeatures
    {
        distanceAdvantage = 3, // AI leading
        isMidGame = true
    };
    strategy = decisionTree.Predict(leadingFeatures);
    Assert.AreEqual(AIStrategy.Blocking, strategy);
}
```

B) ENTROPY ANALYSIS TESTS:
```csharp
void TestEntropyAnalysis()
{
    // High entropy case (diverse scores)
    var highEntropyScores = new List<float> { 100f, 98f, 97f, 95f, 94f };
    float entropy = CalculateTestEntropy(highEntropyScores);
    Assert.IsTrue(entropy > 0.5f);
    
    // Low entropy case (clear winner)
    var lowEntropyScores = new List<float> { 100f, 50f, 50f, 50f, 50f };
    entropy = CalculateTestEntropy(lowEntropyScores);
    Assert.IsTrue(entropy < 0.5f);
}
```

C) SOFTMAX DISTRIBUTION TESTS:
```csharp
void TestSoftmaxDistribution()
{
    var testScores = new List<float> { 100f, 98f, 95f, 90f };
    float temperature = 1.0f;
    
    var probabilities = CalculateSoftmax(testScores, temperature);
    
    // Verify probabilities sum to 1
    float sum = probabilities.Sum();
    Assert.AreEqual(1.0f, sum, 0.001f);
    
    // Verify highest score gets highest probability
    int maxIndex = Array.IndexOf(probabilities, probabilities.Max());
    Assert.AreEqual(0, maxIndex);
}
```

D) PERFORMANCE BENCHMARKS:
- Move generation time: <100ms
- Q-Learning decision: <50ms
- Minimax decision (depth 4): <200ms
- Memory usage: <256MB

8.3 AUTOMATED TESTING:
Tests run automatically ƒë·ªÉ ensure code quality:
- Unit tests cho individual algorithms
- Integration tests cho AI system interactions
- Performance tests cho response times
- Regression tests ƒë·ªÉ prevent quality degradation

================================================================================
                        9. HYBRID AI ARCHITECTURE
================================================================================

9.1 CONCEPT:
Hybrid system k·∫øt h·ª£p strengths c·ªßa multiple algorithms:
- Q-Learning: Data-driven, learns from experience
- Minimax: Logical, guaranteed optimal given evaluation
- Decision Tree: Fast, context-aware switching
- Entropy: Adaptive, handles uncertainty

9.2 ALGORITHM SELECTION LOGIC:

```csharp
public AIMove GetBestMove()
{
    UpdateGameState();
    List<AIMove> possibleMoves = GeneratePossibleMoves();
    
    // Primary: Use Q-Learning if trained model available
    if (useQLearning && qAgent.IsTrainedModel())
    {
        Debug.Log($"üß† Using {specializedStyle} Q-Learning");
        return qAgent.ChooseAction(currentGameState, possibleMoves);
    }
    
    // Fallback: Use Minimax for guaranteed reasonable play
    else
    {
        Debug.Log($"üîç Using {specializedStyle} Minimax");
        return FindBestMoveWithMinimax();
    }
}
```

9.3 SEAMLESS SWITCHING:
- No interruption in gameplay
- Maintains consistent API
- Preserves game state integrity
- Logs algorithm used for debugging

9.4 BENEFITS:
- Robustness: Fallback ensures AI never fails
- Performance: Q-Learning for trained scenarios, Minimax for edge cases
- Flexibility: Can switch algorithms based on situation
- Development: Easy to test individual components

================================================================================
                        10. PERFORMANCE OPTIMIZATION
================================================================================

10.1 Q-TABLE OPTIMIZATION:
- Sparse representation: Only store visited states
- Compression: String interning for state keys
- Lazy loading: Load Q-table on demand
- Periodic saving: Every 100 training episodes

10.2 MINIMAX OPTIMIZATION:
- Alpha-Beta pruning: 50% search reduction
- Move ordering: Promising moves first
- Iterative deepening: Gradual depth increase
- Transposition tables: Cache evaluated positions

10.3 MEMORY MANAGEMENT:
- Object pooling cho GameState objects
- String caching cho repeated state/action encodings
- Garbage collection optimization
- Memory profiling v√† monitoring

10.4 MULTITHREADING CONSIDERATIONS:
- Q-Learning training: Background thread
- Move calculation: Main thread (Unity requirement)
- File I/O: Async operations where possible
- Thread-safe collections cho shared data

10.5 CACHING STRATEGIES:
- Path finding results
- Move generation results
- Evaluation function results
- Feature extraction results

================================================================================
                            K·∫æT LU·∫¨N
================================================================================

D·ª± √°n AI Quoridor Project th·ªÉ hi·ªán m·ªôt implementation comprehensive c·ªßa modern
game AI techniques:

TECHNICAL INNOVATIONS:
‚úÖ Hybrid Q-Learning + Minimax architecture
‚úÖ 6 specialized AI personalities v·ªõi separate training
‚úÖ Advanced decision making v·ªõi entropy analysis
‚úÖ Comprehensive testing v√† validation framework
‚úÖ Professional development tools v√† debugging

EDUCATIONAL VALUE:
‚úÖ Practical machine learning implementation
‚úÖ Traditional game AI algorithms
‚úÖ Modern software engineering practices
‚úÖ Performance optimization techniques
‚úÖ Automated testing methodologies

FUTURE ENHANCEMENTS:
üöÄ Deep Q-Learning (DQN) integration
üöÄ Multi-agent reinforcement learning
üöÄ Neural network-based evaluation functions
üöÄ Online learning v√† adaptation
üöÄ Transfer learning between game variants

D·ª± √°n n√†y demonstrating successful integration c·ªßa classical AI methods v·ªõi
modern machine learning, t·∫°o ra m·ªôt intelligent, adaptive, v√† highly
interactive gaming experience.

================================================================================
                            APPENDIX
================================================================================

FILES LI√äN QUAN:
- Assets/scripts/QLearningAgent.cs - Q-Learning implementation
- Assets/scripts/QuoridorAI.cs - Main AI controller
- Assets/scripts/DecisionTreeClassifier.cs - Decision tree logic
- Assets/scripts/QuoridorSelfPlayTrainer.cs - Training system
- Assets/scripts/AITestScript.cs - Testing framework
- Assets/scripts/Editor/ForceRetrainAI.cs - Development tools

Q-TABLE FILES:
- StreamingAssets/qtable.json - Balanced style
- StreamingAssets/qtable_aggressive.json - Aggressive style
- StreamingAssets/qtable_defensive.json - Defensive style
- StreamingAssets/qtable_wallmaster.json - Wall Master style
- StreamingAssets/qtable_speedrunner.json - Speed Runner style
- StreamingAssets/qtable_tactical.json - Tactical style

DOCUMENTATION:
- AI_ALGORITHMS_DETAILED_DESCRIPTION.txt - This file
- README.md - Project overview
- API_INTEGRATION_GUIDE.md - Voice/Chat integration
- TRAINING_GUIDE.md - AI training instructions

================================================================================
================================================================================
                    AI QUORIDOR GAME PROJECT - THU·∫¨T TO√ÅN AI CHI TI·∫æT
================================================================================

D·ª± √°n: AI Quoridor Game v·ªõi Machine Learning v√† Multi-Platform Integration
Ng√†y t·∫°o: 25/07/2025
T√°c gi·∫£: [Student Name]
Platform: Unity 3D
Language: C#

================================================================================
                            M·ª§C L·ª§C
================================================================================

1. T·ªîNG QUAN KI·∫æN TR√öC AI
2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
4. SPECIALIZED AI STYLES SYSTEM
5. DECISION TREE CLASSIFIER
6. ENTROPY ANALYSIS & SOFTMAX
7. TRAINING SYSTEM & SELF-PLAY
8. AI TESTING FRAMEWORK
9. HYBRID AI ARCHITECTURE
10. PERFORMANCE OPTIMIZATION

================================================================================
                        1. T·ªîNG QUAN KI·∫æN TR√öC AI
================================================================================

D·ª± √°n AI Quoridor s·ª≠ d·ª•ng m·ªôt ki·∫øn tr√∫c AI hybrid ti√™n ti·∫øn, k·∫øt h·ª£p:

1.1 CORE ALGORITHMS:
- Q-Learning (Reinforcement Learning) - Thu·∫≠t to√°n ch√≠nh
- Minimax + Alpha-Beta Pruning - Thu·∫≠t to√°n fallback
- Decision Tree Classifier - Ph√¢n t√≠ch ng·ªØ c·∫£nh game
- Entropy Analysis - ƒê√°nh gi√° ƒëa d·∫°ng move

1.2 SPECIALIZED SYSTEMS:
- 6 AI Personality Styles v·ªõi training ri√™ng bi·ªát
- Self-Play Training Pipeline t·ª± ƒë·ªông
- Real-time Performance Testing Framework
- Dynamic Algorithm Switching

1.3 TECHNICAL FOUNDATION:
- State Space: 13,118+ unique game states
- Action Space: Movement + Wall placement combinations
- Training Method: Self-play v·ªõi 1000 episodes
- Learning Rate: Œ± = 0.1, Discount Factor: Œ≥ = 0.99

================================================================================
                    2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
================================================================================

2.1 GI·ªöI THI·ªÜU Q-LEARNING:
Q-Learning l√† thu·∫≠t to√°n Reinforcement Learning kh√¥ng c·∫ßn model, h·ªçc th√¥ng qua
t∆∞∆°ng t√°c v·ªõi m√¥i tr∆∞·ªùng ƒë·ªÉ t·ªëi ∆∞u h√≥a policy. Trong Quoridor, AI h·ªçc c√°ch
ch·ªçn moves t·ªëi ∆∞u d·ª±a tr√™n kinh nghi·ªám t·ª´ h√†ng ng√†n game.

2.2 C√îNG TH·ª®C TO√ÅN H·ªåC:
Q(s,a) = Q(s,a) + Œ±[r + Œ≥*max(Q(s',a')) - Q(s,a)]

Trong ƒë√≥:
- Q(s,a): Q-value c·ªßa state s v·ªõi action a
- Œ±: Learning rate (0.1) - t·ªëc ƒë·ªô h·ªçc
- r: Immediate reward t·ª´ action
- Œ≥: Discount factor (0.99) - t·∫ßm quan tr·ªçng future rewards
- s': Next state sau khi th·ª±c hi·ªán action
- max(Q(s',a')): Maximum Q-value c·ªßa next state

2.3 IMPLEMENTATION CHI TI·∫æT:

```csharp
public class QLearningAgent
{
    // Core Q-Table: Dictionary l∆∞u tr·ªØ Q-values
    private Dictionary<string, Dictionary<string, float>> qTable;
    
    // Hyperparameters
    private float learningRate = 0.1f;      // T·ªëc ƒë·ªô h·ªçc
    private float discountFactor = 0.99f;   // Discount future rewards
    private float epsilon = 0.995f;         // Exploration rate
    
    // Q-Learning Update Implementation
    public void UpdateQValue(string state, string action, float reward, string nextState)
    {
        // L·∫•y Q-value hi·ªán t·∫°i
        float currentQ = GetQValue(state, action);
        
        // T√¨m max Q-value c·ªßa next state
        float maxNextQ = GetMaxQValue(nextState);
        
        // √Åp d·ª•ng c√¥ng th·ª©c Q-Learning
        float newQ = currentQ + learningRate * (reward + discountFactor * maxNextQ - currentQ);
        
        // C·∫≠p nh·∫≠t Q-table
        SetQValue(state, action, newQ);
    }
}
```

2.4 STATE ENCODING:
Game state ƒë∆∞·ª£c encode th√†nh string duy nh·∫•t:
"aiPos_x,aiPos_y-humanPos_x,humanPos_y-aiWalls-humanWalls-wallsList"

V√≠ d·ª•: "4,8-4,0-9-10-7,4,False|5,6,True"
- AI ·ªü (4,8), Human ·ªü (4,0)
- AI c√≥ 9 walls, Human c√≥ 10 walls
- Walls ƒë√£ ƒë·∫∑t: (7,4,False), (5,6,True)

2.5 ACTION ENCODING:
Actions ƒë∆∞·ª£c encode th√†nh string:
- Movement: "M:x,y" (v√≠ d·ª•: "M:4,7")
- Wall: "W:x,y,horizontal" (v√≠ d·ª•: "W:3,5,True")

2.6 EPSILON-GREEDY STRATEGY:
```csharp
public AIMove ChooseAction(GameState state, List<AIMove> possibleMoves)
{
    // Exploration vs Exploitation trade-off
    if (Random.Range(0f, 1f) < epsilon)
    {
        // EXPLORATION: Ch·ªçn random ƒë·ªÉ kh√°m ph√°
        return possibleMoves[Random.Range(0, possibleMoves.Count)];
    }
    else
    {
        // EXPLOITATION: Ch·ªçn action c√≥ Q-value cao nh·∫•t
        return GetBestAction(state, possibleMoves);
    }
}
```

2.7 EPSILON DECAY:
Epsilon gi·∫£m d·∫ßn t·ª´ 0.995 ‚Üí 0.1 qua training:
- Ban ƒë·∫ßu: High exploration (99.5%)
- Cu·ªëi training: High exploitation (90%)

2.8 REWARD SYSTEM:
- Win game: +1000
- Lose game: -1000
- Move toward goal: +10
- Block opponent: +50
- Waste move: -5
- Strategic position: +15

================================================================================
                3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
================================================================================

3.1 GI·ªöI THI·ªÜU MINIMAX:
Minimax l√† thu·∫≠t to√°n game theory c·ªï ƒëi·ªÉn, t√¨m ki·∫øm move t·ªëi ∆∞u b·∫±ng c√°ch
d·ª± ƒëo√°n tr∆∞·ªõc nhi·ªÅu moves. Alpha-Beta pruning t·ªëi ∆∞u hi·ªáu su·∫•t b·∫±ng c√°ch
lo·∫°i b·ªè c√°c nh√°nh kh√¥ng c·∫ßn thi·∫øt.

3.2 NGUY√äN L√ù HO·∫†T ƒê·ªòNG:
- Maximizing Player (AI): T·ªëi ƒëa h√≥a score
- Minimizing Player (Human): T·ªëi thi·ªÉu h√≥a score AI
- Search depth: 4 levels (configurable)
- Pruning: Lo·∫°i b·ªè nh√°nh kh√¥ng promising

3.3 IMPLEMENTATION:

```csharp
float Minimax(GameState state, int depth, bool isMaximizing, float alpha, float beta)
{
    // Base case: Depth = 0 ho·∫∑c game over
    if (depth == 0 || IsGameOver(state))
        return EvaluatePosition(state);
    
    if (isMaximizing) // AI turn
    {
        float maxEval = float.NegativeInfinity;
        foreach (var move in GeneratePossibleMoves(state, true))
        {
            GameState newState = ApplyMove(state, move, true);
            float eval = Minimax(newState, depth - 1, false, alpha, beta);
            maxEval = Mathf.Max(maxEval, eval);
            alpha = Mathf.Max(alpha, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return maxEval;
    }
    else // Human turn
    {
        float minEval = float.PositiveInfinity;
        foreach (var move in GeneratePossibleMoves(state, false))
        {
            GameState newState = ApplyMove(state, move, false);
            float eval = Minimax(newState, depth - 1, true, alpha, beta);
            minEval = Mathf.Min(minEval, eval);
            beta = Mathf.Min(beta, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return minEval;
    }
}
```

3.4 EVALUATION FUNCTION:
Position evaluation d·ª±a tr√™n multiple factors:

```csharp
float EvaluatePosition(GameState state)
{
    float score = 0f;
    
    // Distance to goal (c√†ng g·∫ßn c√†ng t·ªët)
    int aiDistance = GetShortestPath(state.aiPosition, goalRow);
    int humanDistance = GetShortestPath(state.humanPosition, goalRow);
    score += (humanDistance - aiDistance) * 100;
    
    // Wall advantage
    score += (state.aiWallsLeft - state.humanWallsLeft) * 50;
    
    // Mobility (s·ªë moves c√≥ th·ªÉ)
    int aiMobility = GetValidMoves(state.aiPosition).Count;
    int humanMobility = GetValidMoves(state.humanPosition).Count;
    score += (aiMobility - humanMobility) * 20;
    
    // Center control
    score += GetCenterControlScore(state) * 30;
    
    return score;
}
```

3.5 ALPHA-BETA PRUNING HI·ªÜU QU·∫¢:
- Reduces search space t·ª´ O(b^d) xu·ªëng O(b^(d/2))
- Best case: 50% reduction in nodes evaluated
- Ordering moves by promising gi√∫p pruning hi·ªáu qu·∫£ h∆°n

================================================================================
                        4. SPECIALIZED AI STYLES SYSTEM
================================================================================

4.1 CONCEPT:
6 AI personalities v·ªõi training v√† behavior ri√™ng bi·ªát, m·ªói style c√≥:
- Ri√™ng Q-table file
- Custom reward structure
- Specific move prioritization
- Unique decision making logic

4.2 AI STYLES CHI TI·∫æT:

üî• AGGRESSIVE STYLE:
- M·ª•c ti√™u: T·∫•n c√¥ng v√† ch·∫∑n ƒë·ªëi th·ªß
- Wall Usage: 70% chance
- Reward Structure:
  * blockingOpponentReward = 50 * intensity
  * wallPlacementReward = 30 * intensity
  * defensiveWallPenalty = -20 * intensity
- Q-table: qtable_aggressive.json

üõ°Ô∏è DEFENSIVE STYLE:
- M·ª•c ti√™u: Ph√≤ng th·ªß v√† di chuy·ªÉn an to√†n
- Wall Usage: 50% chance
- Reward Structure:
  * defensiveWallReward = 35 * intensity
  * movementReward = 20 * intensity
  * aggressiveMovePenalty = -15 * intensity
- Q-table: qtable_defensive.json

üß± WALL MASTER STYLE:
- M·ª•c ti√™u: Chuy√™n gia ƒë·∫∑t t∆∞·ªùng v√† t·∫°o m√™ cung
- Wall Usage: 90% chance
- Reward Structure:
  * wallPlacementReward = 60 * intensity
  * strategicWallReward = 40 * intensity
  * wallComboReward = 50 * intensity
  * movementWithoutWallPenalty = -25 * intensity
- Q-table: qtable_wallmaster.json

‚ö° SPEED RUNNER STYLE:
- M·ª•c ti√™u: ƒê·∫°t ƒë√≠ch nhanh nh·∫•t
- Wall Usage: 10% chance
- Reward Structure:
  * movementReward = 40 * intensity
  * progressToGoalReward = 50 * intensity
  * quickWinBonus = 100 * intensity
  * wallPlacementPenalty = -20 * intensity
- Q-table: qtable_speedrunner.json

üß† TACTICAL STYLE:
- M·ª•c ti√™u: L·ª£i th·∫ø v·ªã tr√≠ v√† k·∫ø ho·∫°ch d√†i h·∫°n
- Wall Usage: Variable (strategic)
- Reward Structure:
  * tacticalMoveReward = 45 * intensity
  * positionAdvantageReward = 35 * intensity
  * mobilityReward = 25 * intensity
  * patternRecognitionBonus = 40 * intensity
- Q-table: qtable_tactical.json

‚öñÔ∏è BALANCED STYLE:
- M·ª•c ti√™u: C√¢n b·∫±ng t·∫•t c·∫£ aspects
- Wall Usage: 30% chance
- Reward Structure: Default balanced values
- Q-table: qtable.json

4.3 STYLE SWITCHING MECHANISM:

```csharp
public void SwitchToStyle(PlayStyle newStyle)
{
    // Save current Q-table
    if (allowQTableSaving) qAgent.SaveQTable(GetStyleSpecificQTablePath());
    
    // Load new style Q-table
    specializedStyle = newStyle;
    qAgent.LoadQTable(GetStyleSpecificQTablePath());
    
    // Reconfigure parameters
    ConfigureStyleSpecificTraining();
}
```

4.4 STYLE INTENSITY:
Parameter t·ª´ 0.0 ‚Üí 1.0 control m·ª©c ƒë·ªô chuy√™n h√≥a:
- 0.0: Balanced behavior
- 0.5: Moderate specialization
- 1.0: Extreme specialization

================================================================================
                        5. DECISION TREE CLASSIFIER
================================================================================

5.1 PURPOSE:
Decision Tree ph√¢n t√≠ch game context ƒë·ªÉ ch·ªçn strategy ph√π h·ª£p, d·ª±a tr√™n:
- Game phase (Early/Mid/End)
- Position advantage
- Resource advantage
- Mobility situation

5.2 FEATURE EXTRACTION:

```csharp
public struct DecisionFeatures
{
    public int aiDistanceToGoal;      // AI distance to goal line
    public int humanDistanceToGoal;   // Human distance to goal line
    public int aiWallsLeft;           // AI walls remaining
    public int humanWallsLeft;        // Human walls remaining
    public int aiMobility;            // AI possible moves
    public int humanMobility;         // Human possible moves
    public float distanceAdvantage;   // Human - AI distance
    public float wallAdvantage;       // AI - Human walls
    public bool isEarlyGame;          // > 15 moves estimated remaining
    public bool isMidGame;            // 5-15 moves remaining
    public bool isEndGame;            // < 5 moves remaining
}
```

5.3 DECISION LOGIC:

```csharp
public AIStrategy Predict(DecisionFeatures features)
{
    // Early Game: Play conservatively
    if (features.isEarlyGame)
        return AIStrategy.Balanced;
    
    // AI has significant lead: Focus on blocking
    if (features.distanceAdvantage > 2)
        return AIStrategy.Blocking;
    
    // AI is behind: Be aggressive
    if (features.distanceAdvantage < -1)
        return AIStrategy.Aggressive;
    
    // End game with wall advantage: Play defensively
    if (features.isEndGame && features.wallAdvantage > 2)
        return AIStrategy.Defensive;
    
    // Close game: Balanced approach
    return AIStrategy.Balanced;
}
```

5.4 STRATEGY OUTCOMES:
- Balanced: Equal weight to movement v√† walls
- Blocking: Prioritize moves that increase opponent path
- Aggressive: Focus on attacking positions
- Defensive: Protect own position and path

================================================================================
                        6. ENTROPY ANALYSIS & SOFTMAX
================================================================================

6.1 ENTROPY CONCEPT:
Entropy ƒëo ƒë·ªô ƒëa d·∫°ng/uncertainty c·ªßa move scores. High entropy = nhi·ªÅu moves
c√≥ scores t∆∞∆°ng ƒë∆∞∆°ng, low entropy = m·ªôt move r√µ r√†ng t·ªët nh·∫•t.

6.2 ENTROPY CALCULATION:

```csharp
float CalculateEntropy(List<float> moveScores)
{
    // Normalize th√†nh probabilities
    float total = moveScores.Sum();
    var probabilities = moveScores.Select(s => s / total);
    
    // Calculate Shannon entropy
    float entropy = 0f;
    foreach (float p in probabilities)
    {
        if (p > 0f)
            entropy -= p * Mathf.Log(p, 2f);
    }
    
    return entropy; // Range: 0 (deterministic) to log2(n) (uniform)
}
```

6.3 SOFTMAX DISTRIBUTION:

```csharp
float[] CalculateSoftmax(List<float> scores, float temperature)
{
    float[] probabilities = new float[scores.Count];
    float sum = scores.Sum(s => Mathf.Exp(s / temperature));
    
    for (int i = 0; i < scores.Count; i++)
    {
        probabilities[i] = Mathf.Exp(scores[i] / temperature) / sum;
    }
    
    return probabilities;
}
```

6.4 TEMPERATURE PARAMETER:
- Low temperature (0.1): Sharp distribution, focus on best moves
- High temperature (2.0): Flat distribution, more exploration
- Default temperature (1.0): Balanced exploitation/exploration

6.5 ADAPTIVE MOVE SELECTION:

```csharp
public AIMove ChooseActionWithEntropy(GameState state, List<AIMove> possibleMoves)
{
    var moveScores = EvaluateAllMoves(state, possibleMoves);
    float entropy = CalculateEntropy(moveScores);
    
    if (entropy > entropyThreshold) // High diversity
    {
        // Use softmax for probabilistic selection
        var probabilities = CalculateSoftmax(moveScores, softmaxTemperature);
        return SelectMoveByProbability(possibleMoves, probabilities);
    }
    else // Clear best move
    {
        // Use deterministic best move
        return GetBestScoredMove(possibleMoves, moveScores);
    }
}
```

================================================================================
                        7. TRAINING SYSTEM & SELF-PLAY
================================================================================

7.1 SELF-PLAY CONCEPT:
AI h·ªçc b·∫±ng c√°ch ch∆°i v·ªõi ch√≠nh n√≥ h√†ng ng√†n l·∫ßn, gradually improve performance
th√¥ng qua trial v√† error.

7.2 TRAINING PIPELINE:

```csharp
IEnumerator RunSelfPlayTraining()
{
    for (int episode = 1; episode <= maxEpisodes; episode++)
    {
        // Reset game state
        ResetGameForTraining();
        
        while (!IsGameOver())
        {
            // Get current state
            GameState currentState = GetCurrentGameState();
            List<AIMove> possibleMoves = GetPossibleMoves();
            
            // Choose action using Œµ-greedy
            AIMove chosenMove = qLearningAgent.ChooseAction(currentState, possibleMoves);
            
            // Execute move
            ExecuteMove(chosenMove);
            
            // Calculate reward
            float reward = CalculateReward(currentState, chosenMove);
            
            // Update Q-table
            GameState newState = GetCurrentGameState();
            qLearningAgent.UpdateQValue(
                EncodeState(currentState),
                EncodeAction(chosenMove),
                reward,
                EncodeState(newState)
            );
            
            yield return new WaitForEndOfFrame();
        }
        
        // Epsilon decay
        qLearningAgent.DecayEpsilon();
        
        // Save progress periodically
        if (episode % 100 == 0)
        {
            qLearningAgent.SaveQTable();
        }
    }
}
```

7.3 REWARD ENGINEERING:
Critical cho Q-Learning success:

```csharp
float CalculateReward(GameState state, AIMove move)
{
    float reward = 0f;
    
    // Game outcome rewards
    if (IsWin(state)) reward += 1000f;
    if (IsLoss(state)) reward -= 1000f;
    
    // Progress rewards
    if (move.moveType == MoveType.Movement)
    {
        if (IsMoveTowardGoal(move)) reward += 20f;
        else reward -= 5f;
    }
    
    // Strategic rewards
    if (move.moveType == MoveType.WallPlacement)
    {
        if (IsWallBlockingOpponent(move)) reward += 50f;
        if (IsWallStrategic(move)) reward += 30f;
        if (IsWallWasted(move)) reward -= 25f;
    }
    
    // Style-specific rewards
    reward += GetStyleSpecificReward(state, move);
    
    return reward;
}
```

7.4 TRAINING METRICS:
- Episodes: 1000 standard
- Duration: ~15 minutes
- Convergence: Usually around episode 800
- Q-table size: 13,118+ states at completion
- Final epsilon: 0.1 (90% exploitation)

================================================================================
                        8. AI TESTING FRAMEWORK
================================================================================

8.1 TESTING PHILOSOPHY:
Comprehensive testing ensures AI quality v√† reliability across different
game scenarios v√† edge cases.

8.2 TEST CATEGORIES:

A) DECISION TREE TESTS:
```csharp
void TestDecisionTreeStrategies()
{
    // Test Early Game
    var earlyGameFeatures = new DecisionFeatures
    {
        aiDistanceToGoal = 8, humanDistanceToGoal = 8,
        isEarlyGame = true
    };
    var strategy = decisionTree.Predict(earlyGameFeatures);
    Assert.AreEqual(AIStrategy.Balanced, strategy);
    
    // Test AI Leading
    var leadingFeatures = new DecisionFeatures
    {
        distanceAdvantage = 3, // AI leading
        isMidGame = true
    };
    strategy = decisionTree.Predict(leadingFeatures);
    Assert.AreEqual(AIStrategy.Blocking, strategy);
}
```

B) ENTROPY ANALYSIS TESTS:
```csharp
void TestEntropyAnalysis()
{
    // High entropy case (diverse scores)
    var highEntropyScores = new List<float> { 100f, 98f, 97f, 95f, 94f };
    float entropy = CalculateTestEntropy(highEntropyScores);
    Assert.IsTrue(entropy > 0.5f);
    
    // Low entropy case (clear winner)
    var lowEntropyScores = new List<float> { 100f, 50f, 50f, 50f, 50f };
    entropy = CalculateTestEntropy(lowEntropyScores);
    Assert.IsTrue(entropy < 0.5f);
}
```

C) SOFTMAX DISTRIBUTION TESTS:
```csharp
void TestSoftmaxDistribution()
{
    var testScores = new List<float> { 100f, 98f, 95f, 90f };
    float temperature = 1.0f;
    
    var probabilities = CalculateSoftmax(testScores, temperature);
    
    // Verify probabilities sum to 1
    float sum = probabilities.Sum();
    Assert.AreEqual(1.0f, sum, 0.001f);
    
    // Verify highest score gets highest probability
    int maxIndex = Array.IndexOf(probabilities, probabilities.Max());
    Assert.AreEqual(0, maxIndex);
}
```

D) PERFORMANCE BENCHMARKS:
- Move generation time: <100ms
- Q-Learning decision: <50ms
- Minimax decision (depth 4): <200ms
- Memory usage: <256MB

8.3 AUTOMATED TESTING:
Tests run automatically ƒë·ªÉ ensure code quality:
- Unit tests cho individual algorithms
- Integration tests cho AI system interactions
- Performance tests cho response times
- Regression tests ƒë·ªÉ prevent quality degradation

================================================================================
                        9. HYBRID AI ARCHITECTURE
================================================================================

9.1 CONCEPT:
Hybrid system k·∫øt h·ª£p strengths c·ªßa multiple algorithms:
- Q-Learning: Data-driven, learns from experience
- Minimax: Logical, guaranteed optimal given evaluation
- Decision Tree: Fast, context-aware switching
- Entropy: Adaptive, handles uncertainty

9.2 ALGORITHM SELECTION LOGIC:

```csharp
public AIMove GetBestMove()
{
    UpdateGameState();
    List<AIMove> possibleMoves = GeneratePossibleMoves();
    
    // Primary: Use Q-Learning if trained model available
    if (useQLearning && qAgent.IsTrainedModel())
    {
        Debug.Log($"üß† Using {specializedStyle} Q-Learning");
        return qAgent.ChooseAction(currentGameState, possibleMoves);
    }
    
    // Fallback: Use Minimax for guaranteed reasonable play
    else
    {
        Debug.Log($"üîç Using {specializedStyle} Minimax");
        return FindBestMoveWithMinimax();
    }
}
```

9.3 SEAMLESS SWITCHING:
- No interruption in gameplay
- Maintains consistent API
- Preserves game state integrity
- Logs algorithm used for debugging

9.4 BENEFITS:
- Robustness: Fallback ensures AI never fails
- Performance: Q-Learning for trained scenarios, Minimax for edge cases
- Flexibility: Can switch algorithms based on situation
- Development: Easy to test individual components

================================================================================
                        10. PERFORMANCE OPTIMIZATION
================================================================================

10.1 Q-TABLE OPTIMIZATION:
- Sparse representation: Only store visited states
- Compression: String interning for state keys
- Lazy loading: Load Q-table on demand
- Periodic saving: Every 100 training episodes

10.2 MINIMAX OPTIMIZATION:
- Alpha-Beta pruning: 50% search reduction
- Move ordering: Promising moves first
- Iterative deepening: Gradual depth increase
- Transposition tables: Cache evaluated positions

10.3 MEMORY MANAGEMENT:
- Object pooling cho GameState objects
- String caching cho repeated state/action encodings
- Garbage collection optimization
- Memory profiling v√† monitoring

10.4 MULTITHREADING CONSIDERATIONS:
- Q-Learning training: Background thread
- Move calculation: Main thread (Unity requirement)
- File I/O: Async operations where possible
- Thread-safe collections cho shared data

10.5 CACHING STRATEGIES:
- Path finding results
- Move generation results
- Evaluation function results
- Feature extraction results

================================================================================
                            K·∫æT LU·∫¨N
================================================================================

D·ª± √°n AI Quoridor Project th·ªÉ hi·ªán m·ªôt implementation comprehensive c·ªßa modern
game AI techniques:

TECHNICAL INNOVATIONS:
‚úÖ Hybrid Q-Learning + Minimax architecture
‚úÖ 6 specialized AI personalities v·ªõi separate training
‚úÖ Advanced decision making v·ªõi entropy analysis
‚úÖ Comprehensive testing v√† validation framework
‚úÖ Professional development tools v√† debugging

EDUCATIONAL VALUE:
‚úÖ Practical machine learning implementation
‚úÖ Traditional game AI algorithms
‚úÖ Modern software engineering practices
‚úÖ Performance optimization techniques
‚úÖ Automated testing methodologies

FUTURE ENHANCEMENTS:
üöÄ Deep Q-Learning (DQN) integration
üöÄ Multi-agent reinforcement learning
üöÄ Neural network-based evaluation functions
üöÄ Online learning v√† adaptation
üöÄ Transfer learning between game variants

D·ª± √°n n√†y demonstrating successful integration c·ªßa classical AI methods v·ªõi
modern machine learning, t·∫°o ra m·ªôt intelligent, adaptive, v√† highly
interactive gaming experience.

================================================================================
                            APPENDIX
================================================================================

FILES LI√äN QUAN:
- Assets/scripts/QLearningAgent.cs - Q-Learning implementation
- Assets/scripts/QuoridorAI.cs - Main AI controller
- Assets/scripts/DecisionTreeClassifier.cs - Decision tree logic
- Assets/scripts/QuoridorSelfPlayTrainer.cs - Training system
- Assets/scripts/AITestScript.cs - Testing framework
- Assets/scripts/Editor/ForceRetrainAI.cs - Development tools

Q-TABLE FILES:
- StreamingAssets/qtable.json - Balanced style
- StreamingAssets/qtable_aggressive.json - Aggressive style
- StreamingAssets/qtable_defensive.json - Defensive style
- StreamingAssets/qtable_wallmaster.json - Wall Master style
- StreamingAssets/qtable_speedrunner.json - Speed Runner style
- StreamingAssets/qtable_tactical.json - Tactical style

DOCUMENTATION:
- AI_ALGORITHMS_DETAILED_DESCRIPTION.txt - This file
- README.md - Project overview
- API_INTEGRATION_GUIDE.md - Voice/Chat integration
- TRAINING_GUIDE.md - AI training instructions

================================================================================
================================================================================
                    AI QUORIDOR GAME PROJECT - THU·∫¨T TO√ÅN AI CHI TI·∫æT
================================================================================

D·ª± √°n: AI Quoridor Game v·ªõi Machine Learning v√† Multi-Platform Integration
Ng√†y t·∫°o: 25/07/2025
T√°c gi·∫£: [Student Name]
Platform: Unity 3D
Language: C#

================================================================================
                            M·ª§C L·ª§C
================================================================================

1. T·ªîNG QUAN KI·∫æN TR√öC AI
2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
4. SPECIALIZED AI STYLES SYSTEM
5. DECISION TREE CLASSIFIER
6. ENTROPY ANALYSIS & SOFTMAX
7. TRAINING SYSTEM & SELF-PLAY
8. AI TESTING FRAMEWORK
9. HYBRID AI ARCHITECTURE
10. PERFORMANCE OPTIMIZATION

================================================================================
                        1. T·ªîNG QUAN KI·∫æN TR√öC AI
================================================================================

D·ª± √°n AI Quoridor s·ª≠ d·ª•ng m·ªôt ki·∫øn tr√∫c AI hybrid ti√™n ti·∫øn, k·∫øt h·ª£p:

1.1 CORE ALGORITHMS:
- Q-Learning (Reinforcement Learning) - Thu·∫≠t to√°n ch√≠nh
- Minimax + Alpha-Beta Pruning - Thu·∫≠t to√°n fallback
- Decision Tree Classifier - Ph√¢n t√≠ch ng·ªØ c·∫£nh game
- Entropy Analysis - ƒê√°nh gi√° ƒëa d·∫°ng move

1.2 SPECIALIZED SYSTEMS:
- 6 AI Personality Styles v·ªõi training ri√™ng bi·ªát
- Self-Play Training Pipeline t·ª± ƒë·ªông
- Real-time Performance Testing Framework
- Dynamic Algorithm Switching

1.3 TECHNICAL FOUNDATION:
- State Space: 13,118+ unique game states
- Action Space: Movement + Wall placement combinations
- Training Method: Self-play v·ªõi 1000 episodes
- Learning Rate: Œ± = 0.1, Discount Factor: Œ≥ = 0.99

================================================================================
                    2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
================================================================================

2.1 GI·ªöI THI·ªÜU Q-LEARNING:
Q-Learning l√† thu·∫≠t to√°n Reinforcement Learning kh√¥ng c·∫ßn model, h·ªçc th√¥ng qua
t∆∞∆°ng t√°c v·ªõi m√¥i tr∆∞·ªùng ƒë·ªÉ t·ªëi ∆∞u h√≥a policy. Trong Quoridor, AI h·ªçc c√°ch
ch·ªçn moves t·ªëi ∆∞u d·ª±a tr√™n kinh nghi·ªám t·ª´ h√†ng ng√†n game.

2.2 C√îNG TH·ª®C TO√ÅN H·ªåC:
Q(s,a) = Q(s,a) + Œ±[r + Œ≥*max(Q(s',a')) - Q(s,a)]

Trong ƒë√≥:
- Q(s,a): Q-value c·ªßa state s v·ªõi action a
- Œ±: Learning rate (0.1) - t·ªëc ƒë·ªô h·ªçc
- r: Immediate reward t·ª´ action
- Œ≥: Discount factor (0.99) - t·∫ßm quan tr·ªçng future rewards
- s': Next state sau khi th·ª±c hi·ªán action
- max(Q(s',a')): Maximum Q-value c·ªßa next state

2.3 IMPLEMENTATION CHI TI·∫æT:

```csharp
public class QLearningAgent
{
    // Core Q-Table: Dictionary l∆∞u tr·ªØ Q-values
    private Dictionary<string, Dictionary<string, float>> qTable;
    
    // Hyperparameters
    private float learningRate = 0.1f;      // T·ªëc ƒë·ªô h·ªçc
    private float discountFactor = 0.99f;   // Discount future rewards
    private float epsilon = 0.995f;         // Exploration rate
    
    // Q-Learning Update Implementation
    public void UpdateQValue(string state, string action, float reward, string nextState)
    {
        // L·∫•y Q-value hi·ªán t·∫°i
        float currentQ = GetQValue(state, action);
        
        // T√¨m max Q-value c·ªßa next state
        float maxNextQ = GetMaxQValue(nextState);
        
        // √Åp d·ª•ng c√¥ng th·ª©c Q-Learning
        float newQ = currentQ + learningRate * (reward + discountFactor * maxNextQ - currentQ);
        
        // C·∫≠p nh·∫≠t Q-table
        SetQValue(state, action, newQ);
    }
}
```

2.4 STATE ENCODING:
Game state ƒë∆∞·ª£c encode th√†nh string duy nh·∫•t:
"aiPos_x,aiPos_y-humanPos_x,humanPos_y-aiWalls-humanWalls-wallsList"

V√≠ d·ª•: "4,8-4,0-9-10-7,4,False|5,6,True"
- AI ·ªü (4,8), Human ·ªü (4,0)
- AI c√≥ 9 walls, Human c√≥ 10 walls
- Walls ƒë√£ ƒë·∫∑t: (7,4,False), (5,6,True)

2.5 ACTION ENCODING:
Actions ƒë∆∞·ª£c encode th√†nh string:
- Movement: "M:x,y" (v√≠ d·ª•: "M:4,7")
- Wall: "W:x,y,horizontal" (v√≠ d·ª•: "W:3,5,True")

2.6 EPSILON-GREEDY STRATEGY:
```csharp
public AIMove ChooseAction(GameState state, List<AIMove> possibleMoves)
{
    // Exploration vs Exploitation trade-off
    if (Random.Range(0f, 1f) < epsilon)
    {
        // EXPLORATION: Ch·ªçn random ƒë·ªÉ kh√°m ph√°
        return possibleMoves[Random.Range(0, possibleMoves.Count)];
    }
    else
    {
        // EXPLOITATION: Ch·ªçn action c√≥ Q-value cao nh·∫•t
        return GetBestAction(state, possibleMoves);
    }
}
```

2.7 EPSILON DECAY:
Epsilon gi·∫£m d·∫ßn t·ª´ 0.995 ‚Üí 0.1 qua training:
- Ban ƒë·∫ßu: High exploration (99.5%)
- Cu·ªëi training: High exploitation (90%)

2.8 REWARD SYSTEM:
- Win game: +1000
- Lose game: -1000
- Move toward goal: +10
- Block opponent: +50
- Waste move: -5
- Strategic position: +15

================================================================================
                3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
================================================================================

3.1 GI·ªöI THI·ªÜU MINIMAX:
Minimax l√† thu·∫≠t to√°n game theory c·ªï ƒëi·ªÉn, t√¨m ki·∫øm move t·ªëi ∆∞u b·∫±ng c√°ch
d·ª± ƒëo√°n tr∆∞·ªõc nhi·ªÅu moves. Alpha-Beta pruning t·ªëi ∆∞u hi·ªáu su·∫•t b·∫±ng c√°ch
lo·∫°i b·ªè c√°c nh√°nh kh√¥ng c·∫ßn thi·∫øt.

3.2 NGUY√äN L√ù HO·∫†T ƒê·ªòNG:
- Maximizing Player (AI): T·ªëi ƒëa h√≥a score
- Minimizing Player (Human): T·ªëi thi·ªÉu h√≥a score AI
- Search depth: 4 levels (configurable)
- Pruning: Lo·∫°i b·ªè nh√°nh kh√¥ng promising

3.3 IMPLEMENTATION:

```csharp
float Minimax(GameState state, int depth, bool isMaximizing, float alpha, float beta)
{
    // Base case: Depth = 0 ho·∫∑c game over
    if (depth == 0 || IsGameOver(state))
        return EvaluatePosition(state);
    
    if (isMaximizing) // AI turn
    {
        float maxEval = float.NegativeInfinity;
        foreach (var move in GeneratePossibleMoves(state, true))
        {
            GameState newState = ApplyMove(state, move, true);
            float eval = Minimax(newState, depth - 1, false, alpha, beta);
            maxEval = Mathf.Max(maxEval, eval);
            alpha = Mathf.Max(alpha, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return maxEval;
    }
    else // Human turn
    {
        float minEval = float.PositiveInfinity;
        foreach (var move in GeneratePossibleMoves(state, false))
        {
            GameState newState = ApplyMove(state, move, false);
            float eval = Minimax(newState, depth - 1, true, alpha, beta);
            minEval = Mathf.Min(minEval, eval);
            beta = Mathf.Min(beta, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return minEval;
    }
}
```

3.4 EVALUATION FUNCTION:
Position evaluation d·ª±a tr√™n multiple factors:

```csharp
float EvaluatePosition(GameState state)
{
    float score = 0f;
    
    // Distance to goal (c√†ng g·∫ßn c√†ng t·ªët)
    int aiDistance = GetShortestPath(state.aiPosition, goalRow);
    int humanDistance = GetShortestPath(state.humanPosition, goalRow);
    score += (humanDistance - aiDistance) * 100;
    
    // Wall advantage
    score += (state.aiWallsLeft - state.humanWallsLeft) * 50;
    
    // Mobility (s·ªë moves c√≥ th·ªÉ)
    int aiMobility = GetValidMoves(state.aiPosition).Count;
    int humanMobility = GetValidMoves(state.humanPosition).Count;
    score += (aiMobility - humanMobility) * 20;
    
    // Center control
    score += GetCenterControlScore(state) * 30;
    
    return score;
}
```

3.5 ALPHA-BETA PRUNING HI·ªÜU QU·∫¢:
- Reduces search space t·ª´ O(b^d) xu·ªëng O(b^(d/2))
- Best case: 50% reduction in nodes evaluated
- Ordering moves by promising gi√∫p pruning hi·ªáu qu·∫£ h∆°n

================================================================================
                        4. SPECIALIZED AI STYLES SYSTEM
================================================================================

4.1 CONCEPT:
6 AI personalities v·ªõi training v√† behavior ri√™ng bi·ªát, m·ªói style c√≥:
- Ri√™ng Q-table file
- Custom reward structure
- Specific move prioritization
- Unique decision making logic

4.2 AI STYLES CHI TI·∫æT:

üî• AGGRESSIVE STYLE:
- M·ª•c ti√™u: T·∫•n c√¥ng v√† ch·∫∑n ƒë·ªëi th·ªß
- Wall Usage: 70% chance
- Reward Structure:
  * blockingOpponentReward = 50 * intensity
  * wallPlacementReward = 30 * intensity
  * defensiveWallPenalty = -20 * intensity
- Q-table: qtable_aggressive.json

üõ°Ô∏è DEFENSIVE STYLE:
- M·ª•c ti√™u: Ph√≤ng th·ªß v√† di chuy·ªÉn an to√†n
- Wall Usage: 50% chance
- Reward Structure:
  * defensiveWallReward = 35 * intensity
  * movementReward = 20 * intensity
  * aggressiveMovePenalty = -15 * intensity
- Q-table: qtable_defensive.json

üß± WALL MASTER STYLE:
- M·ª•c ti√™u: Chuy√™n gia ƒë·∫∑t t∆∞·ªùng v√† t·∫°o m√™ cung
- Wall Usage: 90% chance
- Reward Structure:
  * wallPlacementReward = 60 * intensity
  * strategicWallReward = 40 * intensity
  * wallComboReward = 50 * intensity
  * movementWithoutWallPenalty = -25 * intensity
- Q-table: qtable_wallmaster.json

‚ö° SPEED RUNNER STYLE:
- M·ª•c ti√™u: ƒê·∫°t ƒë√≠ch nhanh nh·∫•t
- Wall Usage: 10% chance
- Reward Structure:
  * movementReward = 40 * intensity
  * progressToGoalReward = 50 * intensity
  * quickWinBonus = 100 * intensity
  * wallPlacementPenalty = -20 * intensity
- Q-table: qtable_speedrunner.json

üß† TACTICAL STYLE:
- M·ª•c ti√™u: L·ª£i th·∫ø v·ªã tr√≠ v√† k·∫ø ho·∫°ch d√†i h·∫°n
- Wall Usage: Variable (strategic)
- Reward Structure:
  * tacticalMoveReward = 45 * intensity
  * positionAdvantageReward = 35 * intensity
  * mobilityReward = 25 * intensity
  * patternRecognitionBonus = 40 * intensity
- Q-table: qtable_tactical.json

‚öñÔ∏è BALANCED STYLE:
- M·ª•c ti√™u: C√¢n b·∫±ng t·∫•t c·∫£ aspects
- Wall Usage: 30% chance
- Reward Structure: Default balanced values
- Q-table: qtable.json

4.3 STYLE SWITCHING MECHANISM:

```csharp
public void SwitchToStyle(PlayStyle newStyle)
{
    // Save current Q-table
    if (allowQTableSaving) qAgent.SaveQTable(GetStyleSpecificQTablePath());
    
    // Load new style Q-table
    specializedStyle = newStyle;
    qAgent.LoadQTable(GetStyleSpecificQTablePath());
    
    // Reconfigure parameters
    ConfigureStyleSpecificTraining();
}
```

4.4 STYLE INTENSITY:
Parameter t·ª´ 0.0 ‚Üí 1.0 control m·ª©c ƒë·ªô chuy√™n h√≥a:
- 0.0: Balanced behavior
- 0.5: Moderate specialization
- 1.0: Extreme specialization

================================================================================
                        5. DECISION TREE CLASSIFIER
================================================================================

5.1 PURPOSE:
Decision Tree ph√¢n t√≠ch game context ƒë·ªÉ ch·ªçn strategy ph√π h·ª£p, d·ª±a tr√™n:
- Game phase (Early/Mid/End)
- Position advantage
- Resource advantage
- Mobility situation

5.2 FEATURE EXTRACTION:

```csharp
public struct DecisionFeatures
{
    public int aiDistanceToGoal;      // AI distance to goal line
    public int humanDistanceToGoal;   // Human distance to goal line
    public int aiWallsLeft;           // AI walls remaining
    public int humanWallsLeft;        // Human walls remaining
    public int aiMobility;            // AI possible moves
    public int humanMobility;         // Human possible moves
    public float distanceAdvantage;   // Human - AI distance
    public float wallAdvantage;       // AI - Human walls
    public bool isEarlyGame;          // > 15 moves estimated remaining
    public bool isMidGame;            // 5-15 moves remaining
    public bool isEndGame;            // < 5 moves remaining
}
```

5.3 DECISION LOGIC:

```csharp
public AIStrategy Predict(DecisionFeatures features)
{
    // Early Game: Play conservatively
    if (features.isEarlyGame)
        return AIStrategy.Balanced;
    
    // AI has significant lead: Focus on blocking
    if (features.distanceAdvantage > 2)
        return AIStrategy.Blocking;
    
    // AI is behind: Be aggressive
    if (features.distanceAdvantage < -1)
        return AIStrategy.Aggressive;
    
    // End game with wall advantage: Play defensively
    if (features.isEndGame && features.wallAdvantage > 2)
        return AIStrategy.Defensive;
    
    // Close game: Balanced approach
    return AIStrategy.Balanced;
}
```

5.4 STRATEGY OUTCOMES:
- Balanced: Equal weight to movement v√† walls
- Blocking: Prioritize moves that increase opponent path
- Aggressive: Focus on attacking positions
- Defensive: Protect own position and path

================================================================================
                        6. ENTROPY ANALYSIS & SOFTMAX
================================================================================

6.1 ENTROPY CONCEPT:
Entropy ƒëo ƒë·ªô ƒëa d·∫°ng/uncertainty c·ªßa move scores. High entropy = nhi·ªÅu moves
c√≥ scores t∆∞∆°ng ƒë∆∞∆°ng, low entropy = m·ªôt move r√µ r√†ng t·ªët nh·∫•t.

6.2 ENTROPY CALCULATION:

```csharp
float CalculateEntropy(List<float> moveScores)
{
    // Normalize th√†nh probabilities
    float total = moveScores.Sum();
    var probabilities = moveScores.Select(s => s / total);
    
    // Calculate Shannon entropy
    float entropy = 0f;
    foreach (float p in probabilities)
    {
        if (p > 0f)
            entropy -= p * Mathf.Log(p, 2f);
    }
    
    return entropy; // Range: 0 (deterministic) to log2(n) (uniform)
}
```

6.3 SOFTMAX DISTRIBUTION:

```csharp
float[] CalculateSoftmax(List<float> scores, float temperature)
{
    float[] probabilities = new float[scores.Count];
    float sum = scores.Sum(s => Mathf.Exp(s / temperature));
    
    for (int i = 0; i < scores.Count; i++)
    {
        probabilities[i] = Mathf.Exp(scores[i] / temperature) / sum;
    }
    
    return probabilities;
}
```

6.4 TEMPERATURE PARAMETER:
- Low temperature (0.1): Sharp distribution, focus on best moves
- High temperature (2.0): Flat distribution, more exploration
- Default temperature (1.0): Balanced exploitation/exploration

6.5 ADAPTIVE MOVE SELECTION:

```csharp
public AIMove ChooseActionWithEntropy(GameState state, List<AIMove> possibleMoves)
{
    var moveScores = EvaluateAllMoves(state, possibleMoves);
    float entropy = CalculateEntropy(moveScores);
    
    if (entropy > entropyThreshold) // High diversity
    {
        // Use softmax for probabilistic selection
        var probabilities = CalculateSoftmax(moveScores, softmaxTemperature);
        return SelectMoveByProbability(possibleMoves, probabilities);
    }
    else // Clear best move
    {
        // Use deterministic best move
        return GetBestScoredMove(possibleMoves, moveScores);
    }
}
```

================================================================================
                        7. TRAINING SYSTEM & SELF-PLAY
================================================================================

7.1 SELF-PLAY CONCEPT:
AI h·ªçc b·∫±ng c√°ch ch∆°i v·ªõi ch√≠nh n√≥ h√†ng ng√†n l·∫ßn, gradually improve performance
th√¥ng qua trial v√† error.

7.2 TRAINING PIPELINE:

```csharp
IEnumerator RunSelfPlayTraining()
{
    for (int episode = 1; episode <= maxEpisodes; episode++)
    {
        // Reset game state
        ResetGameForTraining();
        
        while (!IsGameOver())
        {
            // Get current state
            GameState currentState = GetCurrentGameState();
            List<AIMove> possibleMoves = GetPossibleMoves();
            
            // Choose action using Œµ-greedy
            AIMove chosenMove = qLearningAgent.ChooseAction(currentState, possibleMoves);
            
            // Execute move
            ExecuteMove(chosenMove);
            
            // Calculate reward
            float reward = CalculateReward(currentState, chosenMove);
            
            // Update Q-table
            GameState newState = GetCurrentGameState();
            qLearningAgent.UpdateQValue(
                EncodeState(currentState),
                EncodeAction(chosenMove),
                reward,
                EncodeState(newState)
            );
            
            yield return new WaitForEndOfFrame();
        }
        
        // Epsilon decay
        qLearningAgent.DecayEpsilon();
        
        // Save progress periodically
        if (episode % 100 == 0)
        {
            qLearningAgent.SaveQTable();
        }
    }
}
```

7.3 REWARD ENGINEERING:
Critical cho Q-Learning success:

```csharp
float CalculateReward(GameState state, AIMove move)
{
    float reward = 0f;
    
    // Game outcome rewards
    if (IsWin(state)) reward += 1000f;
    if (IsLoss(state)) reward -= 1000f;
    
    // Progress rewards
    if (move.moveType == MoveType.Movement)
    {
        if (IsMoveTowardGoal(move)) reward += 20f;
        else reward -= 5f;
    }
    
    // Strategic rewards
    if (move.moveType == MoveType.WallPlacement)
    {
        if (IsWallBlockingOpponent(move)) reward += 50f;
        if (IsWallStrategic(move)) reward += 30f;
        if (IsWallWasted(move)) reward -= 25f;
    }
    
    // Style-specific rewards
    reward += GetStyleSpecificReward(state, move);
    
    return reward;
}
```

7.4 TRAINING METRICS:
- Episodes: 1000 standard
- Duration: ~15 minutes
- Convergence: Usually around episode 800
- Q-table size: 13,118+ states at completion
- Final epsilon: 0.1 (90% exploitation)

================================================================================
                        8. AI TESTING FRAMEWORK
================================================================================

8.1 TESTING PHILOSOPHY:
Comprehensive testing ensures AI quality v√† reliability across different
game scenarios v√† edge cases.

8.2 TEST CATEGORIES:

A) DECISION TREE TESTS:
```csharp
void TestDecisionTreeStrategies()
{
    // Test Early Game
    var earlyGameFeatures = new DecisionFeatures
    {
        aiDistanceToGoal = 8, humanDistanceToGoal = 8,
        isEarlyGame = true
    };
    var strategy = decisionTree.Predict(earlyGameFeatures);
    Assert.AreEqual(AIStrategy.Balanced, strategy);
    
    // Test AI Leading
    var leadingFeatures = new DecisionFeatures
    {
        distanceAdvantage = 3, // AI leading
        isMidGame = true
    };
    strategy = decisionTree.Predict(leadingFeatures);
    Assert.AreEqual(AIStrategy.Blocking, strategy);
}
```

B) ENTROPY ANALYSIS TESTS:
```csharp
void TestEntropyAnalysis()
{
    // High entropy case (diverse scores)
    var highEntropyScores = new List<float> { 100f, 98f, 97f, 95f, 94f };
    float entropy = CalculateTestEntropy(highEntropyScores);
    Assert.IsTrue(entropy > 0.5f);
    
    // Low entropy case (clear winner)
    var lowEntropyScores = new List<float> { 100f, 50f, 50f, 50f, 50f };
    entropy = CalculateTestEntropy(lowEntropyScores);
    Assert.IsTrue(entropy < 0.5f);
}
```

C) SOFTMAX DISTRIBUTION TESTS:
```csharp
void TestSoftmaxDistribution()
{
    var testScores = new List<float> { 100f, 98f, 95f, 90f };
    float temperature = 1.0f;
    
    var probabilities = CalculateSoftmax(testScores, temperature);
    
    // Verify probabilities sum to 1
    float sum = probabilities.Sum();
    Assert.AreEqual(1.0f, sum, 0.001f);
    
    // Verify highest score gets highest probability
    int maxIndex = Array.IndexOf(probabilities, probabilities.Max());
    Assert.AreEqual(0, maxIndex);
}
```

D) PERFORMANCE BENCHMARKS:
- Move generation time: <100ms
- Q-Learning decision: <50ms
- Minimax decision (depth 4): <200ms
- Memory usage: <256MB

8.3 AUTOMATED TESTING:
Tests run automatically ƒë·ªÉ ensure code quality:
- Unit tests cho individual algorithms
- Integration tests cho AI system interactions
- Performance tests cho response times
- Regression tests ƒë·ªÉ prevent quality degradation

================================================================================
                        9. HYBRID AI ARCHITECTURE
================================================================================

9.1 CONCEPT:
Hybrid system k·∫øt h·ª£p strengths c·ªßa multiple algorithms:
- Q-Learning: Data-driven, learns from experience
- Minimax: Logical, guaranteed optimal given evaluation
- Decision Tree: Fast, context-aware switching
- Entropy: Adaptive, handles uncertainty

9.2 ALGORITHM SELECTION LOGIC:

```csharp
public AIMove GetBestMove()
{
    UpdateGameState();
    List<AIMove> possibleMoves = GeneratePossibleMoves();
    
    // Primary: Use Q-Learning if trained model available
    if (useQLearning && qAgent.IsTrainedModel())
    {
        Debug.Log($"üß† Using {specializedStyle} Q-Learning");
        return qAgent.ChooseAction(currentGameState, possibleMoves);
    }
    
    // Fallback: Use Minimax for guaranteed reasonable play
    else
    {
        Debug.Log($"üîç Using {specializedStyle} Minimax");
        return FindBestMoveWithMinimax();
    }
}
```

9.3 SEAMLESS SWITCHING:
- No interruption in gameplay
- Maintains consistent API
- Preserves game state integrity
- Logs algorithm used for debugging

9.4 BENEFITS:
- Robustness: Fallback ensures AI never fails
- Performance: Q-Learning for trained scenarios, Minimax for edge cases
- Flexibility: Can switch algorithms based on situation
- Development: Easy to test individual components

================================================================================
                        10. PERFORMANCE OPTIMIZATION
================================================================================

10.1 Q-TABLE OPTIMIZATION:
- Sparse representation: Only store visited states
- Compression: String interning for state keys
- Lazy loading: Load Q-table on demand
- Periodic saving: Every 100 training episodes

10.2 MINIMAX OPTIMIZATION:
- Alpha-Beta pruning: 50% search reduction
- Move ordering: Promising moves first
- Iterative deepening: Gradual depth increase
- Transposition tables: Cache evaluated positions

10.3 MEMORY MANAGEMENT:
- Object pooling cho GameState objects
- String caching cho repeated state/action encodings
- Garbage collection optimization
- Memory profiling v√† monitoring

10.4 MULTITHREADING CONSIDERATIONS:
- Q-Learning training: Background thread
- Move calculation: Main thread (Unity requirement)
- File I/O: Async operations where possible
- Thread-safe collections cho shared data

10.5 CACHING STRATEGIES:
- Path finding results
- Move generation results
- Evaluation function results
- Feature extraction results

================================================================================
                            K·∫æT LU·∫¨N
================================================================================

D·ª± √°n AI Quoridor Project th·ªÉ hi·ªán m·ªôt implementation comprehensive c·ªßa modern
game AI techniques:

TECHNICAL INNOVATIONS:
‚úÖ Hybrid Q-Learning + Minimax architecture
‚úÖ 6 specialized AI personalities v·ªõi separate training
‚úÖ Advanced decision making v·ªõi entropy analysis
‚úÖ Comprehensive testing v√† validation framework
‚úÖ Professional development tools v√† debugging

EDUCATIONAL VALUE:
‚úÖ Practical machine learning implementation
‚úÖ Traditional game AI algorithms
‚úÖ Modern software engineering practices
‚úÖ Performance optimization techniques
‚úÖ Automated testing methodologies

FUTURE ENHANCEMENTS:
üöÄ Deep Q-Learning (DQN) integration
üöÄ Multi-agent reinforcement learning
üöÄ Neural network-based evaluation functions
üöÄ Online learning v√† adaptation
üöÄ Transfer learning between game variants

D·ª± √°n n√†y demonstrating successful integration c·ªßa classical AI methods v·ªõi
modern machine learning, t·∫°o ra m·ªôt intelligent, adaptive, v√† highly
interactive gaming experience.

================================================================================
                            APPENDIX
================================================================================

FILES LI√äN QUAN:
- Assets/scripts/QLearningAgent.cs - Q-Learning implementation
- Assets/scripts/QuoridorAI.cs - Main AI controller
- Assets/scripts/DecisionTreeClassifier.cs - Decision tree logic
- Assets/scripts/QuoridorSelfPlayTrainer.cs - Training system
- Assets/scripts/AITestScript.cs - Testing framework
- Assets/scripts/Editor/ForceRetrainAI.cs - Development tools

Q-TABLE FILES:
- StreamingAssets/qtable.json - Balanced style
- StreamingAssets/qtable_aggressive.json - Aggressive style
- StreamingAssets/qtable_defensive.json - Defensive style
- StreamingAssets/qtable_wallmaster.json - Wall Master style
- StreamingAssets/qtable_speedrunner.json - Speed Runner style
- StreamingAssets/qtable_tactical.json - Tactical style

DOCUMENTATION:
- AI_ALGORITHMS_DETAILED_DESCRIPTION.txt - This file
- README.md - Project overview
- API_INTEGRATION_GUIDE.md - Voice/Chat integration
- TRAINING_GUIDE.md - AI training instructions

================================================================================
================================================================================
                    AI QUORIDOR GAME PROJECT - THU·∫¨T TO√ÅN AI CHI TI·∫æT
================================================================================

D·ª± √°n: AI Quoridor Game v·ªõi Machine Learning v√† Multi-Platform Integration
Ng√†y t·∫°o: 25/07/2025
T√°c gi·∫£: [Student Name]
Platform: Unity 3D
Language: C#

================================================================================
                            M·ª§C L·ª§C
================================================================================

1. T·ªîNG QUAN KI·∫æN TR√öC AI
2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
4. SPECIALIZED AI STYLES SYSTEM
5. DECISION TREE CLASSIFIER
6. ENTROPY ANALYSIS & SOFTMAX
7. TRAINING SYSTEM & SELF-PLAY
8. AI TESTING FRAMEWORK
9. HYBRID AI ARCHITECTURE
10. PERFORMANCE OPTIMIZATION

================================================================================
                        1. T·ªîNG QUAN KI·∫æN TR√öC AI
================================================================================

D·ª± √°n AI Quoridor s·ª≠ d·ª•ng m·ªôt ki·∫øn tr√∫c AI hybrid ti√™n ti·∫øn, k·∫øt h·ª£p:

1.1 CORE ALGORITHMS:
- Q-Learning (Reinforcement Learning) - Thu·∫≠t to√°n ch√≠nh
- Minimax + Alpha-Beta Pruning - Thu·∫≠t to√°n fallback
- Decision Tree Classifier - Ph√¢n t√≠ch ng·ªØ c·∫£nh game
- Entropy Analysis - ƒê√°nh gi√° ƒëa d·∫°ng move

1.2 SPECIALIZED SYSTEMS:
- 6 AI Personality Styles v·ªõi training ri√™ng bi·ªát
- Self-Play Training Pipeline t·ª± ƒë·ªông
- Real-time Performance Testing Framework
- Dynamic Algorithm Switching

1.3 TECHNICAL FOUNDATION:
- State Space: 13,118+ unique game states
- Action Space: Movement + Wall placement combinations
- Training Method: Self-play v·ªõi 1000 episodes
- Learning Rate: Œ± = 0.1, Discount Factor: Œ≥ = 0.99

================================================================================
                    2. Q-LEARNING ALGORITHM (THU·∫¨T TO√ÅN CH√çNH)
================================================================================

2.1 GI·ªöI THI·ªÜU Q-LEARNING:
Q-Learning l√† thu·∫≠t to√°n Reinforcement Learning kh√¥ng c·∫ßn model, h·ªçc th√¥ng qua
t∆∞∆°ng t√°c v·ªõi m√¥i tr∆∞·ªùng ƒë·ªÉ t·ªëi ∆∞u h√≥a policy. Trong Quoridor, AI h·ªçc c√°ch
ch·ªçn moves t·ªëi ∆∞u d·ª±a tr√™n kinh nghi·ªám t·ª´ h√†ng ng√†n game.

2.2 C√îNG TH·ª®C TO√ÅN H·ªåC:
Q(s,a) = Q(s,a) + Œ±[r + Œ≥*max(Q(s',a')) - Q(s,a)]

Trong ƒë√≥:
- Q(s,a): Q-value c·ªßa state s v·ªõi action a
- Œ±: Learning rate (0.1) - t·ªëc ƒë·ªô h·ªçc
- r: Immediate reward t·ª´ action
- Œ≥: Discount factor (0.99) - t·∫ßm quan tr·ªçng future rewards
- s': Next state sau khi th·ª±c hi·ªán action
- max(Q(s',a')): Maximum Q-value c·ªßa next state

2.3 IMPLEMENTATION CHI TI·∫æT:

```csharp
public class QLearningAgent
{
    // Core Q-Table: Dictionary l∆∞u tr·ªØ Q-values
    private Dictionary<string, Dictionary<string, float>> qTable;
    
    // Hyperparameters
    private float learningRate = 0.1f;      // T·ªëc ƒë·ªô h·ªçc
    private float discountFactor = 0.99f;   // Discount future rewards
    private float epsilon = 0.995f;         // Exploration rate
    
    // Q-Learning Update Implementation
    public void UpdateQValue(string state, string action, float reward, string nextState)
    {
        // L·∫•y Q-value hi·ªán t·∫°i
        float currentQ = GetQValue(state, action);
        
        // T√¨m max Q-value c·ªßa next state
        float maxNextQ = GetMaxQValue(nextState);
        
        // √Åp d·ª•ng c√¥ng th·ª©c Q-Learning
        float newQ = currentQ + learningRate * (reward + discountFactor * maxNextQ - currentQ);
        
        // C·∫≠p nh·∫≠t Q-table
        SetQValue(state, action, newQ);
    }
}
```

2.4 STATE ENCODING:
Game state ƒë∆∞·ª£c encode th√†nh string duy nh·∫•t:
"aiPos_x,aiPos_y-humanPos_x,humanPos_y-aiWalls-humanWalls-wallsList"

V√≠ d·ª•: "4,8-4,0-9-10-7,4,False|5,6,True"
- AI ·ªü (4,8), Human ·ªü (4,0)
- AI c√≥ 9 walls, Human c√≥ 10 walls
- Walls ƒë√£ ƒë·∫∑t: (7,4,False), (5,6,True)

2.5 ACTION ENCODING:
Actions ƒë∆∞·ª£c encode th√†nh string:
- Movement: "M:x,y" (v√≠ d·ª•: "M:4,7")
- Wall: "W:x,y,horizontal" (v√≠ d·ª•: "W:3,5,True")

2.6 EPSILON-GREEDY STRATEGY:
```csharp
public AIMove ChooseAction(GameState state, List<AIMove> possibleMoves)
{
    // Exploration vs Exploitation trade-off
    if (Random.Range(0f, 1f) < epsilon)
    {
        // EXPLORATION: Ch·ªçn random ƒë·ªÉ kh√°m ph√°
        return possibleMoves[Random.Range(0, possibleMoves.Count)];
    }
    else
    {
        // EXPLOITATION: Ch·ªçn action c√≥ Q-value cao nh·∫•t
        return GetBestAction(state, possibleMoves);
    }
}
```

2.7 EPSILON DECAY:
Epsilon gi·∫£m d·∫ßn t·ª´ 0.995 ‚Üí 0.1 qua training:
- Ban ƒë·∫ßu: High exploration (99.5%)
- Cu·ªëi training: High exploitation (90%)

2.8 REWARD SYSTEM:
- Win game: +1000
- Lose game: -1000
- Move toward goal: +10
- Block opponent: +50
- Waste move: -5
- Strategic position: +15

================================================================================
                3. MINIMAX + ALPHA-BETA PRUNING (THU·∫¨T TO√ÅN PH·ª§)
================================================================================

3.1 GI·ªöI THI·ªÜU MINIMAX:
Minimax l√† thu·∫≠t to√°n game theory c·ªï ƒëi·ªÉn, t√¨m ki·∫øm move t·ªëi ∆∞u b·∫±ng c√°ch
d·ª± ƒëo√°n tr∆∞·ªõc nhi·ªÅu moves. Alpha-Beta pruning t·ªëi ∆∞u hi·ªáu su·∫•t b·∫±ng c√°ch
lo·∫°i b·ªè c√°c nh√°nh kh√¥ng c·∫ßn thi·∫øt.

3.2 NGUY√äN L√ù HO·∫†T ƒê·ªòNG:
- Maximizing Player (AI): T·ªëi ƒëa h√≥a score
- Minimizing Player (Human): T·ªëi thi·ªÉu h√≥a score AI
- Search depth: 4 levels (configurable)
- Pruning: Lo·∫°i b·ªè nh√°nh kh√¥ng promising

3.3 IMPLEMENTATION:

```csharp
float Minimax(GameState state, int depth, bool isMaximizing, float alpha, float beta)
{
    // Base case: Depth = 0 ho·∫∑c game over
    if (depth == 0 || IsGameOver(state))
        return EvaluatePosition(state);
    
    if (isMaximizing) // AI turn
    {
        float maxEval = float.NegativeInfinity;
        foreach (var move in GeneratePossibleMoves(state, true))
        {
            GameState newState = ApplyMove(state, move, true);
            float eval = Minimax(newState, depth - 1, false, alpha, beta);
            maxEval = Mathf.Max(maxEval, eval);
            alpha = Mathf.Max(alpha, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return maxEval;
    }
    else // Human turn
    {
        float minEval = float.PositiveInfinity;
        foreach (var move in GeneratePossibleMoves(state, false))
        {
            GameState newState = ApplyMove(state, move, false);
            float eval = Minimax(newState, depth - 1, true, alpha, beta);
            minEval = Mathf.Min(minEval, eval);
            beta = Mathf.Min(beta, eval);
            
            // Alpha-Beta Pruning
            if (beta <= alpha) break;
        }
        return minEval;
    }
}
```

3.4 EVALUATION FUNCTION:
Position evaluation d·ª±a tr√™n multiple factors:

```csharp
float EvaluatePosition(GameState state)
{
    float score = 0f;
    
    // Distance to goal (c√†ng g·∫ßn c√†ng t·ªët)
    int aiDistance = GetShortestPath(state.aiPosition, goalRow);
    int humanDistance = GetShortestPath(state.humanPosition, goalRow);
    score += (humanDistance - aiDistance) * 100;
    
    // Wall advantage
    score += (state.aiWallsLeft - state.humanWallsLeft) * 50;
    
    // Mobility (s·ªë moves c√≥ th·ªÉ)
    int aiMobility = GetValidMoves(state.aiPosition).Count;
    int humanMobility = GetValidMoves(state.humanPosition).Count;
    score += (aiMobility - humanMobility) * 20;
    
    // Center control
    score += GetCenterControlScore(state) * 30;
    
    return score;
}
```

3.5 ALPHA-BETA PRUNING HI·ªÜU QU·∫¢:
- Reduces search space t·ª´ O(b^d) xu·ªëng O(b^(d/2))
- Best case: 50% reduction in nodes evaluated
- Ordering moves by promising gi√∫p pruning hi·ªáu qu·∫£ h∆°n

================================================================================
                        4. SPECIALIZED AI STYLES SYSTEM
================================================================================

4.1 CONCEPT:
6 AI personalities v·ªõi training v√† behavior ri√™ng bi·ªát, m·ªói style c√≥:
- Ri√™ng Q-table file
- Custom reward structure
- Specific move prioritization
- Unique decision making logic

4.2 AI STYLES CHI TI·∫æT:

üî• AGGRESSIVE STYLE:
- M·ª•c ti√™u: T·∫•n c√¥ng v√† ch·∫∑n ƒë·ªëi th·ªß
- Wall Usage: 70% chance
- Reward Structure:
  * blockingOpponentReward = 50 * intensity
  * wallPlacementReward = 30 * intensity
  * defensiveWallPenalty = -20 * intensity
- Q-table: qtable_aggressive.json

üõ°Ô∏è DEFENSIVE STYLE:
- M·ª•c ti√™u: Ph√≤ng th·ªß v√† di chuy·ªÉn an to√†n
- Wall Usage: 50% chance
- Reward Structure:
  * defensiveWallReward = 35 * intensity
  * movementReward = 20 * intensity
  * aggressiveMovePenalty = -15 * intensity
- Q-table: qtable_defensive.json

üß± WALL MASTER STYLE:
- M·ª•c ti√™u: Chuy√™n gia ƒë·∫∑t t∆∞·ªùng v√† t·∫°o m√™ cung
- Wall Usage: 90% chance
- Reward Structure:
  * wallPlacementReward = 60 * intensity
  * strategicWallReward = 40 * intensity
  * wallComboReward = 50 * intensity
  * movementWithoutWallPenalty = -25 * intensity
- Q-table: qtable_wallmaster.json

‚ö° SPEED RUNNER STYLE:
- M·ª•c ti√™u: ƒê·∫°t ƒë√≠ch nhanh nh·∫•t
- Wall Usage: 10% chance
- Reward Structure:
  * movementReward = 40 * intensity
  * progressToGoalReward = 50 * intensity
  * quickWinBonus = 100 * intensity
  * wallPlacementPenalty = -20 * intensity
- Q-table: qtable_speedrunner.json

üß† TACTICAL STYLE:
- M·ª•c ti√™u: L·ª£i th·∫ø v·ªã tr√≠ v√† k·∫ø ho·∫°ch d√†i h·∫°n
- Wall Usage: Variable (strategic)
- Reward Structure:
  * tacticalMoveReward = 45 * intensity
  * positionAdvantageReward = 35 * intensity
  * mobilityReward = 25 * intensity
  * patternRecognitionBonus = 40 * intensity
- Q-table: qtable_tactical.json

‚öñÔ∏è BALANCED STYLE:
- M·ª•c ti√™u: C√¢n b·∫±ng t·∫•t c·∫£ aspects
- Wall Usage: 30% chance
- Reward Structure: Default balanced values
- Q-table: qtable.json

4.3 STYLE SWITCHING MECHANISM:

```csharp
public void SwitchToStyle(PlayStyle newStyle)
{
    // Save current Q-table
    if (allowQTableSaving) qAgent.SaveQTable(GetStyleSpecificQTablePath());
    
    // Load new style Q-table
    specializedStyle = newStyle;
    qAgent.LoadQTable(GetStyleSpecificQTablePath());
    
    // Reconfigure parameters
    ConfigureStyleSpecificTraining();
}
```

4.4 STYLE INTENSITY:
Parameter t·ª´ 0.0 ‚Üí 1.0 control m·ª©c ƒë·ªô chuy√™n h√≥a:
- 0.0: Balanced behavior
- 0.5: Moderate specialization
- 1.0: Extreme specialization

================================================================================
                        5. DECISION TREE CLASSIFIER
================================================================================

5.1 PURPOSE:
Decision Tree ph√¢n t√≠ch game context ƒë·ªÉ ch·ªçn strategy ph√π h·ª£p, d·ª±a tr√™n:
- Game phase (Early/Mid/End)
- Position advantage
- Resource advantage
- Mobility situation

5.2 FEATURE EXTRACTION:

```csharp
public struct DecisionFeatures
{
    public int aiDistanceToGoal;      // AI distance to goal line
    public int humanDistanceToGoal;   // Human distance to goal line
    public int aiWallsLeft;           // AI walls remaining
    public int humanWallsLeft;        // Human walls remaining
    public int aiMobility;            // AI possible moves
    public int humanMobility;         // Human possible moves
    public float distanceAdvantage;   // Human - AI distance
    public float wallAdvantage;       // AI - Human walls
    public bool isEarlyGame;          // > 15 moves estimated remaining
    public bool isMidGame;            // 5-15 moves remaining
    public bool isEndGame;            // < 5 moves remaining
}
```

5.3 DECISION LOGIC:

```csharp
public AIStrategy Predict(DecisionFeatures features)
{
    // Early Game: Play conservatively
    if (features.isEarlyGame)
        return AIStrategy.Balanced;
    
    // AI has significant lead: Focus on blocking
    if (features.distanceAdvantage > 2)
        return AIStrategy.Blocking;
    
    // AI is behind: Be aggressive
    if (features.distanceAdvantage < -1)
        return AIStrategy.Aggressive;
    
    // End game with wall advantage: Play defensively
    if (features.isEndGame && features.wallAdvantage > 2)
        return AIStrategy.Defensive;
    
    // Close game: Balanced approach
    return AIStrategy.Balanced;
}
```

5.4 STRATEGY OUTCOMES:
- Balanced: Equal weight to movement v√† walls
- Blocking: Prioritize moves that increase opponent path
- Aggressive: Focus on attacking positions
- Defensive: Protect own position and path

================================================================================
                        6. ENTROPY ANALYSIS & SOFTMAX
================================================================================

6.1 ENTROPY CONCEPT:
Entropy ƒëo ƒë·ªô ƒëa d·∫°ng/uncertainty c·ªßa move scores. High entropy = nhi·ªÅu moves
c√≥ scores t∆∞∆°ng ƒë∆∞∆°ng, low entropy = m·ªôt move r√µ r√†ng t·ªët nh·∫•t.

6.2 ENTROPY CALCULATION:

```csharp
float CalculateEntropy(List<float> moveScores)
{
    // Normalize th√†nh probabilities
    float total = moveScores.Sum();
    var probabilities = moveScores.Select(s => s / total);
    
    // Calculate Shannon entropy
    float entropy = 0f;
    foreach (float p in probabilities)
    {
        if (p > 0f)
            entropy -= p * Mathf.Log(p, 2f);
    }
    
    return entropy; // Range: 0 (deterministic) to log2(n) (uniform)
}
```

6.3 SOFTMAX DISTRIBUTION:

```csharp
float[] CalculateSoftmax(List<float> scores, float temperature)
{
    float[] probabilities = new float[scores.Count];
    float sum = scores.Sum(s => Mathf.Exp(s / temperature));
    
    for (int i = 0; i < scores.Count; i++)
    {
        probabilities[i] = Mathf.Exp(scores[i] / temperature) / sum;
    }
    
    return probabilities;
}
```

6.4 TEMPERATURE PARAMETER:
- Low temperature (0.1): Sharp distribution, focus on best moves
- High temperature (2.0): Flat distribution, more exploration
- Default temperature (1.0): Balanced exploitation/exploration

6.5 ADAPTIVE MOVE SELECTION:

```csharp
public AIMove ChooseActionWithEntropy(GameState state, List<AIMove> possibleMoves)
{
    var moveScores = EvaluateAllMoves(state, possibleMoves);
    float entropy = CalculateEntropy(moveScores);
    
    if (entropy > entropyThreshold) // High diversity
    {
        // Use softmax for probabilistic selection
        var probabilities = CalculateSoftmax(moveScores, softmaxTemperature);
        return SelectMoveByProbability(possibleMoves, probabilities);
    }
    else // Clear best move
    {
        // Use deterministic best move
        return GetBestScoredMove(possibleMoves, moveScores);
    }
}
```

================================================================================
                        7. TRAINING SYSTEM & SELF-PLAY
================================================================================

7.1 SELF-PLAY CONCEPT:
AI h·ªçc b·∫±ng c√°ch ch∆°i v·ªõi ch√≠nh n√≥ h√†ng ng√†n l·∫ßn, gradually improve performance
th√¥ng qua trial v√† error.

7.2 TRAINING PIPELINE:

```csharp
IEnumerator RunSelfPlayTraining()
{
    for (int episode = 1; episode <= maxEpisodes; episode++)
    {
        // Reset game state
        ResetGameForTraining();
        
        while (!IsGameOver())
        {
            // Get current state
            GameState currentState = GetCurrentGameState();
            List<AIMove> possibleMoves = GetPossibleMoves();
            
            // Choose action using Œµ-greedy
            AIMove chosenMove = qLearningAgent.ChooseAction(currentState, possibleMoves);
            
            // Execute move
            ExecuteMove(chosenMove);
            
            // Calculate reward
            float reward = CalculateReward(currentState, chosenMove);
            
            // Update Q-table
            GameState newState = GetCurrentGameState();
            qLearningAgent.UpdateQValue(
                EncodeState(currentState),
                EncodeAction(chosenMove),
                reward,
                EncodeState(newState)
            );
            
            yield return new WaitForEndOfFrame();
        }
        
        // Epsilon decay
        qLearningAgent.DecayEpsilon();
        
        // Save progress periodically
        if (episode % 100 == 0)
        {
            qLearningAgent.SaveQTable();
        }
    }
}
```

7.3 REWARD ENGINEERING:
Critical cho Q-Learning success:

```csharp
float CalculateReward(GameState state, AIMove move)
{
    float reward = 0f;
    
    // Game outcome rewards
    if (IsWin(state)) reward += 1000f;
    if (IsLoss(state)) reward -= 1000f;
    
    // Progress rewards
    if (move.moveType == MoveType.Movement)
    {
        if (IsMoveTowardGoal(move)) reward += 20f;
        else reward -= 5f;
    }
    
    // Strategic rewards
    if (move.moveType == MoveType.WallPlacement)
    {
        if (IsWallBlockingOpponent(move)) reward += 50f;
        if (IsWallStrategic(move)) reward += 30f;
        if (IsWallWasted(move)) reward -= 25f;
    }
    
    // Style-specific rewards
    reward += GetStyleSpecificReward(state, move);
    
    return reward;
}
```

7.4 TRAINING METRICS:
- Episodes: 1000 standard
- Duration: ~15 minutes
- Convergence: Usually around episode 800
- Q-table size: 13,118+ states at completion
- Final epsilon: 0.1 (90% exploitation)

================================================================================
                        8. AI TESTING FRAMEWORK
================================================================================

8.1 TESTING PHILOSOPHY:
Comprehensive testing ensures AI quality v√† reliability across different
game scenarios v√† edge cases.

8.2 TEST CATEGORIES:

A) DECISION TREE TESTS:
```csharp
void TestDecisionTreeStrategies()
{
    // Test Early Game
    var earlyGameFeatures = new DecisionFeatures
    {
        aiDistanceToGoal = 8, humanDistanceToGoal = 8,
        isEarlyGame = true
    };
    var strategy = decisionTree.Predict(earlyGameFeatures);
    Assert.AreEqual(AIStrategy.Balanced, strategy);
    
    // Test AI Leading
    var leadingFeatures = new DecisionFeatures
    {
        distanceAdvantage = 3, // AI leading
        isMidGame = true
    };
    strategy = decisionTree.Predict(leadingFeatures);
    Assert.AreEqual(AIStrategy.Blocking, strategy);
}
```

B) ENTROPY ANALYSIS TESTS:
```csharp
void TestEntropyAnalysis()
{
    // High entropy case (diverse scores)
    var highEntropyScores = new List<float> { 100f, 98f, 97f, 95f, 94f };
    float entropy = CalculateTestEntropy(highEntropyScores);
    Assert.IsTrue(entropy > 0.5f);
    
    // Low entropy case (clear winner)
    var lowEntropyScores = new List<float> { 100f, 50f, 50f, 50f, 50f };
    entropy = CalculateTestEntropy(lowEntropyScores);
    Assert.IsTrue(entropy < 0.5f);
}
```

C) SOFTMAX DISTRIBUTION TESTS:
```csharp
void TestSoftmaxDistribution()
{
    var testScores = new List<float> { 100f, 98f, 95f, 90f };
    float temperature = 1.0f;
    
    var probabilities = CalculateSoftmax(testScores, temperature);
    
    // Verify probabilities sum to 1
    float sum = probabilities.Sum();
    Assert.AreEqual(1.0f, sum, 0.001f);
    
    // Verify highest score gets highest probability
    int maxIndex = Array.IndexOf(probabilities, probabilities.Max());
    Assert.AreEqual(0, maxIndex);
}
```

D) PERFORMANCE BENCHMARKS:
- Move generation time: <100ms
- Q-Learning decision: <50ms
- Minimax decision (depth 4): <200ms
- Memory usage: <256MB

8.3 AUTOMATED TESTING:
Tests run automatically ƒë·ªÉ ensure code quality:
- Unit tests cho individual algorithms
- Integration tests cho AI system interactions
- Performance tests cho response times
- Regression tests ƒë·ªÉ prevent quality degradation

================================================================================
                        9. HYBRID AI ARCHITECTURE
================================================================================

9.1 CONCEPT:
Hybrid system k·∫øt h·ª£p strengths c·ªßa multiple algorithms:
- Q-Learning: Data-driven, learns from experience
- Minimax: Logical, guaranteed optimal given evaluation
- Decision Tree: Fast, context-aware switching
- Entropy: Adaptive, handles uncertainty

9.2 ALGORITHM SELECTION LOGIC:

```csharp
public AIMove GetBestMove()
{
    UpdateGameState();
    List<AIMove> possibleMoves = GeneratePossibleMoves();
    
    // Primary: Use Q-Learning if trained model available
    if (useQLearning && qAgent.IsTrainedModel())
    {
        Debug.Log($"üß† Using {specializedStyle} Q-Learning");
        return qAgent.ChooseAction(currentGameState, possibleMoves);
    }
    
    // Fallback: Use Minimax for guaranteed reasonable play
    else
    {
        Debug.Log($"üîç Using {specializedStyle} Minimax");
        return FindBestMoveWithMinimax();
    }
}
```

9.3 SEAMLESS SWITCHING:
- No interruption in gameplay
- Maintains consistent API
- Preserves game state integrity
- Logs algorithm used for debugging

9.4 BENEFITS:
- Robustness: Fallback ensures AI never fails
- Performance: Q-Learning for trained scenarios, Minimax for edge cases
- Flexibility: Can switch algorithms based on situation
- Development: Easy to test individual components

================================================================================
                        10. PERFORMANCE OPTIMIZATION
================================================================================

10.1 Q-TABLE OPTIMIZATION:
- Sparse representation: Only store visited states
- Compression: String interning for state keys
- Lazy loading: Load Q-table on demand
- Periodic saving: Every 100 training episodes

10.2 MINIMAX OPTIMIZATION:
- Alpha-Beta pruning: 50% search reduction
- Move ordering: Promising moves first
- Iterative deepening: Gradual depth increase
- Transposition tables: Cache evaluated positions

10.3 MEMORY MANAGEMENT:
- Object pooling cho GameState objects
- String caching cho repeated state/action encodings
- Garbage collection optimization
- Memory profiling v√† monitoring

10.4 MULTITHREADING CONSIDERATIONS:
- Q-Learning training: Background thread
- Move calculation: Main thread (Unity requirement)
- File I/O: Async operations where possible
- Thread-safe collections cho shared data

10.5 CACHING STRATEGIES:
- Path finding results
- Move generation results
- Evaluation function results
- Feature extraction results

================================================================================
                            K·∫æT LU·∫¨N
================================================================================

D·ª± √°n AI Quoridor Project th·ªÉ hi·ªán m·ªôt implementation comprehensive c·ªßa modern
game AI techniques:

TECHNICAL INNOVATIONS:
‚úÖ Hybrid Q-Learning + Minimax architecture
‚úÖ 6 specialized AI personalities v·ªõi separate training
‚úÖ Advanced decision making v·ªõi entropy analysis
‚úÖ Comprehensive testing v√† validation framework
‚úÖ Professional development tools v√† debugging

EDUCATIONAL VALUE:
‚úÖ Practical machine learning implementation
‚úÖ Traditional game AI algorithms
‚úÖ Modern software engineering practices
‚úÖ Performance optimization techniques
‚úÖ Automated testing methodologies

FUTURE ENHANCEMENTS:
üöÄ Deep Q-Learning (DQN) integration
üöÄ Multi-agent reinforcement learning
üöÄ Neural network-based evaluation functions
üöÄ Online learning v√† adaptation
üöÄ Transfer learning between game variants

D·ª± √°n n√†y demonstrating successful integration c·ªßa classical AI methods v·ªõi
modern machine learning, t·∫°o ra m·ªôt intelligent, adaptive, v√† highly
interactive gaming experience.

================================================================================
                            APPENDIX
================================================================================

FILES LI√äN QUAN:
- Assets/scripts/QLearningAgent.cs - Q-Learning implementation
- Assets/scripts/QuoridorAI.cs - Main AI controller
- Assets/scripts/DecisionTreeClassifier.cs - Decision tree logic
- Assets/scripts/QuoridorSelfPlayTrainer.cs - Training system
- Assets/scripts/AITestScript.cs - Testing framework
- Assets/scripts/Editor/ForceRetrainAI.cs - Development tools

Q-TABLE FILES:
- StreamingAssets/qtable.json - Balanced style
- StreamingAssets/qtable_aggressive.json - Aggressive style
- StreamingAssets/qtable_defensive.json - Defensive style
- StreamingAssets/qtable_wallmaster.json - Wall Master style
- StreamingAssets/qtable_speedrunner.json - Speed Runner style
- StreamingAssets/qtable_tactical.json - Tactical style

DOCUMENTATION:
- AI_ALGORITHMS_DETAILED_DESCRIPTION.txt - This file
- README.md - Project overview
- API_INTEGRATION_GUIDE.md - Voice/Chat integration
- TRAINING_GUIDE.md - AI training instructions

================================================================================
